{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ee80d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5667ef",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0bcedde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageStackDNN(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(ImageStackDNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(256 * 50 * 50, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256 * 50 * 50)\n",
    "\n",
    "        # Final conv to reduce to 1 channel\n",
    "        self.final_conv = nn.Conv2d(256, 1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (1, N*3, 400, 400)\n",
    "        x = Fun.relu(self.conv1(x))      # -> (1, 64, 400, 400)\n",
    "        x = self.pool(x)               # -> (1, 64, 200, 200)\n",
    "        x = Fun.relu(self.conv2(x))      # -> (1, 128, 200, 200)\n",
    "        x = self.pool(x)               # -> (1, 128, 100, 100)\n",
    "        x = Fun.relu(self.conv3(x))      # -> (1, 256, 100, 100)\n",
    "        x = self.pool(x)               # -> (1, 256, 50, 50)\n",
    "\n",
    "        # Flatten for fully connected layers\n",
    "        x_flat = x.view(x.size(0), -1)           # -> (1, 256*50*50)\n",
    "        x_fc = Fun.relu(self.fc1(x_flat))          # -> (1, 1024)\n",
    "        x_fc = Fun.relu(self.fc2(x_fc))            # -> (1, 256*50*50)\n",
    "\n",
    "        # Reshape back to conv feature map\n",
    "        x_reshaped = x_fc.view(x.size(0), 256, 50, 50)\n",
    "\n",
    "        # Upsample back to original size\n",
    "        x_up = Fun.interpolate(x_reshaped, size=(400, 400), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Final conv to get single channel output\n",
    "        output = self.final_conv(x_up)           # -> (1, 1, 400, 400)\n",
    "        return Fun.sigmoid(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1cb7dbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 400, 400])\n"
     ]
    }
   ],
   "source": [
    "N=1\n",
    "model = ImageStackDNN(input_channels=3 * N)  # If you have N RGB images\n",
    "input_tensor = torch.randn(1, 3*N, 400, 400)  # Your image stack\n",
    "output = model(input_tensor)\n",
    "print(output.shape)  # Expected: torch.Size([1, 1, 400, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "654bc9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(N,path=None):\n",
    "    # Initialize Model Shapes\n",
    "    modelsFrames = [FramesModel() for _ in range(N)]\n",
    "    #modelsDiff = [DiffModel() for _ in range(N)]\n",
    "    #modelsOF = [OFModel() for _ in range(N)]\n",
    "    #modelsEdges = [EdgesModel() for _ in range(N)]\n",
    "    #modelsSSIM = [SSIMModel() for _ in range(N)]\n",
    "    #modelsWarp = [WarpModel() for _ in range(N)]\n",
    "    #modelsClusterG = [ClusterGModel() for _ in range(N)]\n",
    "    #modelsClusterRGB = [ClusterRGBModel() for _ in range(N)]\n",
    "    #modelsWarpCluster = [WarpClusterModel() for _ in range(N)]\n",
    "    \n",
    "    # Load the state_dicts\n",
    "    if path is not None:\n",
    "        checkpoint = torch.load(state_path)\n",
    "        for model, state_dict in zip(modelsFrames, checkpoint[\"frames\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsDiff, checkpoint[\"diff\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsOF, checkpoint[\"optical_flow\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsEdges, checkpoint[\"edges\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsSSIM, checkpoint[\"ssim\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsWarp, checkpoint[\"warp\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsClusterG, checkpoint[\"clusterG\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsClusterRGB, checkpoint[\"clusterRGB\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "        for model, state_dict in zip(modelsWarpCluster, checkpoint[\"warpcluster\"]):\n",
    "            model.load_state_dict(state_dict)\n",
    "    return modelsFrames#,modelsDiff,modelsOF,modelsEdges,modelsSSIM,modelsWarp,modelsClusterG,modelsWarpCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "372e4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddInconsistencies(Frames,steps,Size,N):\n",
    "    IncFrames = Frames.copy()\n",
    "            \n",
    "    IncGrid = torch.zeros(Size[0]//25, Size[1]//25)\n",
    "    BinImg = np.zeros(Size)\n",
    "    Bin = torch.tensor(np.zeros(Frames[0].shape[1:]).astype(np.float32))\n",
    "            \n",
    "    steps = random.randint(1, Steps)\n",
    "    for s in range(steps):\n",
    "        x = random.randint(0, 5 * Size[0] // 6)\n",
    "        y = random.randint(0, 5 * Size[1] // 6)\n",
    "        xl = random.randint(10, Size[0] // 10)\n",
    "        yl = random.randint(10, Size[1] // 10)\n",
    "                \n",
    "        for p in range(N):\n",
    "            if random.random() >= 0.5 and p != N // 2:\n",
    "                IncFrames[p], _ = AddOneInc2(torch.tensor(np.zeros(Frames[0].shape[:2]).astype(np.float32)), IncFrames[p], x=x, y=y, xl=xl, yl=yl)\n",
    "        IncFrames[N // 2], Bin = AddOneInc2(Bin, IncFrames[N // 2], x=x, y=y, xl=xl, yl=yl)\n",
    "                \n",
    "    PredGrid = torch.zeros(1, 1, Size[0], Size[1])\n",
    "    Bin = Bin.unsqueeze(0).unsqueeze(0)\n",
    "    return PredGrid,Bin,IncFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9b457",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d94467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Path, state_path=None, N=7, Batch=10, Epochs=10, Steps=5, LR=1e-3,Stride=10):\n",
    "    Size = (400, 400)  # Size of Frames\n",
    "    \n",
    "    #Load Models\n",
    "    #modelsFrames,modelsDiff,modelsOF,modelsEdges,modelsSSIM,modelsWarp,modelsClusterG,modelsWarpCluster\n",
    "    modelsFrames=load_models(N,state_path)\n",
    "    \n",
    "    #Initialize Optimizer\n",
    "    all_parameters = []\n",
    "    for model_list in [modelsFrames,modelsDiff,modelsOF,modelsEdges,modelsSSIM,modelsWarp,modelsClusterG,modelsClusterRGB,modelsWarpCluster]:\n",
    "        for model in model_list:\n",
    "            all_parameters += list(model.parameters())\n",
    "    optimizer = optim.Adam(all_parameters, lr=LR)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion1 = nn.BCELoss()\n",
    "    \n",
    "    Loss = []\n",
    "    start_time = time.time()  # Track the start time of training\n",
    "    \n",
    "    total_batches = Epochs * Batch\n",
    "    \n",
    "    for epoch in range(Epochs):\n",
    "        total_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for batch in range(Batch):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Get Frames\n",
    "            Frames = get_video_frames(Path, N, Size)\n",
    "            \n",
    "            PredGrid,Bin,IncFrames = AddInconsistencies(Frames,steps,Size,N)\n",
    "            \n",
    "            for v in range(Size[0]//Stride):\n",
    "                for h in range(Size[1]//Stride):\n",
    "                    if Stride*v+50<=300 and Stride*h+50<=300:\n",
    "                        feat = features[:, :, v * Stride:v * Stride + 50, h * Stride:h * Stride + 50].clone().detach()\n",
    "                        optimizer.zero_grad()\n",
    "                        Pred = model(feat)\n",
    "                        PredGrid[:, :, v * Stride:v * Stride + 50, h * Stride:h * Stride + 50] = Pred\n",
    "                        b = Bin[:, :, v * Stride:v * Stride + 50, h * Stride:h * Stride + 50].clone().detach()\n",
    "                        if Bin.max()>1:\n",
    "                            print(Bin.max(),v,h)\n",
    "                            \n",
    "                        loss = criterion(Pred, b)+ \\\n",
    "                               criterion(Bin,torch.tensor(BinImg,dtype=torch.float32).unsqueeze(0).unsqueeze(0))+ \\\n",
    "                               criterion1(Pred,b)+ \\\n",
    "                               criterion1(Bin,torch.tensor(BinImg,dtype=torch.float32).unsqueeze(0).unsqueeze(0))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        bi = Frame2Numpy(Pred.squeeze(0).squeeze(0))\n",
    "                        BinImg[v * Stride:v * Stride + 50, h * Stride:h * Stride + 50] = bi\n",
    "\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        processed_batches = epoch * Batch + batch + 1\n",
    "                        remaining_batches = total_batches - processed_batches\n",
    "\n",
    "                        if processed_batches > 0:\n",
    "                            avg_time_per_batch = elapsed_time / processed_batches\n",
    "                            estimated_remaining_time = avg_time_per_batch * remaining_batches\n",
    "                        else:\n",
    "                            estimated_remaining_time = 0\n",
    "\n",
    "                        est_hours = int(estimated_remaining_time // 3600)\n",
    "                        est_mins = int((estimated_remaining_time % 3600) // 60)\n",
    "                        est_secs = int(estimated_remaining_time % 60)\n",
    "\n",
    "                        print(f'Batch [{batch+1}/{Batch}], Loss: {loss.item():.4f}, '\n",
    "                              f'ETA: {est_hours:02d}:{est_mins:02d}:{est_secs:02d}', end='\\r')\n",
    "\n",
    "                        f = Frame2Numpy(features[0, N * 3 // 2:N * 3 // 2 + 3])\n",
    "                        cv2.imshow(\"Predicted Inconsistencies\", BinImg)\n",
    "                        cv2.imshow(\"Thresholded Prediction\",(BinImg>0.5).astype(np.float32))\n",
    "                        cv2.imshow(\"Ground Truth\", Frame2Numpy(Bin.squeeze(0).squeeze(0)))\n",
    "                        cv2.imshow(\"Inconsistencies Frame\", f)\n",
    "                        cv2.waitKey(10)  # Check for key press every frame\n",
    "\n",
    "                        key = cv2.waitKey(1) & 0xFF\n",
    "                        if key == 27:\n",
    "                            print(\"Training stopped by user.\")\n",
    "                            cv2.destroyAllWindows()\n",
    "                            return Loss, model.state_dict()\n",
    "\n",
    "                        total_loss += loss.item()\n",
    "                    \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch [{epoch + 1}/{Epochs}], Loss: {total_loss / (Batch * 11 * 11):.6f}')\n",
    "        \n",
    "        LR *= 0.9\n",
    "        Loss.append(total_loss)\n",
    "    \n",
    "    states = model.state_dict()\n",
    "    cv2.destroyAllWindows()\n",
    "    return Loss, states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c8369f",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5787bc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cf5198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = None#torch.load(\"StatesBID3N300x300V10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eebebaf2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FramesModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-6177d3cf5536>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mState\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VDB/Toons'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mStride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-8c9b7d53c818>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Path, state_path, N, Batch, Epochs, Steps, LR, Stride)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#Load Models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mmodelsFrames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsDiff\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsOF\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsEdges\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsSSIM\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsWarp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsClusterG\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodelsWarpCluster\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mload_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstate_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#Initialize Optimizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-453986607e0a>\u001b[0m in \u001b[0;36mload_models\u001b[1;34m(N, path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Initialize Model Shapes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodelsFrames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mFramesModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#modelsDiff = [DiffModel() for _ in range(N)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#modelsOF = [OFModel() for _ in range(N)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-453986607e0a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m# Initialize Model Shapes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodelsFrames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mFramesModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#modelsDiff = [DiffModel() for _ in range(N)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#modelsOF = [OFModel() for _ in range(N)]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FramesModel' is not defined"
     ]
    }
   ],
   "source": [
    "L,State = train('VDB/Toons',Batch=50,state_path=State,N=3,Epochs=20,Steps=25,LR=1e-4,Stride=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "527dfc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d43f9cb548>]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L) ##Cambios de Escenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d15f0c75",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'State' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e2cbb3a67bb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"StatesBID3N300x300V10.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'State' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(State,\"StatesBID3N300x300V11.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51e8a8",
   "metadata": {},
   "source": [
    "# Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c043d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "States = torch.load(\"StatesBID3N300x300V11.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3604691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = (300,300)\n",
    "Model = DNNModel()\n",
    "Model.load_state_dict(States)\n",
    "Model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e16e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames = get_video_frames(\"AI Gen\",3,Size)\n",
    "Frames_Np = [cv2.cvtColor(Frames[n].permute(1, 2, 0).numpy(), cv2.COLOR_BGR2RGB) for n in range(len(Frames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccf49278",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(Frames_Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2224e1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments: expected 3, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6a9e30b18426>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-049a0aeb054a>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(Frames)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#SSIM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTSSIM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mSimScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-049a0aeb054a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#SSIM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTSSIM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mSimScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not enough arguments: expected 3, got 2"
     ]
    }
   ],
   "source": [
    "features = extract_features(Frames).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "89cf786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredImg = np.zeros((300,300))\n",
    "for v in range(0, 6):\n",
    "    for h in range(0, 6):\n",
    "        feat = features[:, :, v * 50:(v + 1) * 50, h * 50:(h + 1) * 50].clone().detach()\n",
    "        Pred = Model(feat)\n",
    "        PredImg[v * 50:(v + 1) * 50, h * 50:(h + 1) * 50] = Frame2Numpy(Pred.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "02ffa3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPred = (PredImg>0.5).astype(np.float32)\n",
    "NF = [cv2.cvtColor(Frame2Numpy(f),cv2.COLOR_BGR2RGB) for f in Frames]\n",
    "IFrame = NF[1].copy()\n",
    "IFrame[TPred == 1] = np.array([0, 255, 0], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1154892",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([PredImg,TPred]+NF+[IFrame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29c8f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add OF and more frames\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aded677",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 128, 128  # Batch size, channels, height, width\n",
    "image = torch.randn(B, C, H, W, requires_grad=True)  # Input image\n",
    "flow = torch.randn(B, 2, H, W, requires_grad=True)  # Optical flow\n",
    "\n",
    "# Target image (for loss calculation)\n",
    "target_image = torch.randn(B, C, H, W)  \n",
    "\n",
    "# Compute the warped image\n",
    "warped_image = warp_image_tensor(image, flow)\n",
    "\n",
    "# Define a loss function (e.g., MSE loss with the target image)\n",
    "loss = Fun.mse_loss(warped_image, target_image)\n",
    "\n",
    "# Backpropagate\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd3744a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imgs = get_video_frames(\"VDB/Toons\",2,(400,400))\n",
    "ThroughFrames([Frame2Numpy(f) for f in Imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f3881476",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = Frame2Numpy(Imgs[0])\n",
    "img1 = Frame2Numpy(Imgs[1])\n",
    "Imgs[0].requires_grad_()\n",
    "Imgs[1].requires_grad_()\n",
    "flow = pyramid_optical_flow(Imgs[0],Imgs[1],1)\n",
    "flow1 = pyramid_optical_flow(Imgs[0],Imgs[1],2)\n",
    "flow2 = pyramid_optical_flow(Imgs[0],Imgs[1],3)############\n",
    "flow3 = pyramid_optical_flow(Imgs[0],Imgs[1],4)\n",
    "Wimg = warp_image_tensor(Imgs[0].unsqueeze(0),flow.unsqueeze(0))\n",
    "wimg1 = Frame2Numpy(Wimg.squeeze(0))\n",
    "Wimg1 = warp_image_tensor(Imgs[0].unsqueeze(0),flow1.unsqueeze(0))\n",
    "wimg2 = Frame2Numpy(Wimg1.squeeze(0))\n",
    "Wimg2 = warp_image_tensor(Imgs[0].unsqueeze(0),flow2.unsqueeze(0))\n",
    "wimg3 = Frame2Numpy(Wimg2.squeeze(0))\n",
    "Wimg3 = warp_image_tensor(Imgs[0].unsqueeze(0),flow3.unsqueeze(0))\n",
    "wimg4 = Frame2Numpy(Wimg3.squeeze(0))\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(Wimg,Wimg1)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a34df7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44.512917, 26.41956, 22.198652, 108.6468)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThroughFrames([img0,wimg1,wimg2,wimg3,wimg4])\n",
    "((img1-wimg1)**2).sum(),((img1-wimg2)**2).sum(),((img1-wimg3)**2).sum(),((img1-wimg4)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f0ed782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "image_tensor = get_video_frames(\"VDB\\Toons\",1,(500,500))[0].requires_grad_()  # Example RGB image\n",
    "segmented_image = differentiable_segmentation(image_tensor, num_clusters=5,sigma=0.1)\n",
    "\n",
    "# Display using OpenCV\n",
    "display_images([Frame2Numpy(image_tensor),Frame2Numpy(segmented_image)])\n",
    "\n",
    "crit = nn.MSELoss()\n",
    "loss = crit(image_tensor,segmented_image)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd9c9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "image_tensor = get_video_frames(\"VDB\\Toons\",1,(500,500))[0].requires_grad_()\n",
    "\n",
    "# Apply differentiable segmentation\n",
    "num_clusters = 2  # Change the number of clusters as needed\n",
    "segmented_rgb = differentiable_segmentationRGB(image_tensor, num_clusters)\n",
    "\n",
    "display_images([Frame2Numpy(image_tensor),Frame2Numpy(segmented_rgb),Frame2Numpy(image_tensor)-Frame2Numpy(segmented_rgb),Frame2Numpy(segmented_rgb)-Frame2Numpy(image_tensor)])\n",
    "crit = nn.MSELoss()\n",
    "loss = crit(image_tensor,segmented_rgb)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5602d56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31566.203, 32562.615)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "image_tensor = get_video_frames(\"VDB\\Toons\",2,(500,500))\n",
    "img1 = image_tensor[0].requires_grad_()\n",
    "img2 = image_tensor[1]\n",
    "# Apply differentiable segmentation\n",
    "num_clusters = 2  # Change the number of clusters as needed\n",
    "segmented_rgb1 = differentiable_segmentationRGB(img1, num_clusters)\n",
    "segmented_rgb2 = differentiable_segmentationRGB(img2, num_clusters)\n",
    "\n",
    "flow = pyramid_optical_flow(segmented_rgb1,segmented_rgb2,alpha=5.0)\n",
    "flow1 = pyramid_optical_flow(segmented_rgb1,segmented_rgb2,alpha=10.0)\n",
    "wimg = warp_image_tensor(segmented_rgb1.unsqueeze(0),flow.unsqueeze(0)).squeeze(0)\n",
    "wimg1 = warp_image_tensor(segmented_rgb1.unsqueeze(0),flow1.unsqueeze(0)).squeeze(0)\n",
    "ThroughFrames([Frame2Numpy(segmented_rgb1),Frame2Numpy(segmented_rgb2),Frame2Numpy(segmented_rgb1),Frame2Numpy(wimg),Frame2Numpy(segmented_rgb1),Frame2Numpy(wimg1)])\n",
    "display_images([Frame2Numpy(segmented_rgb1),Frame2Numpy(segmented_rgb2),Frame2Numpy(wimg),Frame2Numpy(wimg1),Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg),Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg1)])\n",
    "crit = nn.MSELoss()\n",
    "loss = crit(segmented_rgb2,wimg)\n",
    "loss.backward()\n",
    "((Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg))**2).sum(),((Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg1))**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7510ee8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 125, 125])\n",
      "torch.Size([1, 3, 500, 500])\n",
      "26387.504 13193.752\n"
     ]
    }
   ],
   "source": [
    "image_tensor = get_video_frames(\"VDB\\Toons\",1,(500,500))\n",
    "img1 = image_tensor[0].requires_grad_()\n",
    "display_frame(Frame2Numpy(img1))\n",
    "downimg = recursive_downsample(img1.unsqueeze(0),2)\n",
    "display_frame(Frame2Numpy(downimg.squeeze(0)))\n",
    "print(downimg.shape)\n",
    "upimg = recursive_upsample(downimg,2)\n",
    "display_frame(Frame2Numpy(upimg.squeeze(0)))\n",
    "print(upimg.shape)\n",
    "display_frame(abs(Frame2Numpy(img1)-(Frame2Numpy(upimg.squeeze(0)))))\n",
    "meanimg = (img1+upimg.squeeze(0))/2\n",
    "display_frame(Frame2Numpy(meanimg))\n",
    "print((abs(Frame2Numpy(img1)-(Frame2Numpy(upimg.squeeze(0))))).sum(),(abs(Frame2Numpy(img1)-(Frame2Numpy(meanimg.squeeze(0))))).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a41ba46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 100, 100])\n",
      "torch.Size([1, 3, 400, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_tensor = get_video_frames(\"VDB\\Toons\",1,(400,400))\n",
    "img1 = image_tensor[0].requires_grad_()\n",
    "display_frame(Frame2Numpy(img1))\n",
    "downimg,res = laplacian_downsample(img1.unsqueeze(0),2)\n",
    "display_frame(Frame2Numpy(downimg.squeeze(0)))\n",
    "print(downimg.shape)\n",
    "upimg = laplacian_upsample(downimg,res)\n",
    "display_frame(Frame2Numpy(upimg.squeeze(0)))\n",
    "print(upimg.shape)\n",
    "crit = nn.MSELoss()\n",
    "loss = crit(img1,upimg)\n",
    "loss.backward()\n",
    "img1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc49e51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
