{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "507e7da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d794dbf7",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6779a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(Frames):\n",
    "    # Pixel Differences \n",
    "    diff_pix = [Frames[i]-Frames[i+1] for i in range(len(Frames)-1)]\n",
    "    diff_pix = torch.vstack(diff_pix)\n",
    "    #diff_pix = diff_pix/diff_pix.max() if diff_pix.max()!=0 else diff_pix/(diff_pix.max()+1e-10)\n",
    "    #print(\"dif: \",diff_pix.shape)\n",
    "    \n",
    "    # Edges Frames\n",
    "    edge_frames = [Edges_tensor(Frames[i]) for i in range(len(Frames))]\n",
    "    edge_frames = torch.vstack(edge_frames)\n",
    "    #edge_frames = edge_frames/edge_frames.max() if edge_frames.max()!=0 else edge_frames/(edge_frames.max()+1e-10)\n",
    "    #print(\"edges: \",edge_frames.shape)\n",
    "        \n",
    "    #SSIM    \n",
    "    Scores = [TSSIM(Frames[i].unsqueeze(0),Frames[i+1].unsqueeze(0)) for i in range(len(Frames)-1)]\n",
    "    Score = [t[0].item() for t in Scores]\n",
    "    SimScores = torch.cat([t[1].squeeze(0) for t in Scores])\n",
    "    #SimScores = SimScores/SimScores.max() if SimScores.max()!=0 else SimScores/(SimScores.max()+1e-10)\n",
    "    #print(\"SSIM: \",SimScores.shape)\n",
    "        \n",
    "    #Optical Flow    \n",
    "    OpFlow = torch.vstack([for i in range(len(Frames)-1)])\n",
    "    #OpFlow = OpFlow/OpFlow.max() if OpFlow.max()!=0 else OpFlow/(OpFlow.max()+1e-10)\n",
    "    #print(\"OF: \",OpFlow.shape)\n",
    "    \n",
    "    Frames = torch.cat(Frames)\n",
    "    #print(Frames.shape)\n",
    "\n",
    "    #Concatenate all features\n",
    "    features = torch.cat([Frames,diff_pix, edge_frames, SimScores, ])#, OpFlow])\n",
    "    #print(\"Features: \",features.shape)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda709b",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90d0060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNNModel, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(30, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.fpool1 = nn.FractionalMaxPool2d(kernel_size=2, output_ratio=(0.9, 0.9))\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.fpool2 = nn.FractionalMaxPool2d(kernel_size=2, output_ratio=(0.8, 0.8))\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.fpool3 = nn.FractionalMaxPool2d(kernel_size=2, output_ratio=(0.75, 0.75))\n",
    "\n",
    "        # Feature refinement\n",
    "        self.conv4 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "\n",
    "        # Final layer for output\n",
    "        self.conv_out = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(size=(50, 50), mode='bilinear', align_corners=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = Fun.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.fpool1(x)  \n",
    "\n",
    "        x = Fun.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.fpool2(x)\n",
    "\n",
    "        x = Fun.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.fpool3(x)\n",
    "\n",
    "        x = Fun.relu(self.bn4(self.conv4(x)))\n",
    "        x = Fun.relu(self.bn5(self.conv5(x)))\n",
    "\n",
    "        x = self.conv_out(x)  \n",
    "        x = self.upsample(x)  # Restore 50x50\n",
    "        x = self.sigmoid(x)   # Normalize to [0,1]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4032f7",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c9f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Path, state=None, N=3, Batch=10, Epochs=10, Steps=5, LR=1e-3,Stride=10):\n",
    "    Size = (300, 300)  # Size of Frames\n",
    "    \n",
    "    # Initialize Model Shapes\n",
    "    model = DNNModel()\n",
    "    if state is not None:\n",
    "        model.load_state_dict(state)\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Loss function\n",
    "    criterion = nn.MSELoss()\n",
    "    criterion1 = nn.BCELoss()\n",
    "    \n",
    "    Loss = []\n",
    "    start_time = time.time()  # Track the start time of training\n",
    "    \n",
    "    total_batches = Epochs * Batch\n",
    "    \n",
    "    for epoch in range(Epochs):\n",
    "        total_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        for batch in range(Batch):\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            # Get Frames\n",
    "            Frames = get_video_frames(Path, N, Size)\n",
    "            IncFrames = Frames.copy()\n",
    "            \n",
    "            IncGrid = torch.zeros(Size[0]//25, Size[1]//25)\n",
    "            BinImg = np.zeros(Size)\n",
    "            Bin = torch.tensor(np.zeros(Frames[0].shape[1:]).astype(np.float32))\n",
    "            \n",
    "            steps = random.randint(1, Steps)\n",
    "            for s in range(steps):\n",
    "                x = random.randint(0, 5 * Size[0] // 6)\n",
    "                y = random.randint(0, 5 * Size[1] // 6)\n",
    "                xl = random.randint(10, Size[0] // 10)\n",
    "                yl = random.randint(10, Size[1] // 10)\n",
    "                \n",
    "                for p in range(N):\n",
    "                    if random.random() >= 0.5 and p != N // 2:\n",
    "                        IncFrames[p], _ = AddOneInc2(torch.tensor(np.zeros(Frames[0].shape[:2]).astype(np.float32)), IncFrames[p], x=x, y=y, xl=xl, yl=yl)\n",
    "                IncFrames[N // 2], Bin = AddOneInc2(Bin, IncFrames[N // 2], x=x, y=y, xl=xl, yl=yl)\n",
    "                \n",
    "            \n",
    "            features = extract_features(IncFrames).unsqueeze(0)\n",
    "            PredGrid = torch.zeros(1, 1, Size[0], Size[1])\n",
    "            Bin = Bin.unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            for v in range(Size[0]//Stride):\n",
    "                for h in range(Size[1]//Stride):\n",
    "                    if Stride*v+50<=300 and Stride*h+50<=300:\n",
    "                        feat = features[:, :, v * Stride:v * Stride + 50, h * Stride:h * Stride + 50].clone().detach()\n",
    "                        optimizer.zero_grad()\n",
    "                        Pred = model(feat)\n",
    "                        PredGrid[:, :, v * Stride:v * Stride + 50, h * Stride:h * Stride + 50] = Pred\n",
    "                        b = Bin[:, :, v * Stride:v * Stride + 50, h * Stride:h * Stride + 50].clone().detach()\n",
    "                        if Bin.max()>1:\n",
    "                            print(Bin.max(),v,h)\n",
    "                            \n",
    "                        loss = criterion(Pred, b)+ \\\n",
    "                               criterion(Bin,torch.tensor(BinImg,dtype=torch.float32).unsqueeze(0).unsqueeze(0))+ \\\n",
    "                               criterion1(Pred,b)+ \\\n",
    "                               criterion1(Bin,torch.tensor(BinImg,dtype=torch.float32).unsqueeze(0).unsqueeze(0))\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        bi = Frame2Numpy(Pred.squeeze(0).squeeze(0))\n",
    "                        BinImg[v * Stride:v * Stride + 50, h * Stride:h * Stride + 50] = bi\n",
    "\n",
    "                        elapsed_time = time.time() - start_time\n",
    "                        processed_batches = epoch * Batch + batch + 1\n",
    "                        remaining_batches = total_batches - processed_batches\n",
    "\n",
    "                        if processed_batches > 0:\n",
    "                            avg_time_per_batch = elapsed_time / processed_batches\n",
    "                            estimated_remaining_time = avg_time_per_batch * remaining_batches\n",
    "                        else:\n",
    "                            estimated_remaining_time = 0\n",
    "\n",
    "                        est_hours = int(estimated_remaining_time // 3600)\n",
    "                        est_mins = int((estimated_remaining_time % 3600) // 60)\n",
    "                        est_secs = int(estimated_remaining_time % 60)\n",
    "\n",
    "                        print(f'Batch [{batch+1}/{Batch}], Loss: {loss.item():.4f}, '\n",
    "                              f'ETA: {est_hours:02d}:{est_mins:02d}:{est_secs:02d}', end='\\r')\n",
    "\n",
    "                        f = Frame2Numpy(features[0, N * 3 // 2:N * 3 // 2 + 3])\n",
    "                        cv2.imshow(\"Predicted Inconsistencies\", BinImg)\n",
    "                        cv2.imshow(\"Thresholded Prediction\",(BinImg>0.5).astype(np.float32))\n",
    "                        cv2.imshow(\"Ground Truth\", Frame2Numpy(Bin.squeeze(0).squeeze(0)))\n",
    "                        cv2.imshow(\"Inconsistencies Frame\", f)\n",
    "                        cv2.waitKey(10)  # Check for key press every frame\n",
    "\n",
    "                        key = cv2.waitKey(1) & 0xFF\n",
    "                        if key == 27:\n",
    "                            print(\"Training stopped by user.\")\n",
    "                            cv2.destroyAllWindows()\n",
    "                            return Loss, model.state_dict()\n",
    "\n",
    "                        total_loss += loss.item()\n",
    "                    \n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "        print(f'\\nEpoch [{epoch + 1}/{Epochs}], Loss: {total_loss / (Batch * 11 * 11):.6f}')\n",
    "        \n",
    "        LR *= 0.9\n",
    "        Loss.append(total_loss)\n",
    "    \n",
    "    states = model.state_dict()\n",
    "    cv2.destroyAllWindows()\n",
    "    return Loss, states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87214df",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "bba68cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e5b7ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = None#torch.load(\"StatesBID3N300x300V10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "7bf652d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [50/50], Loss: 7.7516, ETA: 01:42:076\n",
      "Epoch [1/20], Loss: 3.117377\n",
      "Batch [50/50], Loss: 3.5773, ETA: 01:36:301\n",
      "Epoch [2/20], Loss: 1.478269\n",
      "Batch [50/50], Loss: 5.0317, ETA: 01:31:045\n",
      "Epoch [3/20], Loss: 1.564615\n",
      "Batch [50/50], Loss: 3.1076, ETA: 01:25:37\n",
      "Epoch [4/20], Loss: 1.223787\n",
      "Batch [50/50], Loss: 1.8926, ETA: 01:21:014\n",
      "Epoch [5/20], Loss: 1.338302\n",
      "Batch [50/50], Loss: 2.1346, ETA: 01:15:278\n",
      "Epoch [6/20], Loss: 1.210068\n",
      "Batch [50/50], Loss: 4.6285, ETA: 01:10:000\n",
      "Epoch [7/20], Loss: 1.298418\n",
      "Batch [50/50], Loss: 4.0921, ETA: 01:04:32\n",
      "Epoch [8/20], Loss: 1.252693\n",
      "Batch [50/50], Loss: 4.1321, ETA: 00:59:07\n",
      "Epoch [9/20], Loss: 1.210036\n",
      "Batch [50/50], Loss: 3.7822, ETA: 00:53:484\n",
      "Epoch [10/20], Loss: 1.393379\n",
      "Batch [50/50], Loss: 1.7665, ETA: 00:48:23\n",
      "Epoch [11/20], Loss: 1.192489\n",
      "Batch [50/50], Loss: 5.7141, ETA: 00:43:019\n",
      "Epoch [12/20], Loss: 1.271040\n",
      "Batch [50/50], Loss: 4.4479, ETA: 00:37:402\n",
      "Epoch [13/20], Loss: 1.365043\n",
      "Batch [50/50], Loss: 1.8994, ETA: 00:32:17\n",
      "Epoch [14/20], Loss: 0.983571\n",
      "Batch [50/50], Loss: 1.0092, ETA: 00:26:545\n",
      "Epoch [15/20], Loss: 1.091389\n",
      "Batch [50/50], Loss: 7.2337, ETA: 00:21:395\n",
      "Epoch [16/20], Loss: 1.210395\n",
      "Batch [50/50], Loss: 6.1776, ETA: 00:16:147\n",
      "Epoch [17/20], Loss: 1.219871\n",
      "Batch [50/50], Loss: 4.7521, ETA: 00:10:490\n",
      "Epoch [18/20], Loss: 1.084367\n",
      "Batch [50/50], Loss: 4.2587, ETA: 00:05:262\n",
      "Epoch [19/20], Loss: 1.313386\n",
      "Batch [50/50], Loss: 3.8041, ETA: 00:00:009\n",
      "Epoch [20/20], Loss: 1.123136\n"
     ]
    }
   ],
   "source": [
    "L,State = train('VDB/Toons',Batch=50,state=State,N=3,Epochs=20,Steps=25,LR=1e-4,Stride=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9d9fb6dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d43f9cb548>]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L) ##Cambios de Escenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "040cb1be",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'State' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e2cbb3a67bb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"StatesBID3N300x300V10.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'State' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(State,\"StatesBID3N300x300V10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de084480",
   "metadata": {},
   "source": [
    "# Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5488e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "States = torch.load(\"StatesBID3N300x300V10.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b473542",
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = (300,300)\n",
    "Model = DNNModel()\n",
    "Model.load_state_dict(States)\n",
    "Model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dee1111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames = get_video_frames(\"AI Gen\",3,Size)\n",
    "Frames_Np = [cv2.cvtColor(Frames[n].permute(1, 2, 0).numpy(), cv2.COLOR_BGR2RGB) for n in range(len(Frames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "197ad699",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(Frames_Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f52d6af",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments: expected 3, got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-6a9e30b18426>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-049a0aeb054a>\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(Frames)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#SSIM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTSSIM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mSimScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-049a0aeb054a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#SSIM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mTSSIM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[0mScore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mSimScores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mScores\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not enough arguments: expected 3, got 2"
     ]
    }
   ],
   "source": [
    "features = extract_features(Frames).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "da03a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredImg = np.zeros((300,300))\n",
    "for v in range(0, 6):\n",
    "    for h in range(0, 6):\n",
    "        feat = features[:, :, v * 50:(v + 1) * 50, h * 50:(h + 1) * 50].clone().detach()\n",
    "        Pred = Model(feat)\n",
    "        PredImg[v * 50:(v + 1) * 50, h * 50:(h + 1) * 50] = Frame2Numpy(Pred.squeeze(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ef96f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TPred = (PredImg>0.5).astype(np.float32)\n",
    "NF = [cv2.cvtColor(Frame2Numpy(f),cv2.COLOR_BGR2RGB) for f in Frames]\n",
    "IFrame = NF[1].copy()\n",
    "IFrame[TPred == 1] = np.array([0, 255, 0], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "516ad95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images([PredImg,TPred]+NF+[IFrame])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c905fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add OF and more frames\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a5747be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_optical_flow(Frame1, Frame2):\n",
    "    \"\"\"\n",
    "    Compute dense optical flow between two RGB frames using all three channels.\n",
    "    \"\"\"\n",
    "    Frame1_gray = cv2.cvtColor(Frame1, cv2.COLOR_BGR2GRAY)\n",
    "    Frame2_gray = cv2.cvtColor(Frame2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate dense optical flow using Farneback method\n",
    "    flow = cv2.calcOpticalFlowFarneback(Frame1_gray, Frame2_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    return flow\n",
    "\n",
    "def warp_image(Frame1, flow):\n",
    "    \"\"\"\n",
    "    Warp an image using the optical flow field.\n",
    "    \"\"\"\n",
    "    h, w = flow.shape[:2]\n",
    "    \n",
    "    # Create a grid of pixel coordinates (x, y)\n",
    "    x, y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    \n",
    "    # Compute new pixel locations by adding flow vectors\n",
    "    remap_x = (x + flow[..., 0]).astype(np.float32)\n",
    "    remap_y = (y + flow[..., 1]).astype(np.float32)\n",
    "    \n",
    "    # Remap using the computed optical flow\n",
    "    warped = cv2.remap(Frame1, remap_x, remap_y, cv2.INTER_LINEAR)\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64eaaace",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames = get_video_frames(\"VDB/Toons\",2,(300,300))\n",
    "Frame1 = cv2.cvtColor(Frame2Numpy(Frames[0]),cv2.COLOR_RGB2BGR)\n",
    "Frame2 = cv2.cvtColor(Frame2Numpy(Frames[1]),cv2.COLOR_RGB2BGR)\n",
    "ThroughFrames([Frame1,Frame2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7338bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = calculate_optical_flow(Frame1, Frame2)\n",
    "warped_Frame1 = warp_image(Frame1, flow*1e7)\n",
    "display_images([Frame1,Frame2,warped_Frame1,abs(Frame2-warped_Frame1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c54cad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_image(image: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Warp an image given an optical flow tensor.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): The input RGB image tensor of shape (B, C, H, W).\n",
    "        flow (torch.Tensor): The optical flow tensor of shape (B, 2, H, W).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The warped image tensor of shape (B, C, H, W).\n",
    "    \"\"\"\n",
    "    import torch.nn.functional as Fun  # Import functional module\n",
    "    \n",
    "    B, C, H, W = image.shape\n",
    "\n",
    "    # Generate a mesh grid of coordinates (height, width)\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(H, device=image.device), \n",
    "                                    torch.arange(W, device=image.device), indexing='ij')\n",
    "    grid = torch.stack((grid_x, grid_y), dim=-1).float()  # shape: (H, W, 2)\n",
    "\n",
    "    # Normalize the grid to the range [-1, 1]\n",
    "    grid = 2.0 * grid / torch.tensor([W - 1, H - 1], device=image.device) - 1.0  # scale to [-1, 1]\n",
    "    grid = grid.view(1, H, W, 2).repeat(B, 1, 1, 1)  # Use `.repeat()` instead of `.expand()`\n",
    "\n",
    "    # Add the optical flow to the grid\n",
    "    flow = flow.permute(0, 2, 3, 1)  # (B, 2, H, W) -> (B, H, W, 2)\n",
    "\n",
    "    # Normalize the flow by the image dimensions (width and height) to match the grid's [-1, 1] range\n",
    "    flow = flow / torch.tensor([W, H], device=image.device)\n",
    "\n",
    "    # Clone grid before modifying it\n",
    "    grid = grid.clone() + flow  \n",
    "\n",
    "    # Sample the image using the new grid\n",
    "    warped_image = Fun.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "    return warped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f15a7ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames = get_video_frames(\"VDB\",2,(300,300))\n",
    "frames = [cv2.cvtColor(Frame2Numpy(f),cv2.COLOR_RGB2BGR) for f in Frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e22f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07309c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flow = torch.ones(1,2,300,300)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11f04c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "WFrame = warp_image(Frames[0].unsqueeze(0),Flow)\n",
    "display_frame(cv2.cvtColor(Frame2Numpy(WFrame.squeeze(0)),cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71d2d1ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-8b103d8689b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mFrames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mWFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "(Frames[1]-WFrame).backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "343f6771",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, H, W = 2, 3, 128, 128  # Batch size, channels, height, width\n",
    "image = torch.randn(B, C, H, W, requires_grad=True)  # Input image\n",
    "flow = torch.randn(B, 2, H, W, requires_grad=True)  # Optical flow\n",
    "\n",
    "# Target image (for loss calculation)\n",
    "target_image = torch.randn(B, C, H, W)  \n",
    "\n",
    "# Compute the warped image\n",
    "warped_image = warp_image(image, flow)\n",
    "\n",
    "# Define a loss function (e.g., MSE loss with the target image)\n",
    "loss = Fun.mse_loss(warped_image, target_image)\n",
    "\n",
    "# Backpropagate\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b41aa3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Imgs = get_video_frames(\"VDB/Toons\",2,(400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69d5c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames([Frame2Numpy(f) for f in Imgs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a307c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img0 = Frame2Numpy(Imgs[0])\n",
    "img1 = Frame2Numpy(Imgs[1])\n",
    "Imgs[0].requires_grad_()\n",
    "Imgs[1].requires_grad_()\n",
    "flow = pyramid_optical_flow(Imgs[0],Imgs[1],1)\n",
    "flow1 = pyramid_optical_flow(Imgs[0],Imgs[1],2)\n",
    "flow2 = pyramid_optical_flow(Imgs[0],Imgs[1],3)\n",
    "flow3 = pyramid_optical_flow(Imgs[0],Imgs[1],4)\n",
    "Wimg = warp_image_tensor(Imgs[0].unsqueeze(0),flow.unsqueeze(0))\n",
    "wimg1 = Frame2Numpy(Wimg.squeeze(0))\n",
    "Wimg1 = warp_image_tensor(Imgs[0].unsqueeze(0),flow1.unsqueeze(0))\n",
    "wimg2 = Frame2Numpy(Wimg1.squeeze(0))\n",
    "Wimg2 = warp_image_tensor(Imgs[0].unsqueeze(0),flow2.unsqueeze(0))\n",
    "wimg3 = Frame2Numpy(Wimg2.squeeze(0))\n",
    "Wimg3 = warp_image_tensor(Imgs[0].unsqueeze(0),flow3.unsqueeze(0))\n",
    "wimg4 = Frame2Numpy(Wimg3.squeeze(0))\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(Wimg,Wimg1)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eb7dfae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1802.6007, 1010.8973, 1686.9662, 3639.0742)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThroughFrames([img0,wimg1,wimg2,wimg3,wimg4])\n",
    "((img1-wimg1)**2).sum(),((img1-wimg2)**2).sum(),((img1-wimg3)**2).sum(),((img1-wimg4)**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0c3fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "image_tensor = get_video_frames(\"VDB\\Toons\",1,(300,300))[0].requires_grad_()  # Example RGB image\n",
    "segmented_image = differentiable_segmentation(image_tensor, num_clusters=5,sigma=0.1)\n",
    "\n",
    "# Display using OpenCV\n",
    "display_images([Frame2Numpy(image_tensor),Frame2Numpy(segmented_image)])\n",
    "\n",
    "crit = nn.MSELoss()\n",
    "loss = crit(image_tensor,segmented_image)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a26e2a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "image_tensor = get_video_frames(\"VDB\\Toons\",1,(300,300))[0].requires_grad_()\n",
    "\n",
    "# Apply differentiable segmentation\n",
    "num_clusters = 2  # Change the number of clusters as needed\n",
    "segmented_rgb = differentiable_segmentationRGB(image_tensor, num_clusters)\n",
    "\n",
    "display_images([Frame2Numpy(image_tensor),Frame2Numpy(segmented_rgb),Frame2Numpy(image_tensor)-Frame2Numpy(segmented_rgb),Frame2Numpy(segmented_rgb)-Frame2Numpy(image_tensor)])\n",
    "crit = nn.MSELoss()\n",
    "loss = crit(image_tensor,segmented_rgb)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed1b9f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16855.816, 17534.89)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage:\n",
    "image_tensor = get_video_frames(\"VDB\\Toons\",2,(300,300))\n",
    "img1 = image_tensor[0].requires_grad_()\n",
    "img2 = image_tensor[1]\n",
    "# Apply differentiable segmentation\n",
    "num_clusters = 2  # Change the number of clusters as needed\n",
    "segmented_rgb1 = differentiable_segmentationRGB(img1, num_clusters)\n",
    "segmented_rgb2 = differentiable_segmentationRGB(img2, num_clusters)\n",
    "\n",
    "flow = pyramid_optical_flow(segmented_rgb1,segmented_rgb2,alpha=5.0)\n",
    "flow1 = pyramid_optical_flow(segmented_rgb1,segmented_rgb2,alpha=10.0)\n",
    "wimg = warp_image_tensor(segmented_rgb1.unsqueeze(0),flow.unsqueeze(0)).squeeze(0)\n",
    "wimg1 = warp_image_tensor(segmented_rgb1.unsqueeze(0),flow1.unsqueeze(0)).squeeze(0)\n",
    "ThroughFrames([Frame2Numpy(segmented_rgb1),Frame2Numpy(segmented_rgb2),Frame2Numpy(segmented_rgb1),Frame2Numpy(wimg),Frame2Numpy(wimg1)])\n",
    "display_images([Frame2Numpy(segmented_rgb1),Frame2Numpy(segmented_rgb2),Frame2Numpy(wimg),Frame2Numpy(wimg1),Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg),Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg1)])\n",
    "crit = nn.MSELoss()\n",
    "loss = crit(segmented_rgb2,wimg)\n",
    "loss.backward()\n",
    "((Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg))**2).sum(),((Frame2Numpy(segmented_rgb2)-Frame2Numpy(wimg1))**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12846fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
