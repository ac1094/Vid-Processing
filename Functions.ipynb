{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8590ff3",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bf97b7",
   "metadata": {},
   "source": [
    "Import libraries needed in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0c1017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2  #For image and video processing and visualization\n",
    "import os   #To interact with operating system and files\n",
    "import numpy as np  #For matrix operations\n",
    "import random   #To generate random numbers\n",
    "from sklearn.mixture import GaussianMixture  #For clustering\n",
    "from sklearn.cluster import DBSCAN   #For clustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth  #For clustering\n",
    "import numba\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e79df",
   "metadata": {},
   "source": [
    "# Open Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbebcfc9",
   "metadata": {},
   "source": [
    "This function recieves a video path and returns a capture stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "519f2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_vid(input_file): #Video path\n",
    "    cap = cv2.VideoCapture(input_file) #Open capture stream\n",
    "    if not cap.isOpened(): #Check if is available\n",
    "        print(\"Error: Could not open video.\")\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8c2e95",
   "metadata": {},
   "source": [
    "# Get Video Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316058e8",
   "metadata": {},
   "source": [
    "This function gets the video's properties for its width and height in pixels, frames per second (fps) and frame count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14006207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_props(cap, display=1): #Video capture stream and flag to display properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) #Get Width\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) #Get Height\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) #Get FPS\n",
    "    count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) #Get Frame Count\n",
    "    if display==1:  #If flag is 1, display properties\n",
    "        print(\"Width: \",width)\n",
    "        print(\"Height: \",height)\n",
    "        print(\"FPS: \",fps)\n",
    "        print(\"Frame Count: \",count)\n",
    "    return width,height,fps,count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf59c8b",
   "metadata": {},
   "source": [
    "# Get Frames from Video as a List "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed8498",
   "metadata": {},
   "source": [
    "This function takes the capture stream of a video and saves its frames in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d76fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(cap): #Video capture stream\n",
    "    frames = [] #Frames list\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) #Get Frame count\n",
    "    for i in range(frame_count): #For each frame\n",
    "        ret, frame = cap.read() # Read a frame from the video\n",
    "        if not ret: #If couldn't read frame\n",
    "            print(\"Error: Could not read frame.\") #Display error message and return read frames\n",
    "            return frames\n",
    "        frames.append(frame) # Save the frame to the list\n",
    "    if not frames: #If list is empty\n",
    "        print(\"No frames were saved.\") #Display error message \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e7855e",
   "metadata": {},
   "source": [
    "# Delete PNG Files in Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4415e",
   "metadata": {},
   "source": [
    "This function deletes all PNG image files in the specified directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b7b5cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_png_files(directory_path): #Directory path to delete all PNG files\n",
    "    for filename in os.listdir(directory_path): # List all files in the specified directory\n",
    "        file_path = os.path.join(directory_path, filename) # Construct full file path\n",
    "        try:\n",
    "            if os.path.isfile(file_path) and filename.lower().endswith('.png'): # Check if it's a PNG file and remove it\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')  #If not, display message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762d749",
   "metadata": {},
   "source": [
    "# Save Frames as Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3954d",
   "metadata": {},
   "source": [
    "This function saves a list of frames as images in the given directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc8cef05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames, frame_folder): #Frame list, directory path to be saved\n",
    "    if not os.path.exists(frame_folder):  # Create the folder to save frames if it doesn't exist\n",
    "        os.makedirs(frame_folder)\n",
    "    frame_count = 0 #frame index\n",
    "    for i in range(len(frames)): #for each frame\n",
    "        frame_filename = os.path.join(frame_folder, f'frame_{frame_count:03d}.png') #directory path and file name\n",
    "        cv2.imwrite(frame_filename, frames[i]) # Save the frame as an image file\n",
    "        frame_count += 1 #Next frame index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd907b",
   "metadata": {},
   "source": [
    "# Read Images in a Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c8370c",
   "metadata": {},
   "source": [
    "This function reads and stores images from a given directory path to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6c5b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(directory_path):\n",
    "    images = []\n",
    "    # List all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Check if the file has a PNG extension\n",
    "        if filename.lower().endswith('.png'):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            # Read the image using OpenCV\n",
    "            image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)  # cv2.IMREAD_UNCHANGED to keep the alpha channel if present\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "            else:\n",
    "                print(f\"Failed to read image: {file_path}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38888820",
   "metadata": {},
   "source": [
    "# Create Video with List of Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b82e3",
   "metadata": {},
   "source": [
    "This function creates and saves the frames in a list into a video file in the specified directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbdf3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vid(frames,output_file,fps): #List of frames, directory path to be saved, fps\n",
    "    height, width, _ = frames[0].shape     # Get frame dimensions\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Define the codec and create VideoWriter object\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (width, height)) \n",
    "    [out.write(frame) for frame in frames]; # Write the frames to the new video file\n",
    "    out.release() # Release the video writer object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ad3bd8",
   "metadata": {},
   "source": [
    "# Display Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ca5706",
   "metadata": {},
   "source": [
    "This function displays a given image in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c840da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(I): #Image to display\n",
    "    cv2.imshow('I', I)  #Display Image\n",
    "    cv2.waitKey(0) #Press any key to stop displaying\n",
    "    cv2.destroyAllWindows() #Close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80b97b",
   "metadata": {},
   "source": [
    "This function display a list of images at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d35c10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images):\n",
    "    for i, img in enumerate(images):\n",
    "        window_name = f'Image {i+1}'\n",
    "        cv2.imshow(window_name, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef8454",
   "metadata": {},
   "source": [
    "# Play Frames in a Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67bc24",
   "metadata": {},
   "source": [
    "This function displays a video made out of a list of frames with the specified FPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76938655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_frames(frames,fps): #list of frames, fps\n",
    "    delay = int(1000/fps) #Delay between frames\n",
    "    print(\"Delay: \",delay)\n",
    "    for frame in frames:\n",
    "        cv2.imshow('Video Playback', frame) # Display the frame\n",
    "        if cv2.waitKey(delay) & 0xFF == ord('q'): # Exit the playback if 'q' is pressed\n",
    "            break\n",
    "    cv2.destroyAllWindows() #Close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2a39d2",
   "metadata": {},
   "source": [
    "# Optical Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9a50c",
   "metadata": {},
   "source": [
    "Optical Flow can be seen as a vector field that describes the movement between two consecutive images or frames in a video. There are many ways to calculate the oprical flow. Some of the methods to solve optical flow are:\n",
    "- Ferneback\n",
    "- Lucas - Kanade\n",
    "- Phase Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa2e1d3",
   "metadata": {},
   "source": [
    "## Farneback Magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b089c",
   "metadata": {},
   "source": [
    "This function take a list of frames to return a list of the optical flow's vector fields and display the magnitude of these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fa71edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFM(frames): #List of frames\n",
    "    OF = []  #List of optical flow's vector field\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY) #Convert to Grayscale\n",
    "    i = 0  #frame's index\n",
    "    while True:\n",
    "        print(i,end='\\r') #Print frame index\n",
    "        next_gray=cv2.cvtColor(frames[(i+1)%len(frames)],cv2.COLOR_BGR2GRAY)#Convert current & next frame to grayscale\n",
    "        # Calculate the dense optical flow using Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 10, 10, 5, 1.2, 0)\n",
    "                                            # prev     next      flow  dist lvl win it smooth std  flag\n",
    "        # Visualize the optical flow\n",
    "        hsv = np.zeros_like(frames[i]) #Matrix with shape like frames with zeros\n",
    "        if len(hsv.shape) != 3 or hsv.shape[2] != 3: #If color image\n",
    "            hsv = np.zeros((frames[i].shape[0], frames[i].shape[1], 3)) #Matrix of size of frame with 3 color channels\n",
    "        hsv[..., 1] = 255 # ch1 Saturation (Full)\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1]) # Cartesian to Polar\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2  #Angle\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX) #Normalize from 0 to 255\n",
    "        flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)   #Convert to RGB\n",
    "        OF.append(flow) #Add to list of Optical Flow\n",
    "        flow_M  = flow[...,0] + flow[...,1] #Add components of vectors\n",
    "        flow_M = flow_M/np.max(flow_M)*255 #Calculate Magnitude of vectors     \n",
    "        # Display the original frames and the optical flow magnitudes\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[(i+1) % len(frames)])\n",
    "        cv2.imshow('Optical Flow', flow_rgb)\n",
    "        cv2.imshow('Optical Flow Mag', flow_M)\n",
    "        # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key to move backwards\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key to move forwards\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-1):\n",
    "                i = len(frames)-1\n",
    "        prev_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY) # Update the previous frame and grayscale image\n",
    "    cv2.destroyAllWindows() # Release the video capture object and close all OpenCV windows\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e7acd",
   "metadata": {},
   "source": [
    "## Draw Optical Flow Vector Field and Sum of all Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da8994b",
   "metadata": {},
   "source": [
    "These functions recieve the optical flow and the frame to draw onto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8489900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_optical_flow_vectors(flow, frame, step): #Optical flow, frame, window size\n",
    "    h, w = frame.shape[:2] #frame size\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2, -1).astype(int) #Grid of window centers\n",
    "    fx, fy = flow[y, x].T #Separate flow components\n",
    "    mask = np.zeros_like(frame) # Create a mask to draw the vectors  \n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2) # Create line endpoints\n",
    "    lines = np.int32(lines + 0.5) # Add space between\n",
    "    # Draw lines and circles for each vector\n",
    "    for (x1, y1), (x2, y2) in lines:\n",
    "        cv2.line(mask, (x2, y2), (x1, y1), (0, 255, 0), 1)\n",
    "        cv2.circle(frame, (x2, y2), 1, (0, 255, 0), -1)\n",
    "    return cv2.add(frame, mask) #Draw mask on top of frame\n",
    "\n",
    "def draw_sum_vector(flow, frame):\n",
    "    h, w = frame.shape[:2] #Frame size\n",
    "    # Compute the sum of all flow vectors by components\n",
    "    sum_fx = np.sum(flow[..., 0])\n",
    "    sum_fy = np.sum(flow[..., 1])\n",
    "    center_x, center_y = w // 2, h // 2 # Calculate the center point of the frame\n",
    "    # Normalize the sum vector to fit within the image\n",
    "    max_length = min(w, h) // 2\n",
    "    vector_length = np.sqrt(sum_fx**2 + sum_fy**2)\n",
    "    if vector_length > 0:\n",
    "        scale = max_length / vector_length #scale factor\n",
    "        end_x = int(center_x + sum_fx * scale)\n",
    "        end_y = int(center_y + sum_fy * scale)\n",
    "    else:\n",
    "        end_x, end_y = center_x, center_y\n",
    "    # Draw the sum vector as a red arrow\n",
    "    frame_with_vector = np.copy(frame) #copy original frame\n",
    "    cv2.arrowedLine(frame_with_vector, (center_x, center_y), (end_x, end_y), (0, 0, 255), 2, tipLength=0.2) \n",
    "    return frame_with_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45201aa1",
   "metadata": {},
   "source": [
    "## Farneack with Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5610a598",
   "metadata": {},
   "source": [
    "The function OFV take a list of frames to return and display the optical flow's vector fields as well as the sum of all vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c29df203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFV(frames, step=15): #Frame list, window size\n",
    "    OF = [] #List of Optical flow's vector fields\n",
    "    i = 0 #Frame's index\n",
    "    while True:\n",
    "        #convert it to grayscale\n",
    "        prev_gray = cv2.cvtColor(np.copy(frames[i]), cv2.COLOR_BGR2GRAY)\n",
    "        next_gray = cv2.cvtColor(np.copy(frames[i+1]), cv2.COLOR_BGR2GRAY)\n",
    "        # Calculate the dense optical flow using Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        # Draw the optical flow vectors on the frame\n",
    "        flow_frame = draw_optical_flow_vectors(flow, cv2.addWeighted(frames[i], 0.5, frames[i+1], 0.5, 0), step)\n",
    "        OF.append(flow) #Add flow to list\n",
    "        # Draw the sum vector on the frame\n",
    "        frame_with_sum_vector = draw_sum_vector(flow, flow_frame)\n",
    "        # Display the original frame with optical flow vectors and sum vector\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        #cv2.imshow('Optical Flow Vectors', flow_frame)\n",
    "        cv2.imshow('Sum Vector', frame_with_sum_vector)\n",
    "        key = cv2.waitKeyEx(0) #Read pressed key\n",
    "        if key == ord('q'): #Stop if pressed\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key for previous frames\n",
    "            i = i - 1\n",
    "            if i < 0: #prevent non existent frames\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key for next frames\n",
    "            i = i + 1\n",
    "            if i > (len(frames) - 2): #prevent non existent frames\n",
    "                i = len(frames) - 2\n",
    "    cv2.destroyAllWindows() #Close all windows\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecfcda4",
   "metadata": {},
   "source": [
    "## Lucas-Kanade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a2d4fa",
   "metadata": {},
   "source": [
    "This function utilizes the Lukas-Kanade method to solve and display the optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f0263c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFLK(frames, step_size=15): #Video Frames, window size\n",
    "    OF = [] #List of optical flow\n",
    "    lk_params=dict(winSize=(step_size,step_size),maxLevel=10,criteria=(cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_COUNT,30,0.01))\n",
    "    #lucas-kanade parameters  #window Size        #PyramidLevel   #End Criteria\n",
    "    i = 0 #Frame index\n",
    "    while i < len(frames) - 1:\n",
    "        #Convert to Grayscale\n",
    "        old_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY) \n",
    "        frame_gray = cv2.cvtColor(frames[i + 1], cv2.COLOR_BGR2GRAY)\n",
    "        #Neighborhood grid centers\n",
    "        grid_y, grid_x = np.mgrid[step_size//2:old_gray.shape[0]:step_size, step_size//2:old_gray.shape[1]:step_size]\n",
    "        p0 = np.vstack((grid_x.ravel(), grid_y.ravel())).T.astype(np.float32).reshape(-1, 1, 2)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params) #Calculate Optical Flow\n",
    "        #Set to 1 if change has been found\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "        #Frames average\n",
    "        frame = cv2.addWeighted(frames[i], 0.5, frames[i + 1], 0.5, 0)\n",
    "        #Calculate Vector Field\n",
    "        flow = np.zeros((old_gray.shape[0], old_gray.shape[1], 2))\n",
    "        flow[good_old[:, 1].astype(int), good_old[:, 0].astype(int), 0] = good_new[:, 0] - good_old[:, 0]\n",
    "        flow[good_old[:, 1].astype(int), good_old[:, 0].astype(int), 1] = good_new[:, 1] - good_old[:, 1]\n",
    "        #Draw Vectors\n",
    "        frame_with_vectors = draw_optical_flow_vectors(flow, frame, step_size)\n",
    "        frame_with_sum_vector = draw_sum_vector(flow, frame_with_vectors)\n",
    "        OF.append(flow) #add to flow list\n",
    "        #Display Frames and Flow\n",
    "        cv2.imshow('Previous Frame',frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        #cv2.imshow('Optical Flow Vectors', frame_with_vectors)\n",
    "        cv2.imshow('Sum Vector', frame_with_sum_vector)\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key to move backward\n",
    "            i = max(0, i - 1)\n",
    "        if key == 2555904:  # Right arrow key to move forward\n",
    "            i = min(len(frames) - 2, i + 1)\n",
    "    cv2.destroyAllWindows()\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b34f10e",
   "metadata": {},
   "source": [
    "## Phase Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7be0de",
   "metadata": {},
   "source": [
    "The PhaseC function calculates and displays the optical flow of the given frames by using Phase Correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3032bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sum_vector_phase_c(sum_dx, sum_dy, frame): #Y component, X component, frame to drawn on\n",
    "    h, w = frame.shape[:2] #Size\n",
    "    center_x, center_y = w // 2, h // 2 # Calculate the center point of the frame\n",
    "    # Normalize the sum vector to fit within the image\n",
    "    max_length = min(w, h) // 2\n",
    "    vector_length = np.sqrt(sum_dx**2 + sum_dy**2)\n",
    "    if vector_length > 0:\n",
    "        scale = 0.1#max_length / vector_length #Scale factor\n",
    "        end_x = int(center_x + sum_dx * scale)\n",
    "        end_y = int(center_y + sum_dy * scale)\n",
    "    else:\n",
    "        end_x, end_y = center_x, center_y\n",
    "    # Draw the sum vector as a red arrow\n",
    "    frame_with_vector = np.copy(frame)\n",
    "    cv2.arrowedLine(frame_with_vector, (center_x, center_y), (end_x, end_y), (0, 0, 255), 2, tipLength=0.2)\n",
    "    return frame_with_vector\n",
    "\n",
    "def PhaseC(frames, block_size=15, grid_step=15):\n",
    "    OF = [] #Optical Flow List\n",
    "    i = 0 #Frame index\n",
    "    while True:\n",
    "        prev_frame = np.copy(frames[i])\n",
    "        next_frame = np.copy(frames[i + 1])\n",
    "\n",
    "        # Convert frames to grayscale\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "        next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Create an image to visualize the flow\n",
    "        flow_img = cv2.cvtColor(prev_gray, cv2.COLOR_GRAY2BGR)\n",
    "        frame = cv2.addWeighted(frames[i], 0.5, frames[i+1], 0.5, 0)\n",
    "\n",
    "        sum_dx, sum_dy = 0, 0\n",
    "\n",
    "        # Iterate over the grid\n",
    "        for y in range(0, prev_gray.shape[0] - block_size, grid_step):\n",
    "            for x in range(0, prev_gray.shape[1] - block_size, grid_step):\n",
    "                # Extract the blocks\n",
    "                prev_block = prev_gray[y:y + block_size, x:x + block_size]\n",
    "                next_block = next_gray[y:y + block_size, x:x + block_size]\n",
    "\n",
    "                # Compute phase correlation\n",
    "                shift, _ = cv2.phaseCorrelate(prev_block.astype(np.float32), next_block.astype(np.float32))\n",
    "                dx, dy = shift\n",
    "\n",
    "                # Sum the vectors\n",
    "                sum_dx += dx\n",
    "                sum_dy += dy\n",
    "\n",
    "                # Scale down the length of the arrows and size of the tips\n",
    "                scale = 1\n",
    "                tip_length = 0.2\n",
    "\n",
    "                # Draw the vector on the flow image\n",
    "                cv2.arrowedLine(frame, (x + block_size // 2, y + block_size // 2),\n",
    "                                (int(x + block_size // 2 + dx * scale), int(y + block_size // 2 + dy * scale)),\n",
    "                                (0, 255, 0), 1, tipLength=tip_length)\n",
    "\n",
    "        # Draw the sum vector on the frame\n",
    "        frame_with_sum_vector = draw_sum_vector_phase_c(sum_dx, sum_dy, frame)\n",
    "\n",
    "        OF.append(flow_img)\n",
    "\n",
    "        # Display the frames and the flow\n",
    "        cv2.imshow('Previous Frame', prev_frame)\n",
    "        cv2.imshow('Next Frame', next_frame)\n",
    "        #cv2.imshow('Optical Flow', frame)\n",
    "        cv2.imshow('Sum Vector', frame_with_sum_vector)\n",
    "\n",
    "        # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i < 0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i > (len(frames) - 2):\n",
    "                i = len(frames) - 2\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2c70b",
   "metadata": {},
   "source": [
    "# Real Time Optical Flow with Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd387b2",
   "metadata": {},
   "source": [
    "This function captures video from a camera and displays it with the optical flow using Furneback's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd18d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_optical_flow(frame, flow, step=16, max_magnitude=100):\n",
    "    h, w = frame.shape[:2]\n",
    "    y, x = np.mgrid[step//2:h:step, step//2:w:step].reshape(2, -1).astype(int)\n",
    "    fx, fy = flow[y, x].T\n",
    "\n",
    "    lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "\n",
    "    vis = frame.copy()\n",
    "    for (x1, y1), (x2, y2) in lines:\n",
    "        cv2.arrowedLine(vis, (x1, y1), (x2, y2), (0, 255, 0), 1, tipLength=0.4)\n",
    "\n",
    "    # Compute the sum of all flow vectors\n",
    "    sum_fx = np.sum(fx)\n",
    "    sum_fy = np.sum(fy)\n",
    "    \n",
    "    # Normalize the sum vector\n",
    "    norm = np.sqrt(sum_fx**2 + sum_fy**2)\n",
    "    if norm > 0:\n",
    "        sum_fx /= norm\n",
    "        sum_fy /= norm\n",
    "\n",
    "    # Scale the normalized vector by the maximum magnitude\n",
    "    sum_fx *= min(norm, max_magnitude)\n",
    "    sum_fy *= min(norm, max_magnitude)\n",
    "    \n",
    "    # Draw the red vector representing the normalized sum of all flow vectors\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    end_x = int(center_x + sum_fx)  # Scale for better visualization\n",
    "    end_y = int(center_y + sum_fy)\n",
    "    cv2.arrowedLine(vis, (center_x, center_y), (end_x, end_y), (0, 0, 255), 2, tipLength=0.4)\n",
    "    \n",
    "    return vis\n",
    "\n",
    "def cap_of():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read initial frame.\")\n",
    "        return\n",
    "    \n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, \n",
    "                                            0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        prev_gray = gray\n",
    "\n",
    "        # Draw the optical flow vectors on the RGB frame\n",
    "        optical_flow_frame = draw_optical_flow(frame, flow)\n",
    "        \n",
    "        # Display the resulting frame with optical flow\n",
    "        cv2.imshow('Webcam Video with Optical Flow', optical_flow_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb35f2",
   "metadata": {},
   "source": [
    "# Frames Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2273c3",
   "metadata": {},
   "source": [
    "This function takes a list of frames of a video and calculates the absolute diffences between two consecutive frames with a threshold to display a binary image with the regions where the most differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23f4c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_dif(frames, threshold=30): #List of frames, threshold\n",
    "    D = [] #Differences List\n",
    "    i = 0 #Frame index\n",
    "    while True:\n",
    "        # Convert frames to grayscale\n",
    "        prev_gray = cv2.cvtColor(np.copy(frames[i]),cv2.COLOR_BGR2GRAY)\n",
    "        curr_gray = cv2.cvtColor(np.copy(frames[i+1]), cv2.COLOR_BGR2GRAY)\n",
    "        # Compute absolute difference between frames\n",
    "        diff = cv2.absdiff(prev_gray, curr_gray) \n",
    "        # Create a mask to highlight pixels with significant changes\n",
    "        mask = np.zeros_like(diff)\n",
    "        mask[diff > threshold] = 255 #Apply Threshold\n",
    "        D.append(mask) #Add to list\n",
    "        # Show original frame and mask\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        cv2.imshow('Pixels with Most Changes', mask)\n",
    "         # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-2):\n",
    "                i = len(frames)-2\n",
    "        prev_frame = frames[i]\n",
    "    # Release the video capture object\n",
    "    cv2.destroyAllWindows()\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d87526",
   "metadata": {},
   "source": [
    "This function takes a list of frames and calculates the normalized absolute difference between 2 consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5fd09df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_dif1(frames): #List of frames, threshold\n",
    "    D = [] #Differences List\n",
    "    i = 0 #Frame index\n",
    "    while True:\n",
    "        # Convert frames to grayscale\n",
    "        prev_gray = cv2.cvtColor(np.copy(frames[i]), cv2.COLOR_BGR2GRAY)\n",
    "        curr_gray = cv2.cvtColor(np.copy(frames[i+1]), cv2.COLOR_BGR2GRAY)\n",
    "        # Compute absolute difference between frames\n",
    "        diff = abs(prev_gray-curr_gray) #Absolute difference\n",
    "        dif = (1/diff.max())*diff*255 #Normalize\n",
    "        D.append(diff) #Add to list\n",
    "        # Show original frame and mask\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        cv2.imshow('Pixels with Most Changes', diff)\n",
    "         # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-2):\n",
    "                i = len(frames)-2\n",
    "        prev_frame = frames[i]\n",
    "    # Release the video capture object\n",
    "    cv2.destroyAllWindows()\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c4834c",
   "metadata": {},
   "source": [
    "# Alter Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e033c3c",
   "metadata": {},
   "source": [
    "The next fucntions are used to alter the frames in a video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f1ec7",
   "metadata": {},
   "source": [
    "## Change Color Channel's Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a430d7",
   "metadata": {},
   "source": [
    "This function changes the range of each individual color channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c14a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_range_colors(image, min_vals=(0, 0, 0), max_vals=(255, 255, 255)):\n",
    "    # Split the image into its BGR channels\n",
    "    b, g, r = cv2.split(image)\n",
    "    # Clip each channel to its respective range\n",
    "    b = np.clip(b, min_vals[0], max_vals[0])\n",
    "    g = np.clip(g, min_vals[1], max_vals[1])\n",
    "    r = np.clip(r, min_vals[2], max_vals[2])\n",
    "    # Merge the channels back together\n",
    "    new_image = cv2.merge((b, g, r))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d97e3",
   "metadata": {},
   "source": [
    "## Add Occlusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e316714",
   "metadata": {},
   "source": [
    "This function adds occlusions in the shape of rectangles and/or circles into the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f47a0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusions(image, num_occlusions=1,loc=[],sizes=[],shapes=['rectangle','circle'], colors=(-1,-1,-1)):    \n",
    "    output_image = np.copy(image) #Copy of frame\n",
    "    height, width = image.shape[:2] #Gets size\n",
    "    num_occlusions = num_occlusions if len(loc)==0 else len(loc) #number of occlusions\n",
    "    # Draw occlusions on the image\n",
    "    for i in range(num_occlusions):\n",
    "        shape_type = random.choice(shapes)\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) if colors==(-1,-1,-1) else colors  # Random color\n",
    "        if shape_type == 'rectangle':\n",
    "            x = random.randint(0, width - 1) if len(loc)==0 else loc[i][0]\n",
    "            y = random.randint(0, height - 1) if len(loc)==0 else loc[i][1]\n",
    "            width_rect = random.randint(5, width//2) if len(sizes)==0 else loc[i][0]\n",
    "            height_rect = random.randint(5, height//2) if len(sizes)==0 else loc[i][1]\n",
    "            cv2.rectangle(output_image, (x, y), (x + width_rect, y + height_rect), color, -1)  # Filled rectangle\n",
    "        elif shape_type == 'circle':\n",
    "            center = (random.randint(0, width - 1), random.randint(0, height - 1)) if len(loc)==0 else (loc[i][0],loc[i][1])\n",
    "            radius = 50#random.randint(5, 100)\n",
    "            cv2.circle(output_image, center, radius, color, -1)  # Filled circle\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32af527f",
   "metadata": {},
   "source": [
    "## Draw Random Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600fde19",
   "metadata": {},
   "source": [
    "This functions draws straight lines in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d0c0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnd_lines(image, num_lines):\n",
    "    # Get the dimensions of the image\n",
    "    height, width = image.shape[:2]\n",
    "    # Copy the image to avoid modifying the original\n",
    "    output_image = image.copy()\n",
    "    for _ in range(num_lines):\n",
    "        # Generate random start point\n",
    "        start_point = (random.randint(0, width-1), random.randint(0, height-1))\n",
    "        # Generate a random angle and length for the line\n",
    "        angle = random.uniform(0, 2 * np.pi)\n",
    "        length = random.randint(1, min(width, height) // 2)  # Limit length to half of the smallest dimension\n",
    "        # Calculate the end point using the angle and length\n",
    "        end_point = (int(start_point[0] + length * np.cos(angle)), \n",
    "                     int(start_point[1] + length * np.sin(angle)))\n",
    "        # Ensure the end point is within the image boundaries\n",
    "        end_point = (min(max(end_point[0], 0), width-1), min(max(end_point[1], 0), height-1))\n",
    "        # Generate a random color (BGR format)\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        # Generate a random thickness for the line\n",
    "        thickness = random.randint(1, 10)\n",
    "        # Draw the line on the image\n",
    "        cv2.line(output_image, start_point, end_point, color, thickness)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4674ea3",
   "metadata": {},
   "source": [
    "## Random Region's Color Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a19f3",
   "metadata": {},
   "source": [
    "This function changes the color channel's range in a certain number of rectangle or ellipse regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8bc6a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnd_regions(image, num_regions):\n",
    "    # Get the dimensions of the image\n",
    "    height, width = image.shape[:2]\n",
    "    # Copy the image to avoid modifying the original\n",
    "    output_image = image.copy()\n",
    "    for _ in range(num_regions):\n",
    "        # Generate random region shape and size\n",
    "        region_shape = random.choice(['rectangle', 'ellipse'])\n",
    "        if region_shape == 'rectangle':\n",
    "            region_width = random.randint(10, width // 3)\n",
    "            region_height = random.randint(10, height // 3)\n",
    "            top_left_x = random.randint(0, width - region_width)\n",
    "            top_left_y = random.randint(0, height - region_height)\n",
    "            # Define the region\n",
    "            region = output_image[top_left_y:top_left_y + region_height, top_left_x:top_left_x + region_width]\n",
    "        elif region_shape == 'ellipse':\n",
    "            center_x = random.randint(width // 3, width - width // 3)\n",
    "            center_y = random.randint(height // 3, height - height // 3)\n",
    "            axis_length = (random.randint(10, width // 3), random.randint(10, height // 3))\n",
    "            angle = random.randint(0, 360)\n",
    "            start_angle = 0\n",
    "            end_angle = 360\n",
    "            # Create a mask for the ellipse\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            cv2.ellipse(mask, (center_x, center_y), axis_length, angle, start_angle, end_angle, 255, -1)\n",
    "            # Extract the region using the mask\n",
    "            region = cv2.bitwise_and(output_image, output_image, mask=mask)\n",
    "        # Change color channels within the region\n",
    "        for channel in range(3):  # Assuming BGR format\n",
    "            # Generate random ranges for the color channel\n",
    "            low = random.randint(0, 255)\n",
    "            high = random.randint(low, 255)\n",
    "            if region_shape == 'rectangle':\n",
    "                region[..., channel] = np.clip(region[..., channel], low, high)\n",
    "            elif region_shape == 'ellipse':\n",
    "                # Apply changes to the region using the mask\n",
    "                channel_region = output_image[..., channel]\n",
    "                channel_region[mask == 255] = np.clip(channel_region[mask == 255], low, high)\n",
    "                output_image[..., channel] = channel_region\n",
    "        if region_shape == 'rectangle':\n",
    "            # Place the modified region back into the image for rectangles\n",
    "            output_image[top_left_y:top_left_y + region_height, top_left_x:top_left_x + region_width] = region\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a59cb4",
   "metadata": {},
   "source": [
    "# Simple Image Cartoonization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10396898",
   "metadata": {},
   "source": [
    "The next functions are used to cartoonize an image pixel wise by using different algorithms and tecniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aafad539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartoonize_image(img, k=10, it = 10, t1 = 150, t2 = 255, ks = 1 , kc=1):\n",
    "    # Apply bilateral filter to smooth the image\n",
    "    img_color = cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    # Convert to grayscale\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply Gaussian Blur\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (kc, kc), 0)\n",
    "    # Detect edges using Canny edge detection\n",
    "    edges = cv2.Canny(img_blur, threshold1=t1, threshold2=t2)\n",
    "    # Dilate the edges to make them more prominent\n",
    "    kernel = np.ones((ks, ks), np.uint8)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    # Invert the edges\n",
    "    edges = cv2.bitwise_not(edges)\n",
    "    # Convert edges back to color, so we can combine with color image\n",
    "    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    # Perform K-means clustering\n",
    "    img_data = np.float32(img_color).reshape((-1, 3))\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, it, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(img_data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    centers = np.uint8(centers)\n",
    "    img_clustered = centers[labels.flatten()]\n",
    "    img_clustered = img_clustered.reshape(img_color.shape)\n",
    "    # Combine edge and clustered image\n",
    "    cartoon = cv2.bitwise_and(img_clustered, edges_colored)\n",
    "    return cartoon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93647c",
   "metadata": {},
   "source": [
    "# Image Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6a40c",
   "metadata": {},
   "source": [
    "The following functions calculate different variability indexes of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "204c34b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_luminance(image):\n",
    "    b, g, r = cv2.split(image)\n",
    "    luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b\n",
    "    #return np.mean(luminance)\n",
    "    return luminance.sum()/(image.shape[0]*image.shape[1]*255)\n",
    "\n",
    "def color_variance(image):\n",
    "    variance_b = np.var(image[:, :, 0])\n",
    "    variance_g = np.var(image[:, :, 1])\n",
    "    variance_r = np.var(image[:, :, 2])\n",
    "    return ((variance_b+variance_g+variance_r)/3)/(image.shape[0]*image.shape[1])\n",
    "\n",
    "def calculate_variability(image,kernel_size=3):\n",
    "    kernel = np.ones((kernel_size,kernel_size))/(kernel_size**2-1)\n",
    "    kernel[kernel_size//2,kernel_size//2] = -1\n",
    "    V = cv2.filter2D(image,-1,kernel)\n",
    "    return abs(V).sum()/image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99eddefe",
   "metadata": {},
   "source": [
    "# Color Limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fff73680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_limit(img,N_colors=20):\n",
    "    # Convert BGR (OpenCV format) to RGB (PIL format)\n",
    "    image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Convert NumPy array (RGB) to PIL Image\n",
    "    pil_image = Image.fromarray(image_rgb)\n",
    "    # Apply the quantize method in PIL\n",
    "    quantized_image = pil_image.quantize(N_colors)\n",
    "    # Convert the quantized PIL image back to NumPy array (RGB format)\n",
    "    image_rgb_back = np.array(quantized_image.convert('RGB'))  # Convert quantized image back to RGB\n",
    "    #Convert RGB back to BGR for displaying with OpenCV\n",
    "    image_bgr_back = cv2.cvtColor(image_rgb_back, cv2.COLOR_RGB2BGR)\n",
    "    return image_bgr_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5c68e",
   "metadata": {},
   "source": [
    "# Pallete Applier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a268663",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\n",
    "    (0, 0, 0),   # Black\n",
    "    (255, 255, 255),  # White\n",
    "    (255, 0, 0),   # Red\n",
    "    (0, 255, 0),   # Green\n",
    "    (0, 0, 255),   # Blue\n",
    "    # Add more colors as needed\n",
    "]\n",
    "\n",
    "def apply_palette(img, palette):\n",
    "    img = img.convert(\"RGB\")\n",
    "    palette_img = Image.new(\"P\", (1, 1))\n",
    "    palette_img.putpalette(sum(palette, ()))\n",
    "    return cv2.cvtColor(np.array(img.quantize(palette=palette_img).convert(\"RGB\")),cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73442f16",
   "metadata": {},
   "source": [
    "# Add One Inconsistency to Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc1d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOneInc(F):\n",
    "    Bin = np.zeros(F[0].shape[:2],dtype=np.uint8)\n",
    "    I = F.copy()\n",
    "    x,y = random.randint(0,2*I.shape[0]//3),random.randint(0,2*I.shape[1]//3)\n",
    "    l = random.randint(20,I.shape[1]//5)\n",
    "    Op = random.randint(0,3)\n",
    "    if Op==0:\n",
    "        I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(random.randint(30,140),random.randint(30,140),random.randint(30,140)),(random.randint(150,255),random.randint(150,255),random.randint(150,255)))\n",
    "        Bin[x:x+l,y:y+l]=255\n",
    "    elif Op==1:\n",
    "        R,G,B = random.randint(0,255),random.randint(0,255),random.randint(0,255)\n",
    "        I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(R,G,B),(R,G,B))\n",
    "        Bin[x:x+l,y:y+l]=255\n",
    "    elif Op==2:\n",
    "        Thick = random.randint(1,10)\n",
    "        l2 = random.randint(10,I.shape[1]//8)\n",
    "        I = cv2.line(I,(x,y),(x+l,y+l2),(random.randint(0,255),random.randint(0,255),random.randint(0,255)),Thick)\n",
    "        Bin = cv2.line(Bin,(x,y),(x+l,y+l),255,Thick)\n",
    "    else:\n",
    "        Thick = random.randint(1,10)\n",
    "        l2 = random.randint(10,I.shape[1]//8)\n",
    "        I = cv2.line(I,(x,y),(x+l,y+l2),(0,0,0),Thick)\n",
    "        Bin = cv2.line(Bin,(x,y),(x+l,y+l),255,Thick)\n",
    "    return I,Bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f001f45",
   "metadata": {},
   "source": [
    "# Add Inconsistencies to Video Each N Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87244e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddInc(F,N=3):\n",
    "    Inc = []\n",
    "    Bin = [np.zeros(F[0].shape[:2],dtype=np.uint8) for _ in F]\n",
    "    for i in range(len(F)):\n",
    "        if i%N==0 and i!=0:\n",
    "            I = F[i].copy()\n",
    "            x,y = random.randint(0,2*I.shape[0]//3),random.randint(0,2*I.shape[1]//3)\n",
    "            print(I.shape)\n",
    "            l = random.randint(20,I.shape[1]//5)\n",
    "            Op = random.randint(0,3)\n",
    "            if Op==0:\n",
    "                I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(random.randint(30,140),random.randint(30,140),random.randint(30,140)),(random.randint(150,255),random.randint(150,255),random.randint(150,255)))\n",
    "                Bin[i][x:x+l,y:y+l]=255\n",
    "            elif Op==1:\n",
    "                R,G,B = random.randint(0,255),random.randint(0,255),random.randint(0,255)\n",
    "                I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(R,G,B),(R,G,B))\n",
    "                Bin[i][x:x+l,y:y+l]=255\n",
    "            elif Op==2:\n",
    "                Thick = random.randint(1,10)\n",
    "                l2 = random.randint(10,I.shape[1]//8)\n",
    "                I = cv2.line(I,(x,y),(x+l,y+l2),(random.randint(0,255),random.randint(0,255),random.randint(0,255)),Thick)\n",
    "                Bin[i] = cv2.line(Bin[i],(x,y),(x+l,y+l),255,Thick)\n",
    "            else:\n",
    "                Thick = random.randint(1,10)\n",
    "                l2 = random.randint(10,I.shape[1]//8)\n",
    "                I = cv2.line(I,(x,y),(x+l,y+l2),(0,0,0),Thick)\n",
    "                Bin[i] = cv2.line(Bin[i],(x,y),(x+l,y+l),255,Thick)\n",
    "            Inc.append(I)\n",
    "        else:\n",
    "            Inc.append(F[i])\n",
    "    return Inc,Bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c1318a",
   "metadata": {},
   "source": [
    "# Cartoonize Video using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5f0ee1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartoonize_vid(F,t1 = 150, t2 = 255, ks = 1 , kc=1):\n",
    "    C = []\n",
    "    centers = np.uint8(np.asarray([[r,g,b] for r in range(0,255,40) for g in range(0,255,40) for b in range(0,255,40)]))\n",
    "    It = 0\n",
    "    for f in F:\n",
    "        It = It+1\n",
    "        print(It,'/',len(F),end='\\r')\n",
    "        img_color = cv2.bilateralFilter(f, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        # Convert to grayscale\n",
    "        img_gray = cv2.cvtColor(f, cv2.COLOR_RGB2GRAY)\n",
    "        # Apply Gaussian Blur\n",
    "        img_blur = cv2.GaussianBlur(img_gray, (kc, kc), 0)\n",
    "        # Detect edges using Canny edge detection\n",
    "        edges = cv2.Canny(img_blur, threshold1=t1, threshold2=t2)\n",
    "        # Dilate the edges to make them more prominent\n",
    "        kernel = np.ones((ks, ks), np.uint8)\n",
    "        edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "        # Invert the edges\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "        # Convert edges back to color, so we can combine with color image\n",
    "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "        # Perform K-means clustering\n",
    "        img_data = np.float32(img_color).reshape((-1, 3))\n",
    "\n",
    "        distances = np.linalg.norm(img_data[:, np.newaxis] - centers, axis=2)\n",
    "        closest_clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Map the pixels in the second image to the closest cluster centers\n",
    "        segmented_pixels2 = centers[closest_clusters]\n",
    "        segmented_image2 = segmented_pixels2.reshape(f.shape)\n",
    "\n",
    "        # Combine edge and clustered image\n",
    "        cartoon = cv2.bitwise_and(segmented_image2, edges_colored)\n",
    "        C.append(cartoon)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa9ddb",
   "metadata": {},
   "source": [
    "# Canny Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7e52e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny_edge(image,tl=150,th=255,inverted=True):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Canny edge detector\n",
    "    edges = cv2.Canny(gray_image, threshold1=tl, threshold2=th)\n",
    "    # Invert the binary image (0 becomes 255, and 255 becomes 0)\n",
    "    if inverted:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return edges.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419c8648",
   "metadata": {},
   "source": [
    "# Sobel Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e98617b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute the gradients using the Sobel operator\n",
    "    sobelx = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)  # Gradient in x-direction\n",
    "    sobely = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)  # Gradient in y-direction\n",
    "    # Calculate the magnitude of the gradient\n",
    "    magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Normalize to range 0 to 255 and convert to uint8\n",
    "    magnitude = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(magnitude, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b417d8",
   "metadata": {},
   "source": [
    "# Prewitt Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "653255b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewitt_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Define Prewitt kernels\n",
    "    kernelx = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=int)\n",
    "    kernely = np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]], dtype=int)\n",
    "    # Apply the Prewitt operator\n",
    "    prewittx = cv2.filter2D(gray_image, -1, kernelx)\n",
    "    prewitty = cv2.filter2D(gray_image, -1, kernely)\n",
    "    # Calculate the magnitude of the gradient\n",
    "    magnitude = np.sqrt(prewittx**2 + prewitty**2)\n",
    "    # Normalize to range 0 to 255 and convert to uint8\n",
    "    magnitude = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(magnitude, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008a6de5",
   "metadata": {},
   "source": [
    "# Scharr Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "15fe4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scharr_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute the gradients using the Scharr operator\n",
    "    scharrx = cv2.Scharr(gray_image, cv2.CV_64F, 1, 0)  # Gradient in x-direction\n",
    "    scharry = cv2.Scharr(gray_image, cv2.CV_64F, 0, 1)  # Gradient in y-direction\n",
    "    # Calculate the magnitude of the gradient\n",
    "    magnitude = np.sqrt(scharrx**2 + scharry**2)\n",
    "    # Normalize to range 0 to 255 and convert to uint8\n",
    "    magnitude = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(magnitude, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ff7e6f",
   "metadata": {},
   "source": [
    "# Laplacian Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bc0b3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Laplacian operator to find edges\n",
    "    laplacian_edges = cv2.Laplacian(gray_image, cv2.CV_64F)\n",
    "    # Convert to absolute values and normalize to range 0 to 255\n",
    "    laplacian_edges = cv2.convertScaleAbs(laplacian_edges)\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(laplacian_edges, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75951fe3",
   "metadata": {},
   "source": [
    "# Read First N Frames from Videos in Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9cea9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_frames(directory,N):\n",
    "    video_frames = []  # Dictionary to store video names and their first 10 frames\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mp4\"):  # Check for video file extensions\n",
    "            video_path = os.path.join(directory, filename)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            frame_count = 0\n",
    "            while frame_count < N and cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break  # If the video ends before 10 frames\n",
    "                frames.append(frame)\n",
    "                frame_count += 1\n",
    "            cap.release()  # Release the video capture object\n",
    "            video_frames += frames  # Store the frames with the video name\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cacc53",
   "metadata": {},
   "source": [
    "# Calculate Gradients from 2 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1b1ed8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grads(img1, img2):\n",
    "    # Calculate the spatial gradients using Sobel operator\n",
    "    gx1 = cv2.Sobel(img1, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    gy1 = cv2.Sobel(img1, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    gx2 = cv2.Sobel(img2, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    gy2 = cv2.Sobel(img2, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    # Calculate the temporal gradient\n",
    "    tg = cv2.absdiff(img2, img1)\n",
    "    # Normalize gradients for display purposes\n",
    "    gx1d = cv2.convertScaleAbs(gx1)\n",
    "    gy1d = cv2.convertScaleAbs(gy1)\n",
    "    gx2d = cv2.convertScaleAbs(gx2)\n",
    "    gy2d = cv2.convertScaleAbs(gy2)\n",
    "    tgd = cv2.convertScaleAbs(tg)\n",
    "    return gx1,gy1,gx2,gy2,tg,gx1d,gy1d,gx2d,gy2d,tgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ad0706",
   "metadata": {},
   "source": [
    "# Calculate Movement with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ea975b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_movement(img1,img2):\n",
    "    # Calculate gradients\n",
    "    gx1,gy1,gx2,gy2,tg,gx1d,gy1d,gx2d,gy2d,tgd = calculate_and_display_gradients(img1, img2)\n",
    "    # Calculate the magnitude and angle of optical flow\n",
    "    magnitude = cv2.magnitude(gx2,gy2)\n",
    "    angle = cv2.phase(gx2,gy2,angleInDegrees=True)\n",
    "    # Normalize magnitude for display purposes\n",
    "    magnitude_display = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    magnitude_display = cv2.convertScaleAbs(magnitude_display)\n",
    "    # Optional: Visualize flow direction using HSV color space\n",
    "    hsv = np.zeros((img1.shape[0], img1.shape[1], 3), dtype=np.uint8)\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 0] = angle / 2\n",
    "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    optical_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return magnitude_display,optical_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b05561",
   "metadata": {},
   "source": [
    "# Calculate Frames Differences with 3 Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "278f89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_differences(img1,img2):\n",
    "    # Convert images to RGB (OpenCV loads images in BGR format by default)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    # Convert images to grayscale\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate the absolute difference between the images\n",
    "    diff_rgb = np.abs(img1_rgb - img2_rgb)\n",
    "    diff_gray = np.abs(img1_gray.astype(np.float32) - img2_gray.astype(np.float32))\n",
    "    # Normalize the differences to [0, 255]\n",
    "    norm_diff_rgb = cv2.normalize(diff_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_diff_gray = cv2.normalize(diff_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Split the RGB difference into color channels\n",
    "    diff_r, diff_g, diff_b = cv2.split(norm_diff_rgb)\n",
    "    # Convert the single channel differences back to 3-channel images for display\n",
    "    diff_r_colored = cv2.merge([diff_r, np.zeros_like(diff_r), np.zeros_like(diff_r)])\n",
    "    diff_g_colored = cv2.merge([np.zeros_like(diff_g), diff_g, np.zeros_like(diff_g)])\n",
    "    diff_b_colored = cv2.merge([np.zeros_like(diff_b), np.zeros_like(diff_b), diff_b])\n",
    "    return diff_r_colored,diff_g_colored,diff_b_colored,norm_diff_gray.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1032e464",
   "metadata": {},
   "source": [
    "# Calculate Temporal Gradient for each Color Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7b7ac9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_temporal_gradient(img1, img2):\n",
    "    # Convert images to RGB (OpenCV loads images in BGR format by default)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    # Convert images to grayscale\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate the temporal gradient (difference) between the images\n",
    "    grad_rgb = np.abs(img2_rgb - img1_rgb)\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "    # Normalize the gradients to [0, 255]\n",
    "    norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Split the RGB gradient into color channels\n",
    "    grad_r, grad_g, grad_b = cv2.split(norm_grad_rgb)\n",
    "    # Convert the single channel gradients back to 3-channel images for display\n",
    "    grad_r_colored = cv2.merge([grad_r, np.zeros_like(grad_r), np.zeros_like(grad_r)])\n",
    "    grad_g_colored = cv2.merge([np.zeros_like(grad_g), grad_g, np.zeros_like(grad_g)])\n",
    "    grad_b_colored = cv2.merge([np.zeros_like(grad_b), np.zeros_like(grad_b), grad_b])\n",
    "    return grad_r_colored,grad_g_colored,grad_b_colored,norm_grad_gray.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063076b5",
   "metadata": {},
   "source": [
    "# Temporal Consistency Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c898c3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_temporal_consistency(video_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count < 2:\n",
    "        raise ValueError(\"Video must have at least two frames to calculate temporal consistency.\")\n",
    "    \n",
    "    # Initialize variables to accumulate error values\n",
    "    total_error_rgb = np.zeros(3)  # R, G, B channels\n",
    "    total_error_gray = 0\n",
    "    num_frames = 0\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Compute the absolute difference between consecutive frames\n",
    "        diff_rgb = np.abs(curr_frame_rgb.astype(np.float32) - prev_frame_rgb.astype(np.float32))\n",
    "        diff_gray = np.abs(curr_frame_gray.astype(np.float32) - prev_frame_gray.astype(np.float32))\n",
    "        \n",
    "        # Calculate the mean absolute error for the current frame pair\n",
    "        mean_error_rgb = np.mean(diff_rgb, axis=(0, 1))  # Mean error for R, G, B channels\n",
    "        mean_error_gray = np.mean(diff_gray)\n",
    "        \n",
    "        total_error_rgb += mean_error_rgb\n",
    "        total_error_gray += mean_error_gray\n",
    "        num_frames += 1\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Compute the average temporal consistency index\n",
    "    if num_frames == 0:\n",
    "        raise ValueError(\"No frames processed. Check the video file.\")\n",
    "    \n",
    "    temporal_consistency_index_rgb = total_error_rgb / num_frames\n",
    "    temporal_consistency_index_gray = total_error_gray / num_frames\n",
    "    \n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    return temporal_consistency_index_rgb, temporal_consistency_index_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f6ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def identify_abnormality(image1_path, image2_path):\n",
    "    # Read the images\n",
    "    img1 = cv2.imread(image1_path)\n",
    "    img2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both images could not be loaded. Check the file paths.\")\n",
    "    \n",
    "    # Calculate temporal gradients\n",
    "    grad_rgb, grad_gray = calculate_temporal_gradient(img1, img2)\n",
    "    \n",
    "    # Calculate similarity indexes\n",
    "    similarity_indexes = calculate_similarity_indexes(img1, img2)\n",
    "    \n",
    "    # Combine gradients and similarity indexes to identify abnormalities\n",
    "    # Normalize gradients for visibility\n",
    "    norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Create a mask where significant changes or low similarity occur\n",
    "    threshold = 50  # Set an appropriate threshold value\n",
    "    abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "    abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "    # Highlight the regions of interest\n",
    "    highlighted_rgb = img1.copy()\n",
    "    highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "    highlighted_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "    \n",
    "    # Display the results\n",
    "    cv2.imshow('Original Image 1', img1)\n",
    "    cv2.imshow('Original Image 2', img2)\n",
    "    cv2.imshow('Temporal Gradient (RGB)', norm_grad_rgb.astype(np.uint8))\n",
    "    cv2.imshow('Temporal Gradient (Gray)', norm_grad_gray.astype(np.uint8))\n",
    "    cv2.imshow('Abnormal Regions (RGB)', highlighted_rgb)\n",
    "    cv2.imshow('Abnormal Regions (Gray)', highlighted_gray)\n",
    "    \n",
    "    # Print similarity indexes\n",
    "    print(f'Similarity Indexes: {similarity_indexes}')\n",
    "    \n",
    "    # Wait until a key is pressed and close the windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "#image1_path = 'saved_frames/frame_000.png'\n",
    "#image2_path = 'saved_frames/frame_001.png'\n",
    "#identify_abnormality(image1_path, image2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703f754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using MP4 codec\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width * 2, frame_height * 2))  # Updated size for combined frames\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = rgb2gray(prev_frame_rgb)\n",
    "    \n",
    "    while True:\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = rgb2gray(curr_frame_rgb)\n",
    "        \n",
    "        # Calculate temporal gradients and similarity indexes\n",
    "        grad_rgb, grad_gray = calculate_temporal_gradient(prev_frame, curr_frame)\n",
    "        similarity_indexes = calculate_similarity_indexes(prev_frame, curr_frame)\n",
    "        \n",
    "        # Normalize gradients for visibility\n",
    "        norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Create masks for abnormalities\n",
    "        threshold = 50  # Set an appropriate threshold value\n",
    "        abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "        abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "        # Highlight the regions of interest\n",
    "        highlighted_rgb = curr_frame.copy()\n",
    "        highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "        highlighted_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "        \n",
    "        # Combine the results into a single frame\n",
    "        combined_frame = np.hstack((curr_frame, cv2.cvtColor(highlighted_rgb, cv2.COLOR_BGR2RGB)))\n",
    "        combined_frame = np.vstack((combined_frame, np.hstack((cv2.cvtColor(highlighted_gray, cv2.COLOR_GRAY2BGR), norm_grad_rgb.astype(np.uint8)))))\n",
    "        \n",
    "        # Write the combined frame to the output video\n",
    "        out.write(combined_frame)\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "#video_path = 'Cartoonized/U_toon.mp4'\n",
    "#output_path = 'output_abnormalities.mp4'\n",
    "#process_video(video_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2687e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def add_label(image, text, position=(10, 30), font_scale=1, color=(255, 255, 255), thickness=2):\n",
    "    \"\"\"Add a label to an image.\"\"\"\n",
    "    return cv2.putText(image, text, position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using MP4 codec\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width * 3, frame_height * 2))  # Updated size for combined frames\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = rgb2gray(prev_frame_rgb)\n",
    "    \n",
    "    while True:\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = rgb2gray(curr_frame_rgb)\n",
    "        \n",
    "        # Calculate temporal gradients and similarity indexes\n",
    "        grad_rgb, grad_gray = calculate_temporal_gradient(prev_frame, curr_frame)\n",
    "        similarity_indexes = calculate_similarity_indexes(prev_frame, curr_frame)\n",
    "        \n",
    "        # Normalize gradients for visibility\n",
    "        norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        \n",
    "        # Convert single-channel grayscale to 3-channel RGB for displaying\n",
    "        norm_grad_gray_rgb = cv2.cvtColor(norm_grad_gray, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Create masks for abnormalities\n",
    "        threshold = 50  # Set an appropriate threshold value\n",
    "        abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "        abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "        # Highlight the regions of interest\n",
    "        highlighted_rgb = curr_frame.copy()\n",
    "        highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "        highlighted_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "        \n",
    "        # Create the combined frame\n",
    "        combined_frame = np.zeros((frame_height * 2, frame_width * 3, 3), dtype=np.uint8)\n",
    "        combined_frame[0:frame_height, 0:frame_width] = prev_frame\n",
    "        combined_frame[0:frame_height, frame_width:frame_width*2] = curr_frame\n",
    "        combined_frame[0:frame_height, frame_width*2:frame_width*3] = highlighted_rgb\n",
    "        combined_frame[frame_height:frame_height*2, 0:frame_width] = cv2.cvtColor(highlighted_gray, cv2.COLOR_GRAY2BGR)\n",
    "        combined_frame[frame_height:frame_height*2, frame_width:frame_width*2] = norm_grad_rgb\n",
    "        combined_frame[frame_height:frame_height*2, frame_width*2:frame_width*3] = norm_grad_gray_rgb\n",
    "        \n",
    "        # Add labels to the combined frame\n",
    "        combined_frame = add_label(combined_frame, \"Previous Frame\", position=(10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Current Frame\", position=(frame_width + 10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Highlighted Changes\", position=(frame_width * 2 + 10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Gray Gradient\", position=(10, frame_height + 30))\n",
    "        combined_frame = add_label(combined_frame, \"RGB Gradient\", position=(frame_width + 10, frame_height + 30))\n",
    "        combined_frame = add_label(combined_frame, \"Gray Gradient (Norm)\", position=(frame_width * 2 + 10, frame_height + 30))\n",
    "        \n",
    "        # Write the combined frame to the output video\n",
    "        out.write(combined_frame)\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame = curr_frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "#video_path = 'Cartoonized/U_toon.mp4'\n",
    "#output_path = 'output_abnormalities1.mp4'\n",
    "#process_video(video_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d62377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "def random_smooth_optical_flow_warp(image, max_movement=10, smoothness=50):\n",
    "    \"\"\"\n",
    "    Warp an RGB image using a smooth optical flow field, where flow directions\n",
    "    are similar across neighboring regions.\n",
    "\n",
    "    Parameters:\n",
    "    - image: The input RGB image as a NumPy array.\n",
    "    - max_movement: Maximum pixel movement allowed for the optical flow vectors. Default is 10.\n",
    "    - smoothness: Controls how smooth the optical flow is. Higher values make the flow field more uniform.\n",
    "    \n",
    "    Returns:\n",
    "    - warped_image: The warped image based on the smooth optical flow.\n",
    "    \"\"\"\n",
    "    # Get the height and width of the image\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    # Generate random noise-based flow fields for X and Y directions\n",
    "    flow_X = np.random.uniform(-1, 1, (h, w)).astype(np.float32)\n",
    "    flow_Y = np.random.uniform(-1, 1, (h, w)).astype(np.float32)\n",
    "\n",
    "    # Smooth the noise to ensure consistency across regions\n",
    "    flow_X = gaussian_filter(flow_X, sigma=smoothness)\n",
    "    flow_Y = gaussian_filter(flow_Y, sigma=smoothness)\n",
    "\n",
    "    # Normalize the flow fields to the range of [-max_movement, max_movement]\n",
    "    flow_X = cv2.normalize(flow_X, None, -max_movement, max_movement, cv2.NORM_MINMAX)\n",
    "    flow_Y = cv2.normalize(flow_Y, None, -max_movement, max_movement, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Generate a meshgrid of coordinates (X, Y)\n",
    "    X, Y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "\n",
    "    # Create a map that distorts the image based on the smoothed optical flow\n",
    "    map_X = (X + flow_X).astype(np.float32)\n",
    "    map_Y = (Y + flow_Y).astype(np.float32)\n",
    "\n",
    "    # Warp the image using remap function\n",
    "    warped_image = cv2.remap(image, map_X, map_Y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "\n",
    "    return warped_image\n",
    "\n",
    "# Example usage\n",
    "# Load an RGB image\n",
    "image = cv2.imread('IDB/therock.jpg')\n",
    "\n",
    "# Apply random smooth optical flow warp\n",
    "warped_image = random_smooth_optical_flow_warp(image, max_movement=20, smoothness=200)\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow(\"Warped Image\", warped_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
