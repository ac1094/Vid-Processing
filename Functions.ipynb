{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934f820",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc9d957a",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3da6a3",
   "metadata": {},
   "source": [
    "Import libraries needed in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "06c531cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import cv2  #For image and video processing and visualization\n",
    "import os   #To interact with operating system and files\n",
    "import numpy as np  #For matrix operations\n",
    "import random   #To generate random numbers\n",
    "from sklearn.mixture import GaussianMixture  #For clustering\n",
    "from sklearn.cluster import DBSCAN   #For clustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth  #For clustering\n",
    "import numba\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.stats import entropy\n",
    "import torch\n",
    "import torch.nn.functional as Fun\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "import time\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.interpolate import splprep, splev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c307bfa9",
   "metadata": {},
   "source": [
    "# Open Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b825c4",
   "metadata": {},
   "source": [
    "This function recieves a video path and returns a capture stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c475c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_vid(input_file): #Video path\n",
    "    cap = cv2.VideoCapture(input_file) #Open capture stream\n",
    "    if not cap.isOpened(): #Check if is available\n",
    "        print(\"Error: Could not open video.\")\n",
    "    return cap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4123cb7",
   "metadata": {},
   "source": [
    "# Get Video Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b09f83",
   "metadata": {},
   "source": [
    "This function gets the video's properties for its width and height in pixels, frames per second (fps) and frame count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f83f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_props(cap, display=1): #Video capture stream and flag to display properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) #Get Width\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) #Get Height\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) #Get FPS\n",
    "    count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) #Get Frame Count\n",
    "    if display==1:  #If flag is 1, display properties\n",
    "        print(\"Width: \",width)\n",
    "        print(\"Height: \",height)\n",
    "        print(\"FPS: \",fps)\n",
    "        print(\"Frame Count: \",count)\n",
    "    return width,height,fps,count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94baf3",
   "metadata": {},
   "source": [
    "# Get Frames from Video as a List "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae017e0",
   "metadata": {},
   "source": [
    "This function takes the capture stream of a video and saves its frames in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c89684c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(cap): #Video capture stream\n",
    "    frames = [] #Frames list\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) #Get Frame count\n",
    "    for i in range(frame_count): #For each frame\n",
    "        ret, frame = cap.read() # Read a frame from the video\n",
    "        if not ret: #If couldn't read frame\n",
    "            print(\"Error: Could not read frame.\") #Display error message and return read frames\n",
    "            return frames\n",
    "        frames.append(frame) # Save the frame to the list\n",
    "    if not frames: #If list is empty\n",
    "        print(\"No frames were saved.\") #Display error message \n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768abe38",
   "metadata": {},
   "source": [
    "# Delete PNG Files in Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348aa237",
   "metadata": {},
   "source": [
    "This function deletes all PNG image files in the specified directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab517a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_png_files(directory_path): #Directory path to delete all PNG files\n",
    "    for filename in os.listdir(directory_path): # List all files in the specified directory\n",
    "        file_path = os.path.join(directory_path, filename) # Construct full file path\n",
    "        try:\n",
    "            if os.path.isfile(file_path) and filename.lower().endswith('.png'): # Check if it's a PNG file and remove it\n",
    "                os.remove(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')  #If not, display message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638dfbda",
   "metadata": {},
   "source": [
    "# Save Frames as Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888c39a",
   "metadata": {},
   "source": [
    "This function saves a list of frames as images in the given directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b104578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(frames, frame_folder): #Frame list, directory path to be saved\n",
    "    if not os.path.exists(frame_folder):  # Create the folder to save frames if it doesn't exist\n",
    "        os.makedirs(frame_folder)\n",
    "    frame_count = 0 #frame index\n",
    "    for i in range(len(frames)): #for each frame\n",
    "        frame_filename = os.path.join(frame_folder, f'frame_{frame_count:03d}.png') #directory path and file name\n",
    "        cv2.imwrite(frame_filename, frames[i]) # Save the frame as an image file\n",
    "        frame_count += 1 #Next frame index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fab46b",
   "metadata": {},
   "source": [
    "# Read Images in a Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d3a95",
   "metadata": {},
   "source": [
    "This function reads and stores images from a given directory path to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bb27420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(directory_path):\n",
    "    images = []\n",
    "    # List all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Check if the file has a PNG extension\n",
    "        if filename.lower().endswith('.png'):\n",
    "            # Construct full file path\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            # Read the image using OpenCV\n",
    "            image = cv2.imread(file_path, cv2.IMREAD_UNCHANGED)  # cv2.IMREAD_UNCHANGED to keep the alpha channel if present\n",
    "            if image is not None:\n",
    "                images.append(image)\n",
    "            else:\n",
    "                print(f\"Failed to read image: {file_path}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573857bf",
   "metadata": {},
   "source": [
    "# Create Video with List of Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f84491",
   "metadata": {},
   "source": [
    "This function creates and saves the frames in a list into a video file in the specified directory path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6a29c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vid(frames,output_file,fps): #List of frames, directory path to be saved, fps\n",
    "    height, width, _ = frames[0].shape     # Get frame dimensions\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Define the codec and create VideoWriter object\n",
    "    out = cv2.VideoWriter(output_file, fourcc, fps, (width, height)) \n",
    "    [out.write(frame) for frame in frames]; # Write the frames to the new video file\n",
    "    out.release() # Release the video writer object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9710b738",
   "metadata": {},
   "source": [
    "# Display Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5b8410",
   "metadata": {},
   "source": [
    "This function displays a given image in a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626ee2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frame(I): #Image to display\n",
    "    cv2.imshow('I', I)  #Display Image\n",
    "    cv2.waitKey(0) #Press any key to stop displaying\n",
    "    cv2.destroyAllWindows() #Close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb435df",
   "metadata": {},
   "source": [
    "This function display a list of images at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1449450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(images):\n",
    "    for i, img in enumerate(images):\n",
    "        window_name = f'Image {i+1}'\n",
    "        cv2.imshow(window_name, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d47868",
   "metadata": {},
   "source": [
    "# Play Frames in a Window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848a0fd",
   "metadata": {},
   "source": [
    "This function displays a video made out of a list of frames with the specified FPS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965e8a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_frames(frames,fps): #list of frames, fps\n",
    "    delay = int(1000/fps) #Delay between frames\n",
    "    print(\"Delay: \",delay)\n",
    "    for frame in frames:\n",
    "        cv2.imshow('Video Playback', frame) # Display the frame\n",
    "        if cv2.waitKey(delay) & 0xFF == ord('q'): # Exit the playback if 'q' is pressed\n",
    "            break\n",
    "    cv2.destroyAllWindows() #Close all windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7cd7c1",
   "metadata": {},
   "source": [
    "## Display Frames in List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc31f30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ThroughFrames(frames):\n",
    "    i = 0\n",
    "    while True:\n",
    "        cv2.imshow('Frame', frames[i])\n",
    "         # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-1):\n",
    "                i = len(frames)-1\n",
    "    # Release the video capture object\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9edd523",
   "metadata": {},
   "source": [
    "# Optical Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58775d66",
   "metadata": {},
   "source": [
    "Optical Flow can be seen as a vector field that describes the movement between two consecutive images or frames in a video. There are many ways to calculate the oprical flow. Some of the methods to solve optical flow are:\n",
    "- Ferneback\n",
    "- Lucas - Kanade\n",
    "- Phase Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914c002",
   "metadata": {},
   "source": [
    "## Farneback Magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480ca883",
   "metadata": {},
   "source": [
    "This function take a list of frames to return a list of the optical flow's vector fields and display the magnitude of these vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fbb656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFM(frames): #List of frames\n",
    "    OF = []  #List of optical flow's vector field\n",
    "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY) #Convert to Grayscale\n",
    "    i = 0  #frame's index\n",
    "    while True:\n",
    "        print(i,end='\\r') #Print frame index\n",
    "        next_gray=cv2.cvtColor(frames[(i+1)%len(frames)],cv2.COLOR_BGR2GRAY)#Convert current & next frame to grayscale\n",
    "        # Calculate the dense optical flow using Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 10, 10, 5, 1.2, 0)\n",
    "                                            # prev     next      flow  dist lvl win it smooth std  flag\n",
    "        # Visualize the optical flow\n",
    "        hsv = np.zeros_like(frames[i]) #Matrix with shape like frames with zeros\n",
    "        if len(hsv.shape) != 3 or hsv.shape[2] != 3: #If color image\n",
    "            hsv = np.zeros((frames[i].shape[0], frames[i].shape[1], 3)) #Matrix of size of frame with 3 color channels\n",
    "        hsv[..., 1] = 255 # ch1 Saturation (Full)\n",
    "        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1]) # Cartesian to Polar\n",
    "        hsv[..., 0] = ang * 180 / np.pi / 2  #Angle\n",
    "        hsv[..., 2] = cv2.normalize(mag, None, 0, 255, cv2.NORM_MINMAX) #Normalize from 0 to 255\n",
    "        flow_rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)   #Convert to RGB\n",
    "        OF.append(flow) #Add to list of Optical Flow\n",
    "        flow_M  = flow[...,0] + flow[...,1] #Add components of vectors\n",
    "        flow_M = flow_M/np.max(flow_M)*255 #Calculate Magnitude of vectors     \n",
    "        # Display the original frames and the optical flow magnitudes\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[(i+1) % len(frames)])\n",
    "        cv2.imshow('Optical Flow', flow_rgb)\n",
    "        cv2.imshow('Optical Flow Mag', flow_M)\n",
    "        # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key to move backwards\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key to move forwards\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-1):\n",
    "                i = len(frames)-1\n",
    "        prev_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY) # Update the previous frame and grayscale image\n",
    "    cv2.destroyAllWindows() # Release the video capture object and close all OpenCV windows\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f76ab",
   "metadata": {},
   "source": [
    "## Draw Optical Flow Vector Field and Sum of all Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29acd50",
   "metadata": {},
   "source": [
    "These functions recieve the optical flow and the frame to draw onto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc792e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_optical_flow_vectors(flow, frame, step): #Optical flow, frame, window size\n",
    "    h, w = frame.shape[:2] #frame size\n",
    "    y, x = np.mgrid[step/2:h:step, step/2:w:step].reshape(2, -1).astype(int) #Grid of window centers\n",
    "    fx, fy = flow[y, x].T #Separate flow components\n",
    "    mask = np.zeros_like(frame) # Create a mask to draw the vectors  \n",
    "    lines = np.vstack([x, y, x+fx, y+fy]).T.reshape(-1, 2, 2) # Create line endpoints\n",
    "    lines = np.int32(lines + 0.5) # Add space between\n",
    "    # Draw lines and circles for each vector\n",
    "    for (x1, y1), (x2, y2) in lines:\n",
    "        cv2.line(mask, (x2, y2), (x1, y1), (0, 255, 0), 1)\n",
    "        cv2.circle(frame, (x2, y2), 1, (0, 255, 0), -1)\n",
    "    return cv2.add(frame, mask) #Draw mask on top of frame\n",
    "\n",
    "def draw_sum_vector(flow, frame):\n",
    "    h, w = frame.shape[:2] #Frame size\n",
    "    # Compute the sum of all flow vectors by components\n",
    "    sum_fx = np.sum(flow[..., 0])\n",
    "    sum_fy = np.sum(flow[..., 1])\n",
    "    center_x, center_y = w // 2, h // 2 # Calculate the center point of the frame\n",
    "    # Normalize the sum vector to fit within the image\n",
    "    max_length = min(w, h) // 2\n",
    "    vector_length = np.sqrt(sum_fx**2 + sum_fy**2)\n",
    "    if vector_length > 0:\n",
    "        scale = max_length / vector_length #scale factor\n",
    "        end_x = int(center_x + sum_fx * scale)\n",
    "        end_y = int(center_y + sum_fy * scale)\n",
    "    else:\n",
    "        end_x, end_y = center_x, center_y\n",
    "    # Draw the sum vector as a red arrow\n",
    "    frame_with_vector = np.copy(frame) #copy original frame\n",
    "    cv2.arrowedLine(frame_with_vector, (center_x, center_y), (end_x, end_y), (0, 0, 255), 2, tipLength=0.2) \n",
    "    return frame_with_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c1bde9",
   "metadata": {},
   "source": [
    "## Farneack with Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5578f21",
   "metadata": {},
   "source": [
    "The function OFV take a list of frames to return and display the optical flow's vector fields as well as the sum of all vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ddb9a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFV(frames, step=15): #Frame list, window size\n",
    "    OF = [] #List of Optical flow's vector fields\n",
    "    i = 0 #Frame's index\n",
    "    while True:\n",
    "        #convert it to grayscale\n",
    "        prev_gray = cv2.cvtColor(np.copy(frames[i]), cv2.COLOR_BGR2GRAY)\n",
    "        next_gray = cv2.cvtColor(np.copy(frames[i+1]), cv2.COLOR_BGR2GRAY)\n",
    "        # Calculate the dense optical flow using Farneback method\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        # Draw the optical flow vectors on the frame\n",
    "        flow_frame = draw_optical_flow_vectors(flow, cv2.addWeighted(frames[i], 0.5, frames[i+1], 0.5, 0), step)\n",
    "        OF.append(flow) #Add flow to list\n",
    "        # Draw the sum vector on the frame\n",
    "        frame_with_sum_vector = draw_sum_vector(flow, flow_frame)\n",
    "        # Display the original frame with optical flow vectors and sum vector\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        #cv2.imshow('Optical Flow Vectors', flow_frame)\n",
    "        cv2.imshow('Sum Vector', frame_with_sum_vector)\n",
    "        key = cv2.waitKeyEx(0) #Read pressed key\n",
    "        if key == ord('q'): #Stop if pressed\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key for previous frames\n",
    "            i = i - 1\n",
    "            if i < 0: #prevent non existent frames\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key for next frames\n",
    "            i = i + 1\n",
    "            if i > (len(frames) - 2): #prevent non existent frames\n",
    "                i = len(frames) - 2\n",
    "    cv2.destroyAllWindows() #Close all windows\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2106515f",
   "metadata": {},
   "source": [
    "## Lucas-Kanade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b54e3",
   "metadata": {},
   "source": [
    "This function utilizes the Lukas-Kanade method to solve and display the optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3824b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFLK(frames, step_size=15): #Video Frames, window size\n",
    "    OF = [] #List of optical flow\n",
    "    lk_params=dict(winSize=(step_size,step_size),maxLevel=10,criteria=(cv2.TERM_CRITERIA_EPS|cv2.TERM_CRITERIA_COUNT,30,0.01))\n",
    "    #lucas-kanade parameters  #window Size        #PyramidLevel   #End Criteria\n",
    "    i = 0 #Frame index\n",
    "    while i < len(frames) - 1:\n",
    "        #Convert to Grayscale\n",
    "        old_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY) \n",
    "        frame_gray = cv2.cvtColor(frames[i + 1], cv2.COLOR_BGR2GRAY)\n",
    "        #Neighborhood grid centers\n",
    "        grid_y, grid_x = np.mgrid[step_size//2:old_gray.shape[0]:step_size, step_size//2:old_gray.shape[1]:step_size]\n",
    "        p0 = np.vstack((grid_x.ravel(), grid_y.ravel())).T.astype(np.float32).reshape(-1, 1, 2)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params) #Calculate Optical Flow\n",
    "        #Set to 1 if change has been found\n",
    "        good_new = p1[st == 1]\n",
    "        good_old = p0[st == 1]\n",
    "        #Frames average\n",
    "        frame = cv2.addWeighted(frames[i], 0.5, frames[i + 1], 0.5, 0)\n",
    "        #Calculate Vector Field\n",
    "        flow = np.zeros((old_gray.shape[0], old_gray.shape[1], 2))\n",
    "        flow[good_old[:, 1].astype(int), good_old[:, 0].astype(int), 0] = good_new[:, 0] - good_old[:, 0]\n",
    "        flow[good_old[:, 1].astype(int), good_old[:, 0].astype(int), 1] = good_new[:, 1] - good_old[:, 1]\n",
    "        #Draw Vectors\n",
    "        frame_with_vectors = draw_optical_flow_vectors(flow, frame, step_size)\n",
    "        frame_with_sum_vector = draw_sum_vector(flow, frame_with_vectors)\n",
    "        OF.append(flow) #add to flow list\n",
    "        #Display Frames and Flow\n",
    "        cv2.imshow('Previous Frame',frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        #cv2.imshow('Optical Flow Vectors', frame_with_vectors)\n",
    "        cv2.imshow('Sum Vector', frame_with_sum_vector)\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key to move backward\n",
    "            i = max(0, i - 1)\n",
    "        if key == 2555904:  # Right arrow key to move forward\n",
    "            i = min(len(frames) - 2, i + 1)\n",
    "    cv2.destroyAllWindows()\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d748f8bd",
   "metadata": {},
   "source": [
    "## Phase Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e0ab67",
   "metadata": {},
   "source": [
    "The PhaseC function calculates and displays the optical flow of the given frames by using Phase Correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5888eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_sum_vector_phase_c(sum_dx, sum_dy, frame): #Y component, X component, frame to drawn on\n",
    "    h, w = frame.shape[:2] #Size\n",
    "    center_x, center_y = w // 2, h // 2 # Calculate the center point of the frame\n",
    "    # Normalize the sum vector to fit within the image\n",
    "    max_length = min(w, h) // 2\n",
    "    vector_length = np.sqrt(sum_dx**2 + sum_dy**2)\n",
    "    if vector_length > 0:\n",
    "        scale = 0.1#max_length / vector_length #Scale factor\n",
    "        end_x = int(center_x + sum_dx * scale)\n",
    "        end_y = int(center_y + sum_dy * scale)\n",
    "    else:\n",
    "        end_x, end_y = center_x, center_y\n",
    "    # Draw the sum vector as a red arrow\n",
    "    frame_with_vector = np.copy(frame)\n",
    "    cv2.arrowedLine(frame_with_vector, (center_x, center_y), (end_x, end_y), (0, 0, 255), 2, tipLength=0.2)\n",
    "    return frame_with_vector\n",
    "\n",
    "def PhaseC(frames, block_size=15, grid_step=15):\n",
    "    OF = [] #Optical Flow List\n",
    "    i = 0 #Frame index\n",
    "    while True:\n",
    "        prev_frame = np.copy(frames[i])\n",
    "        next_frame = np.copy(frames[i + 1])\n",
    "\n",
    "        # Convert frames to grayscale\n",
    "        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "        next_gray = cv2.cvtColor(next_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Create an image to visualize the flow\n",
    "        flow_img = cv2.cvtColor(prev_gray, cv2.COLOR_GRAY2BGR)\n",
    "        frame = cv2.addWeighted(frames[i], 0.5, frames[i+1], 0.5, 0)\n",
    "\n",
    "        sum_dx, sum_dy = 0, 0\n",
    "\n",
    "        # Iterate over the grid\n",
    "        for y in range(0, prev_gray.shape[0] - block_size, grid_step):\n",
    "            for x in range(0, prev_gray.shape[1] - block_size, grid_step):\n",
    "                # Extract the blocks\n",
    "                prev_block = prev_gray[y:y + block_size, x:x + block_size]\n",
    "                next_block = next_gray[y:y + block_size, x:x + block_size]\n",
    "\n",
    "                # Compute phase correlation\n",
    "                shift, _ = cv2.phaseCorrelate(prev_block.astype(np.float32), next_block.astype(np.float32))\n",
    "                dx, dy = shift\n",
    "\n",
    "                # Sum the vectors\n",
    "                sum_dx += dx\n",
    "                sum_dy += dy\n",
    "\n",
    "                # Scale down the length of the arrows and size of the tips\n",
    "                scale = 1\n",
    "                tip_length = 0.2\n",
    "\n",
    "                # Draw the vector on the flow image\n",
    "                cv2.arrowedLine(frame, (x + block_size // 2, y + block_size // 2),\n",
    "                                (int(x + block_size // 2 + dx * scale), int(y + block_size // 2 + dy * scale)),\n",
    "                                (0, 255, 0), 1, tipLength=tip_length)\n",
    "\n",
    "        # Draw the sum vector on the frame\n",
    "        frame_with_sum_vector = draw_sum_vector_phase_c(sum_dx, sum_dy, frame)\n",
    "\n",
    "        OF.append(flow_img)\n",
    "\n",
    "        # Display the frames and the flow\n",
    "        cv2.imshow('Previous Frame', prev_frame)\n",
    "        cv2.imshow('Next Frame', next_frame)\n",
    "        #cv2.imshow('Optical Flow', frame)\n",
    "        cv2.imshow('Sum Vector', frame_with_sum_vector)\n",
    "\n",
    "        # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i < 0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i > (len(frames) - 2):\n",
    "                i = len(frames) - 2\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return OF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f26d78",
   "metadata": {},
   "source": [
    "# Real Time Optical Flow with Camera"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14394eb6",
   "metadata": {},
   "source": [
    "This function captures video from a camera and displays it with the optical flow using Furneback's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c692dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_optical_flow(frame, flow, step=16, max_magnitude=100):\n",
    "    h, w = frame.shape[:2]\n",
    "    y, x = np.mgrid[step//2:h:step, step//2:w:step].reshape(2, -1).astype(int)\n",
    "    fx, fy = flow[y, x].T\n",
    "\n",
    "    lines = np.vstack([x, y, x + fx, y + fy]).T.reshape(-1, 2, 2)\n",
    "    lines = np.int32(lines + 0.5)\n",
    "\n",
    "    vis = frame.copy()\n",
    "    for (x1, y1), (x2, y2) in lines:\n",
    "        cv2.arrowedLine(vis, (x1, y1), (x2, y2), (0, 255, 0), 1, tipLength=0.4)\n",
    "\n",
    "    # Compute the sum of all flow vectors\n",
    "    sum_fx = np.sum(fx)\n",
    "    sum_fy = np.sum(fy)\n",
    "    \n",
    "    # Normalize the sum vector\n",
    "    norm = np.sqrt(sum_fx**2 + sum_fy**2)\n",
    "    if norm > 0:\n",
    "        sum_fx /= norm\n",
    "        sum_fy /= norm\n",
    "\n",
    "    # Scale the normalized vector by the maximum magnitude\n",
    "    sum_fx *= min(norm, max_magnitude)\n",
    "    sum_fy *= min(norm, max_magnitude)\n",
    "    \n",
    "    # Draw the red vector representing the normalized sum of all flow vectors\n",
    "    center_x, center_y = w // 2, h // 2\n",
    "    end_x = int(center_x + sum_fx)  # Scale for better visualization\n",
    "    end_y = int(center_y + sum_fy)\n",
    "    cv2.arrowedLine(vis, (center_x, center_y), (end_x, end_y), (0, 0, 255), 2, tipLength=0.4)\n",
    "    \n",
    "    return vis\n",
    "\n",
    "def cap_of():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read initial frame.\")\n",
    "        return\n",
    "    \n",
    "    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Could not read frame.\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, \n",
    "                                            0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        \n",
    "        prev_gray = gray\n",
    "\n",
    "        # Draw the optical flow vectors on the RGB frame\n",
    "        optical_flow_frame = draw_optical_flow(frame, flow)\n",
    "        \n",
    "        # Display the resulting frame with optical flow\n",
    "        cv2.imshow('Webcam Video with Optical Flow', optical_flow_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1eb984b",
   "metadata": {},
   "source": [
    "# Frames Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc418e",
   "metadata": {},
   "source": [
    "This function takes a list of frames of a video and calculates the absolute diffences between two consecutive frames with a threshold to display a binary image with the regions where the most differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6715d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_dif(frames, threshold=30): #List of frames, threshold\n",
    "    D = [] #Differences List\n",
    "    i = 0 #Frame index\n",
    "    while True:\n",
    "        # Convert frames to grayscale\n",
    "        prev_gray = cv2.cvtColor(np.copy(frames[i]),cv2.COLOR_BGR2GRAY)\n",
    "        curr_gray = cv2.cvtColor(np.copy(frames[i+1]), cv2.COLOR_BGR2GRAY)\n",
    "        # Compute absolute difference between frames\n",
    "        diff = cv2.absdiff(prev_gray, curr_gray) \n",
    "        # Create a mask to highlight pixels with significant changes\n",
    "        mask = np.zeros_like(diff)\n",
    "        mask[diff > threshold] = 255 #Apply Threshold\n",
    "        D.append(mask) #Add to list\n",
    "        # Show original frame and mask\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        cv2.imshow('Pixels with Most Changes', mask)\n",
    "         # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-2):\n",
    "                i = len(frames)-2\n",
    "        prev_frame = frames[i]\n",
    "    # Release the video capture object\n",
    "    cv2.destroyAllWindows()\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf2efd",
   "metadata": {},
   "source": [
    "This function takes a list of frames and calculates the normalized absolute difference between 2 consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01842186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_dif1(frames): #List of frames, threshold\n",
    "    D = [] #Differences List\n",
    "    i = 0 #Frame index\n",
    "    while True:\n",
    "        # Convert frames to grayscale\n",
    "        prev_gray = cv2.cvtColor(np.copy(frames[i]), cv2.COLOR_BGR2GRAY)\n",
    "        curr_gray = cv2.cvtColor(np.copy(frames[i+1]), cv2.COLOR_BGR2GRAY)\n",
    "        # Compute absolute difference between frames\n",
    "        diff = abs(prev_gray-curr_gray) #Absolute difference\n",
    "        dif = (1/diff.max())*diff*255 #Normalize\n",
    "        D.append(diff) #Add to list\n",
    "        # Show original frame and mask\n",
    "        cv2.imshow('Prev Frame', frames[i])\n",
    "        cv2.imshow('Next Frame', frames[i+1])\n",
    "        cv2.imshow('Pixels with Most Changes', diff)\n",
    "         # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-2):\n",
    "                i = len(frames)-2\n",
    "        prev_frame = frames[i]\n",
    "    # Release the video capture object\n",
    "    cv2.destroyAllWindows()\n",
    "    return D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7645be",
   "metadata": {},
   "source": [
    "# Alter Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc677c89",
   "metadata": {},
   "source": [
    "The next fucntions are used to alter the frames in a video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a2171",
   "metadata": {},
   "source": [
    "## Change Color Channel's Ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da602e",
   "metadata": {},
   "source": [
    "This function changes the range of each individual color channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9a57c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_range_colors(image, min_vals=(0, 0, 0), max_vals=(255, 255, 255)):\n",
    "    # Split the image into its BGR channels\n",
    "    b, g, r = cv2.split(image)\n",
    "    # Clip each channel to its respective range\n",
    "    b = np.clip(b, min_vals[0], max_vals[0])\n",
    "    g = np.clip(g, min_vals[1], max_vals[1])\n",
    "    r = np.clip(r, min_vals[2], max_vals[2])\n",
    "    # Merge the channels back together\n",
    "    new_image = cv2.merge((b, g, r))\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0411859a",
   "metadata": {},
   "source": [
    "## Add Occlusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f85e736",
   "metadata": {},
   "source": [
    "This function adds occlusions in the shape of rectangles and/or circles into the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd254701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occlusions(image, num_occlusions=1,loc=[],sizes=[],shapes=['rectangle','circle'], colors=(-1,-1,-1)):    \n",
    "    output_image = np.copy(image) #Copy of frame\n",
    "    height, width = image.shape[:2] #Gets size\n",
    "    num_occlusions = num_occlusions if len(loc)==0 else len(loc) #number of occlusions\n",
    "    # Draw occlusions on the image\n",
    "    for i in range(num_occlusions):\n",
    "        shape_type = random.choice(shapes)\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) if colors==(-1,-1,-1) else colors  # Random color\n",
    "        if shape_type == 'rectangle':\n",
    "            x = random.randint(0, width - 1) if len(loc)==0 else loc[i][0]\n",
    "            y = random.randint(0, height - 1) if len(loc)==0 else loc[i][1]\n",
    "            width_rect = random.randint(5, width//2) if len(sizes)==0 else loc[i][0]\n",
    "            height_rect = random.randint(5, height//2) if len(sizes)==0 else loc[i][1]\n",
    "            cv2.rectangle(output_image, (x, y), (x + width_rect, y + height_rect), color, -1)  # Filled rectangle\n",
    "        elif shape_type == 'circle':\n",
    "            center = (random.randint(0, width - 1), random.randint(0, height - 1)) if len(loc)==0 else (loc[i][0],loc[i][1])\n",
    "            radius = 50#random.randint(5, 100)\n",
    "            cv2.circle(output_image, center, radius, color, -1)  # Filled circle\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2c089",
   "metadata": {},
   "source": [
    "## Draw Random Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c6750",
   "metadata": {},
   "source": [
    "This functions draws straight lines in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e40c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnd_lines(image, num_lines):\n",
    "    # Get the dimensions of the image\n",
    "    height, width = image.shape[:2]\n",
    "    # Copy the image to avoid modifying the original\n",
    "    output_image = image.copy()\n",
    "    for _ in range(num_lines):\n",
    "        # Generate random start point\n",
    "        start_point = (random.randint(0, width-1), random.randint(0, height-1))\n",
    "        # Generate a random angle and length for the line\n",
    "        angle = random.uniform(0, 2 * np.pi)\n",
    "        length = random.randint(1, min(width, height) // 2)  # Limit length to half of the smallest dimension\n",
    "        # Calculate the end point using the angle and length\n",
    "        end_point = (int(start_point[0] + length * np.cos(angle)), \n",
    "                     int(start_point[1] + length * np.sin(angle)))\n",
    "        # Ensure the end point is within the image boundaries\n",
    "        end_point = (min(max(end_point[0], 0), width-1), min(max(end_point[1], 0), height-1))\n",
    "        # Generate a random color (BGR format)\n",
    "        color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "        # Generate a random thickness for the line\n",
    "        thickness = random.randint(1, 10)\n",
    "        # Draw the line on the image\n",
    "        cv2.line(output_image, start_point, end_point, color, thickness)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9f36d9",
   "metadata": {},
   "source": [
    "## Random Region's Color Change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dd48e9",
   "metadata": {},
   "source": [
    "This function changes the color channel's range in a certain number of rectangle or ellipse regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e823a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnd_regions(image, num_regions):\n",
    "    # Get the dimensions of the image\n",
    "    height, width = image.shape[:2]\n",
    "    # Copy the image to avoid modifying the original\n",
    "    output_image = image.copy()\n",
    "    for _ in range(num_regions):\n",
    "        # Generate random region shape and size\n",
    "        region_shape = random.choice(['rectangle', 'ellipse'])\n",
    "        if region_shape == 'rectangle':\n",
    "            region_width = random.randint(10, width // 3)\n",
    "            region_height = random.randint(10, height // 3)\n",
    "            top_left_x = random.randint(0, width - region_width)\n",
    "            top_left_y = random.randint(0, height - region_height)\n",
    "            # Define the region\n",
    "            region = output_image[top_left_y:top_left_y + region_height, top_left_x:top_left_x + region_width]\n",
    "        elif region_shape == 'ellipse':\n",
    "            center_x = random.randint(width // 3, width - width // 3)\n",
    "            center_y = random.randint(height // 3, height - height // 3)\n",
    "            axis_length = (random.randint(10, width // 3), random.randint(10, height // 3))\n",
    "            angle = random.randint(0, 360)\n",
    "            start_angle = 0\n",
    "            end_angle = 360\n",
    "            # Create a mask for the ellipse\n",
    "            mask = np.zeros((height, width), dtype=np.uint8)\n",
    "            cv2.ellipse(mask, (center_x, center_y), axis_length, angle, start_angle, end_angle, 255, -1)\n",
    "            # Extract the region using the mask\n",
    "            region = cv2.bitwise_and(output_image, output_image, mask=mask)\n",
    "        # Change color channels within the region\n",
    "        for channel in range(3):  # Assuming BGR format\n",
    "            # Generate random ranges for the color channel\n",
    "            low = random.randint(0, 255)\n",
    "            high = random.randint(low, 255)\n",
    "            if region_shape == 'rectangle':\n",
    "                region[..., channel] = np.clip(region[..., channel], low, high)\n",
    "            elif region_shape == 'ellipse':\n",
    "                # Apply changes to the region using the mask\n",
    "                channel_region = output_image[..., channel]\n",
    "                channel_region[mask == 255] = np.clip(channel_region[mask == 255], low, high)\n",
    "                output_image[..., channel] = channel_region\n",
    "        if region_shape == 'rectangle':\n",
    "            # Place the modified region back into the image for rectangles\n",
    "            output_image[top_left_y:top_left_y + region_height, top_left_x:top_left_x + region_width] = region\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354c305e",
   "metadata": {},
   "source": [
    "# Simple Image Cartoonization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2883d10",
   "metadata": {},
   "source": [
    "The next functions are used to cartoonize an image pixel wise by using different algorithms and tecniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8551771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartoonize_image(img, k=10, it = 10, t1 = 150, t2 = 255, ks = 1 , kc=1):\n",
    "    # Apply bilateral filter to smooth the image\n",
    "    img_color = cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    # Convert to grayscale\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    # Apply Gaussian Blur\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (kc, kc), 0)\n",
    "    # Detect edges using Canny edge detection\n",
    "    edges = cv2.Canny(img_blur, threshold1=t1, threshold2=t2)\n",
    "    # Dilate the edges to make them more prominent\n",
    "    kernel = np.ones((ks, ks), np.uint8)\n",
    "    edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "    # Invert the edges\n",
    "    edges = cv2.bitwise_not(edges)\n",
    "    # Convert edges back to color, so we can combine with color image\n",
    "    edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "    # Perform K-means clustering\n",
    "    img_data = np.float32(img_color).reshape((-1, 3))\n",
    "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, it, 0.2)\n",
    "    _, labels, centers = cv2.kmeans(img_data, k, None, criteria, 10, cv2.KMEANS_RANDOM_CENTERS)\n",
    "    centers = np.uint8(centers)\n",
    "    img_clustered = centers[labels.flatten()]\n",
    "    img_clustered = img_clustered.reshape(img_color.shape)\n",
    "    # Combine edge and clustered image\n",
    "    cartoon = cv2.bitwise_and(img_clustered, edges_colored)\n",
    "    return cartoon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c559a4",
   "metadata": {},
   "source": [
    "# Image Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2123486",
   "metadata": {},
   "source": [
    "The following functions calculate different variability indexes of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f90d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_luminance(image):\n",
    "    b, g, r = cv2.split(image)\n",
    "    luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b\n",
    "    #return np.mean(luminance)\n",
    "    return luminance.sum()/(image.shape[0]*image.shape[1]*255)\n",
    "\n",
    "def color_variance(image):\n",
    "    variance_b = np.var(image[:, :, 0])\n",
    "    variance_g = np.var(image[:, :, 1])\n",
    "    variance_r = np.var(image[:, :, 2])\n",
    "    return ((variance_b+variance_g+variance_r)/3)/(image.shape[0]*image.shape[1])\n",
    "\n",
    "def calculate_variability(image,kernel_size=3):\n",
    "    kernel = np.ones((kernel_size,kernel_size))/(kernel_size**2-1)\n",
    "    kernel[kernel_size//2,kernel_size//2] = -1\n",
    "    V = cv2.filter2D(image,-1,kernel)\n",
    "    return abs(V).sum()/image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef5e19",
   "metadata": {},
   "source": [
    "# Color Limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "10133d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_limit(img,N_colors=20):\n",
    "    # Convert BGR (OpenCV format) to RGB (PIL format)\n",
    "    image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # Convert NumPy array (RGB) to PIL Image\n",
    "    pil_image = Image.fromarray(image_rgb)\n",
    "    # Apply the quantize method in PIL\n",
    "    quantized_image = pil_image.quantize(N_colors)\n",
    "    # Convert the quantized PIL image back to NumPy array (RGB format)\n",
    "    image_rgb_back = np.array(quantized_image.convert('RGB'))  # Convert quantized image back to RGB\n",
    "    #Convert RGB back to BGR for displaying with OpenCV\n",
    "    image_bgr_back = cv2.cvtColor(image_rgb_back, cv2.COLOR_RGB2BGR)\n",
    "    return image_bgr_back"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c51375",
   "metadata": {},
   "source": [
    "# Pallete Applier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f42b3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [\n",
    "    (0, 0, 0),   # Black\n",
    "    (255, 255, 255),  # White\n",
    "    (255, 0, 0),   # Red\n",
    "    (0, 255, 0),   # Green\n",
    "    (0, 0, 255),   # Blue\n",
    "    # Add more colors as needed\n",
    "]\n",
    "\n",
    "def apply_palette(img, palette):\n",
    "    img = img.convert(\"RGB\")\n",
    "    palette_img = Image.new(\"P\", (1, 1))\n",
    "    palette_img.putpalette(sum(palette, ()))\n",
    "    return cv2.cvtColor(np.array(img.quantize(palette=palette_img).convert(\"RGB\")),cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e26f2d0",
   "metadata": {},
   "source": [
    "# Add One Inconsistency to Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f473c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOneInc(F):\n",
    "    Bin = np.zeros(F[0].shape[:2],dtype=np.uint8)\n",
    "    I = F.copy()\n",
    "    x,y = random.randint(0,2*I.shape[0]//3),random.randint(0,2*I.shape[1]//3)\n",
    "    l = random.randint(20,I.shape[1]//5)\n",
    "    Op = random.randint(0,3)\n",
    "    if Op==0:\n",
    "        I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(random.randint(30,140),random.randint(30,140),random.randint(30,140)),(random.randint(150,255),random.randint(150,255),random.randint(150,255)))\n",
    "        Bin[x:x+l,y:y+l]=255\n",
    "    elif Op==1:\n",
    "        R,G,B = random.randint(0,255),random.randint(0,255),random.randint(0,255)\n",
    "        I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(R,G,B),(R,G,B))\n",
    "        Bin[x:x+l,y:y+l]=255\n",
    "    elif Op==2:\n",
    "        Thick = random.randint(1,10)\n",
    "        l2 = random.randint(10,I.shape[1]//8)\n",
    "        I = cv2.line(I,(x,y),(x+l,y+l2),(random.randint(0,255),random.randint(0,255),random.randint(0,255)),Thick)\n",
    "        Bin = cv2.line(Bin,(x,y),(x+l,y+l),255,Thick)\n",
    "    else:\n",
    "        Thick = random.randint(1,10)\n",
    "        l2 = random.randint(10,I.shape[1]//8)\n",
    "        I = cv2.line(I,(x,y),(x+l,y+l2),(0,0,0),Thick)\n",
    "        Bin = cv2.line(Bin,(x,y),(x+l,y+l),255,Thick)\n",
    "    return I,Bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39a40d3",
   "metadata": {},
   "source": [
    "# Add Inconsistencies to Video Each N Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "128a8a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddInc(F,N=3):\n",
    "    Inc = []\n",
    "    Bin = [np.zeros(F[0].shape[:2],dtype=np.uint8) for _ in F]\n",
    "    for i in range(len(F)):\n",
    "        if i%N==0 and i!=0:\n",
    "            I = F[i].copy()\n",
    "            x,y = random.randint(0,2*I.shape[0]//3),random.randint(0,2*I.shape[1]//3)\n",
    "            print(I.shape)\n",
    "            l = random.randint(20,I.shape[1]//5)\n",
    "            Op = random.randint(0,3)\n",
    "            if Op==0:\n",
    "                I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(random.randint(30,140),random.randint(30,140),random.randint(30,140)),(random.randint(150,255),random.randint(150,255),random.randint(150,255)))\n",
    "                Bin[i][x:x+l,y:y+l]=255\n",
    "            elif Op==1:\n",
    "                R,G,B = random.randint(0,255),random.randint(0,255),random.randint(0,255)\n",
    "                I[x:x+l,y:y+l] = change_range_colors(I[x:x+l,y:y+l],(R,G,B),(R,G,B))\n",
    "                Bin[i][x:x+l,y:y+l]=255\n",
    "            elif Op==2:\n",
    "                Thick = random.randint(1,10)\n",
    "                l2 = random.randint(10,I.shape[1]//8)\n",
    "                I = cv2.line(I,(x,y),(x+l,y+l2),(random.randint(0,255),random.randint(0,255),random.randint(0,255)),Thick)\n",
    "                Bin[i] = cv2.line(Bin[i],(x,y),(x+l,y+l),255,Thick)\n",
    "            else:\n",
    "                Thick = random.randint(1,10)\n",
    "                l2 = random.randint(10,I.shape[1]//8)\n",
    "                I = cv2.line(I,(x,y),(x+l,y+l2),(0,0,0),Thick)\n",
    "                Bin[i] = cv2.line(Bin[i],(x,y),(x+l,y+l),255,Thick)\n",
    "            Inc.append(I)\n",
    "        else:\n",
    "            Inc.append(F[i])\n",
    "    return Inc,Bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00fd1f1",
   "metadata": {},
   "source": [
    "# Cartoonize Video using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "245a50d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartoonize_vid(F,t1 = 150, t2 = 255, ks = 1 , kc=1):\n",
    "    C = []\n",
    "    centers = np.uint8(np.asarray([[r,g,b] for r in range(0,255,40) for g in range(0,255,40) for b in range(0,255,40)]))\n",
    "    It = 0\n",
    "    for f in F:\n",
    "        It = It+1\n",
    "        print(It,'/',len(F),end='\\r')\n",
    "        img_color = cv2.bilateralFilter(f, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "        # Convert to grayscale\n",
    "        img_gray = cv2.cvtColor(f, cv2.COLOR_RGB2GRAY)\n",
    "        # Apply Gaussian Blur\n",
    "        img_blur = cv2.GaussianBlur(img_gray, (kc, kc), 0)\n",
    "        # Detect edges using Canny edge detection\n",
    "        edges = cv2.Canny(img_blur, threshold1=t1, threshold2=t2)\n",
    "        # Dilate the edges to make them more prominent\n",
    "        kernel = np.ones((ks, ks), np.uint8)\n",
    "        edges = cv2.dilate(edges, kernel, iterations=1)\n",
    "        # Invert the edges\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "        # Convert edges back to color, so we can combine with color image\n",
    "        edges_colored = cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB)\n",
    "        # Perform K-means clustering\n",
    "        img_data = np.float32(img_color).reshape((-1, 3))\n",
    "\n",
    "        distances = np.linalg.norm(img_data[:, np.newaxis] - centers, axis=2)\n",
    "        closest_clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Map the pixels in the second image to the closest cluster centers\n",
    "        segmented_pixels2 = centers[closest_clusters]\n",
    "        segmented_image2 = segmented_pixels2.reshape(f.shape)\n",
    "\n",
    "        # Combine edge and clustered image\n",
    "        cartoon = cv2.bitwise_and(segmented_image2, edges_colored)\n",
    "        C.append(cartoon)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09cac79",
   "metadata": {},
   "source": [
    "# Canny Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e3ac939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canny_edge(image,tl=150,th=255,inverted=True):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Canny edge detector\n",
    "    edges = cv2.Canny(gray_image, threshold1=tl, threshold2=th)\n",
    "    # Invert the binary image (0 becomes 255, and 255 becomes 0)\n",
    "    if inverted:\n",
    "        edges = cv2.bitwise_not(edges)\n",
    "    return edges.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6327bbf6",
   "metadata": {},
   "source": [
    "# Sobel Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac69c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sobel_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute the gradients using the Sobel operator\n",
    "    sobelx = cv2.Sobel(gray_image, cv2.CV_64F, 1, 0, ksize=3)  # Gradient in x-direction\n",
    "    sobely = cv2.Sobel(gray_image, cv2.CV_64F, 0, 1, ksize=3)  # Gradient in y-direction\n",
    "    # Calculate the magnitude of the gradient\n",
    "    magnitude = np.sqrt(sobelx**2 + sobely**2)\n",
    "    # Normalize to range 0 to 255 and convert to uint8\n",
    "    magnitude = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(magnitude, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee2e7cc",
   "metadata": {},
   "source": [
    "# Prewitt Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d7fa8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewitt_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Define Prewitt kernels\n",
    "    kernelx = np.array([[1, 0, -1], [1, 0, -1], [1, 0, -1]], dtype=int)\n",
    "    kernely = np.array([[1, 1, 1], [0, 0, 0], [-1, -1, -1]], dtype=int)\n",
    "    # Apply the Prewitt operator\n",
    "    prewittx = cv2.filter2D(gray_image, -1, kernelx)\n",
    "    prewitty = cv2.filter2D(gray_image, -1, kernely)\n",
    "    # Calculate the magnitude of the gradient\n",
    "    magnitude = np.sqrt(prewittx**2 + prewitty**2)\n",
    "    # Normalize to range 0 to 255 and convert to uint8\n",
    "    magnitude = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(magnitude, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67afa34",
   "metadata": {},
   "source": [
    "# Scharr Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "030f1c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scharr_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute the gradients using the Scharr operator\n",
    "    scharrx = cv2.Scharr(gray_image, cv2.CV_64F, 1, 0)  # Gradient in x-direction\n",
    "    scharry = cv2.Scharr(gray_image, cv2.CV_64F, 0, 1)  # Gradient in y-direction\n",
    "    # Calculate the magnitude of the gradient\n",
    "    magnitude = np.sqrt(scharrx**2 + scharry**2)\n",
    "    # Normalize to range 0 to 255 and convert to uint8\n",
    "    magnitude = np.uint8(255 * magnitude / np.max(magnitude))\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(magnitude, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d7530",
   "metadata": {},
   "source": [
    "# Laplacian Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56f38151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian_edge(image,tl=150,th=255):\n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Laplacian operator to find edges\n",
    "    laplacian_edges = cv2.Laplacian(gray_image, cv2.CV_64F)\n",
    "    # Convert to absolute values and normalize to range 0 to 255\n",
    "    laplacian_edges = cv2.convertScaleAbs(laplacian_edges)\n",
    "    # Convert to binary image (0 or 255) and invert it\n",
    "    _, binary_edges = cv2.threshold(laplacian_edges, tl, th, cv2.THRESH_BINARY_INV)\n",
    "    return binary_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efc1adc",
   "metadata": {},
   "source": [
    "# Read First N Frames from Videos in Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc1669f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_first_frames(directory,N):\n",
    "    video_frames = []  # Dictionary to store video names and their first 10 frames\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".mp4\"):  # Check for video file extensions\n",
    "            video_path = os.path.join(directory, filename)\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frames = []\n",
    "            frame_count = 0\n",
    "            while frame_count < N and cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break  # If the video ends before 10 frames\n",
    "                frames.append(frame)\n",
    "                frame_count += 1\n",
    "            cap.release()  # Release the video capture object\n",
    "            video_frames += frames  # Store the frames with the video name\n",
    "    return video_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c554db",
   "metadata": {},
   "source": [
    "# Calculate Gradients from 2 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ba8ec616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grads(img1, img2):\n",
    "    # Calculate the spatial gradients using Sobel operator\n",
    "    gx1 = cv2.Sobel(img1, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    gy1 = cv2.Sobel(img1, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    gx2 = cv2.Sobel(img2, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    gy2 = cv2.Sobel(img2, cv2.CV_64F, 0, 1, ksize=5)\n",
    "    # Calculate the temporal gradient\n",
    "    tg = cv2.absdiff(img2, img1)\n",
    "    # Normalize gradients for display purposes\n",
    "    gx1d = cv2.convertScaleAbs(gx1)\n",
    "    gy1d = cv2.convertScaleAbs(gy1)\n",
    "    gx2d = cv2.convertScaleAbs(gx2)\n",
    "    gy2d = cv2.convertScaleAbs(gy2)\n",
    "    tgd = cv2.convertScaleAbs(tg)\n",
    "    return gx1,gy1,gx2,gy2,tg,gx1d,gy1d,gx2d,gy2d,tgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393c27a",
   "metadata": {},
   "source": [
    "# Calculate Movement with Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cf60c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad_movement(img1,img2):\n",
    "    # Calculate gradients\n",
    "    gx1,gy1,gx2,gy2,tg,gx1d,gy1d,gx2d,gy2d,tgd = calculate_and_display_gradients(img1, img2)\n",
    "    # Calculate the magnitude and angle of optical flow\n",
    "    magnitude = cv2.magnitude(gx2,gy2)\n",
    "    angle = cv2.phase(gx2,gy2,angleInDegrees=True)\n",
    "    # Normalize magnitude for display purposes\n",
    "    magnitude_display = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    magnitude_display = cv2.convertScaleAbs(magnitude_display)\n",
    "    # Optional: Visualize flow direction using HSV color space\n",
    "    hsv = np.zeros((img1.shape[0], img1.shape[1], 3), dtype=np.uint8)\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 0] = angle / 2\n",
    "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    optical_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    return magnitude_display,optical_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d580e",
   "metadata": {},
   "source": [
    "# Calculate Frames Differences with 3 Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "40bd0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_differences(img1,img2):\n",
    "    # Convert images to RGB (OpenCV loads images in BGR format by default)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    # Convert images to grayscale\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate the absolute difference between the images\n",
    "    diff_rgb = np.abs(img1_rgb - img2_rgb)\n",
    "    diff_gray = np.abs(img1_gray.astype(np.float32) - img2_gray.astype(np.float32))\n",
    "    # Normalize the differences to [0, 255]\n",
    "    norm_diff_rgb = cv2.normalize(diff_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_diff_gray = cv2.normalize(diff_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Split the RGB difference into color channels\n",
    "    diff_r, diff_g, diff_b = cv2.split(norm_diff_rgb)\n",
    "    # Convert the single channel differences back to 3-channel images for display\n",
    "    diff_r_colored = cv2.merge([diff_r, np.zeros_like(diff_r), np.zeros_like(diff_r)])\n",
    "    diff_g_colored = cv2.merge([np.zeros_like(diff_g), diff_g, np.zeros_like(diff_g)])\n",
    "    diff_b_colored = cv2.merge([np.zeros_like(diff_b), np.zeros_like(diff_b), diff_b])\n",
    "    return diff_r_colored,diff_g_colored,diff_b_colored,norm_diff_gray.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715d3629",
   "metadata": {},
   "source": [
    "# Calculate Temporal Gradient for each Color Channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4bfb5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_temporal_gradient(img1, img2):\n",
    "    # Convert images to RGB (OpenCV loads images in BGR format by default)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    # Convert images to grayscale\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate the temporal gradient (difference) between the images\n",
    "    grad_rgb = np.abs(img2_rgb - img1_rgb)\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "    # Normalize the gradients to [0, 255]\n",
    "    norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    # Split the RGB gradient into color channels\n",
    "    grad_r, grad_g, grad_b = cv2.split(norm_grad_rgb)\n",
    "    # Convert the single channel gradients back to 3-channel images for display\n",
    "    grad_r_colored = cv2.merge([grad_r, np.zeros_like(grad_r), np.zeros_like(grad_r)])\n",
    "    grad_g_colored = cv2.merge([np.zeros_like(grad_g), grad_g, np.zeros_like(grad_g)])\n",
    "    grad_b_colored = cv2.merge([np.zeros_like(grad_b), np.zeros_like(grad_b), grad_b])\n",
    "    return grad_r_colored,grad_g_colored,grad_b_colored,norm_grad_gray.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64b7c5",
   "metadata": {},
   "source": [
    "# Temporal Consistency Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_temporal_consistency(video_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count < 2:\n",
    "        raise ValueError(\"Video must have at least two frames to calculate temporal consistency.\")\n",
    "    \n",
    "    # Initialize variables to accumulate error values\n",
    "    total_error_rgb = np.zeros(3)  # R, G, B channels\n",
    "    total_error_gray = 0\n",
    "    num_frames = 0\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Compute the absolute difference between consecutive frames\n",
    "        diff_rgb = np.abs(curr_frame_rgb.astype(np.float32) - prev_frame_rgb.astype(np.float32))\n",
    "        diff_gray = np.abs(curr_frame_gray.astype(np.float32) - prev_frame_gray.astype(np.float32))\n",
    "        \n",
    "        # Calculate the mean absolute error for the current frame pair\n",
    "        mean_error_rgb = np.mean(diff_rgb, axis=(0, 1))  # Mean error for R, G, B channels\n",
    "        mean_error_gray = np.mean(diff_gray)\n",
    "        \n",
    "        total_error_rgb += mean_error_rgb\n",
    "        total_error_gray += mean_error_gray\n",
    "        num_frames += 1\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Compute the average temporal consistency index\n",
    "    if num_frames == 0:\n",
    "        raise ValueError(\"No frames processed. Check the video file.\")\n",
    "    \n",
    "    temporal_consistency_index_rgb = total_error_rgb / num_frames\n",
    "    temporal_consistency_index_gray = total_error_gray / num_frames\n",
    "    \n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    return temporal_consistency_index_rgb, temporal_consistency_index_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def identify_abnormality(image1_path, image2_path):\n",
    "    # Read the images\n",
    "    img1 = cv2.imread(image1_path)\n",
    "    img2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both images could not be loaded. Check the file paths.\")\n",
    "    \n",
    "    # Calculate temporal gradients\n",
    "    grad_rgb, grad_gray = calculate_temporal_gradient(img1, img2)\n",
    "    \n",
    "    # Calculate similarity indexes\n",
    "    similarity_indexes = calculate_similarity_indexes(img1, img2)\n",
    "    \n",
    "    # Combine gradients and similarity indexes to identify abnormalities\n",
    "    # Normalize gradients for visibility\n",
    "    norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Create a mask where significant changes or low similarity occur\n",
    "    threshold = 50  # Set an appropriate threshold value\n",
    "    abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "    abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "    # Highlight the regions of interest\n",
    "    highlighted_rgb = img1.copy()\n",
    "    highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "    highlighted_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "    \n",
    "    # Display the results\n",
    "    cv2.imshow('Original Image 1', img1)\n",
    "    cv2.imshow('Original Image 2', img2)\n",
    "    cv2.imshow('Temporal Gradient (RGB)', norm_grad_rgb.astype(np.uint8))\n",
    "    cv2.imshow('Temporal Gradient (Gray)', norm_grad_gray.astype(np.uint8))\n",
    "    cv2.imshow('Abnormal Regions (RGB)', highlighted_rgb)\n",
    "    cv2.imshow('Abnormal Regions (Gray)', highlighted_gray)\n",
    "    \n",
    "    # Print similarity indexes\n",
    "    print(f'Similarity Indexes: {similarity_indexes}')\n",
    "    \n",
    "    # Wait until a key is pressed and close the windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "#image1_path = 'saved_frames/frame_000.png'\n",
    "#image2_path = 'saved_frames/frame_001.png'\n",
    "#identify_abnormality(image1_path, image2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using MP4 codec\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width * 2, frame_height * 2))  # Updated size for combined frames\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = rgb2gray(prev_frame_rgb)\n",
    "    \n",
    "    while True:\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = rgb2gray(curr_frame_rgb)\n",
    "        \n",
    "        # Calculate temporal gradients and similarity indexes\n",
    "        grad_rgb, grad_gray = calculate_temporal_gradient(prev_frame, curr_frame)\n",
    "        similarity_indexes = calculate_similarity_indexes(prev_frame, curr_frame)\n",
    "        \n",
    "        # Normalize gradients for visibility\n",
    "        norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Create masks for abnormalities\n",
    "        threshold = 50  # Set an appropriate threshold value\n",
    "        abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "        abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "        # Highlight the regions of interest\n",
    "        highlighted_rgb = curr_frame.copy()\n",
    "        highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "        highlighted_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "        \n",
    "        # Combine the results into a single frame\n",
    "        combined_frame = np.hstack((curr_frame, cv2.cvtColor(highlighted_rgb, cv2.COLOR_BGR2RGB)))\n",
    "        combined_frame = np.vstack((combined_frame, np.hstack((cv2.cvtColor(highlighted_gray, cv2.COLOR_GRAY2BGR), norm_grad_rgb.astype(np.uint8)))))\n",
    "        \n",
    "        # Write the combined frame to the output video\n",
    "        out.write(combined_frame)\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "#video_path = 'Cartoonized/U_toon.mp4'\n",
    "#output_path = 'output_abnormalities.mp4'\n",
    "#process_video(video_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35e035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def add_label(image, text, position=(10, 30), font_scale=1, color=(255, 255, 255), thickness=2):\n",
    "    \"\"\"Add a label to an image.\"\"\"\n",
    "    return cv2.putText(image, text, position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using MP4 codec\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width * 3, frame_height * 2))  # Updated size for combined frames\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = rgb2gray(prev_frame_rgb)\n",
    "    \n",
    "    while True:\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = rgb2gray(curr_frame_rgb)\n",
    "        \n",
    "        # Calculate temporal gradients and similarity indexes\n",
    "        grad_rgb, grad_gray = calculate_temporal_gradient(prev_frame, curr_frame)\n",
    "        similarity_indexes = calculate_similarity_indexes(prev_frame, curr_frame)\n",
    "        \n",
    "        # Normalize gradients for visibility\n",
    "        norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        \n",
    "        # Convert single-channel grayscale to 3-channel RGB for displaying\n",
    "        norm_grad_gray_rgb = cv2.cvtColor(norm_grad_gray, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Create masks for abnormalities\n",
    "        threshold = 50  # Set an appropriate threshold value\n",
    "        abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "        abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "        # Highlight the regions of interest\n",
    "        highlighted_rgb = curr_frame.copy()\n",
    "        highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "        highlighted_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "        \n",
    "        # Create the combined frame\n",
    "        combined_frame = np.zeros((frame_height * 2, frame_width * 3, 3), dtype=np.uint8)\n",
    "        combined_frame[0:frame_height, 0:frame_width] = prev_frame\n",
    "        combined_frame[0:frame_height, frame_width:frame_width*2] = curr_frame\n",
    "        combined_frame[0:frame_height, frame_width*2:frame_width*3] = highlighted_rgb\n",
    "        combined_frame[frame_height:frame_height*2, 0:frame_width] = cv2.cvtColor(highlighted_gray, cv2.COLOR_GRAY2BGR)\n",
    "        combined_frame[frame_height:frame_height*2, frame_width:frame_width*2] = norm_grad_rgb\n",
    "        combined_frame[frame_height:frame_height*2, frame_width*2:frame_width*3] = norm_grad_gray_rgb\n",
    "        \n",
    "        # Add labels to the combined frame\n",
    "        combined_frame = add_label(combined_frame, \"Previous Frame\", position=(10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Current Frame\", position=(frame_width + 10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Highlighted Changes\", position=(frame_width * 2 + 10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Gray Gradient\", position=(10, frame_height + 30))\n",
    "        combined_frame = add_label(combined_frame, \"RGB Gradient\", position=(frame_width + 10, frame_height + 30))\n",
    "        combined_frame = add_label(combined_frame, \"Gray Gradient (Norm)\", position=(frame_width * 2 + 10, frame_height + 30))\n",
    "        \n",
    "        # Write the combined frame to the output video\n",
    "        out.write(combined_frame)\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame = curr_frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "#video_path = 'Cartoonized/U_toon.mp4'\n",
    "#output_path = 'output_abnormalities1.mp4'\n",
    "#process_video(video_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1e6ea",
   "metadata": {},
   "source": [
    "# Random Region Warping to image (Deformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2b1dcb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_deformation(image, max_movement=10, smoothness=50):\n",
    "    # Get the height and width of the image\n",
    "    h, w, _ = image.shape\n",
    "    # Generate random noise-based flow fields for X and Y directions\n",
    "    flow_X = np.random.uniform(-1, 1, (h, w)).astype(np.float32)\n",
    "    flow_Y = np.random.uniform(-1, 1, (h, w)).astype(np.float32)\n",
    "    # Smooth the noise to ensure consistency across regions\n",
    "    flow_X = gaussian_filter(flow_X, sigma=smoothness)\n",
    "    flow_Y = gaussian_filter(flow_Y, sigma=smoothness)\n",
    "    # Normalize the flow fields to the range of [-max_movement, max_movement]\n",
    "    flow_X = cv2.normalize(flow_X, None, -max_movement, max_movement, cv2.NORM_MINMAX)\n",
    "    flow_Y = cv2.normalize(flow_Y, None, -max_movement, max_movement, cv2.NORM_MINMAX)\n",
    "    # Generate a meshgrid of coordinates (X, Y)\n",
    "    X, Y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    # Create a map that distorts the image based on the smoothed optical flow\n",
    "    map_X = (X + flow_X).astype(np.float32)\n",
    "    map_Y = (Y + flow_Y).astype(np.float32)\n",
    "    # Warp the image using remap function\n",
    "    warped_image = cv2.remap(image, map_X, map_Y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681dbd9f",
   "metadata": {},
   "source": [
    "# Random Point Warping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f590a5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_point_warp(image, max_movement=1):\n",
    "    # Get the height and width of the image\n",
    "    h, w, _ = image.shape\n",
    "    # Generate a meshgrid of coordinates (X, Y)\n",
    "    X, Y = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    # Generate random optical flow for X and Y directions\n",
    "    flow_X = np.random.uniform(-max_movement, max_movement, (h, w)).astype(np.float32)\n",
    "    flow_Y = np.random.uniform(-max_movement, max_movement, (h, w)).astype(np.float32)\n",
    "    # Create a map that distorts the image based on the optical flow\n",
    "    map_X = (X + flow_X).astype(np.float32)\n",
    "    map_Y = (Y + flow_Y).astype(np.float32)\n",
    "    # Warp the image using remap function\n",
    "    warped_image = cv2.remap(image, map_X, map_Y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6753b3c",
   "metadata": {},
   "source": [
    "# Random Perspective, Rotation and Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "09fb7d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_PRR(image, max_shift=5, scale_range=(0.9, 1.1), max_angle=5):\n",
    "    # Get the height and width of the original image\n",
    "    h, w, _ = image.shape\n",
    "    # ----- Resizing -----\n",
    "    # Choose a random scale factor within the specified range\n",
    "    scale_factor = np.random.uniform(scale_range[0], scale_range[1])\n",
    "    # Compute new dimensions based on the scale factor\n",
    "    new_w, new_h = int(w * scale_factor), int(h * scale_factor)\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_w, new_h))\n",
    "    # ---- Rotation -----\n",
    "    # Choose a random angle for rotation (within max_angle degrees)\n",
    "    angle = np.random.uniform(-max_angle, max_angle)\n",
    "    # Get the rotation matrix (centered at the new image's center)\n",
    "    center = (new_w // 2, new_h // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    # Rotate the resized image\n",
    "    rotated_image = cv2.warpAffine(resized_image, rotation_matrix, (new_w, new_h))\n",
    "    # ----- Perspective Warp -----\n",
    "    # Define the original corner points of the rotated image\n",
    "    src_points = np.float32([[0, 0], [new_w, 0], [new_w, new_h], [0, new_h]])\n",
    "    # Generate small random shifts for each corner\n",
    "    dst_points = src_points + np.random.randint(-max_shift, max_shift, src_points.shape).astype(np.float32)\n",
    "    # Get the perspective transformation matrix\n",
    "    M = cv2.getPerspectiveTransform(src_points, dst_points)\n",
    "    # Apply the warp to the rotated image\n",
    "    warped_image = cv2.warpPerspective(rotated_image, M, (new_w, new_h))\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528be55",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9998583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Entropy(image):\n",
    "    # Convert the image to grayscale using manual conversion\n",
    "    gray_image = 0.299 * image[:, :, 2] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 0]\n",
    "    # Calculate the histogram of the grayscale image\n",
    "    hist, _ = np.histogram(gray_image, bins=256, range=(0, 256), density=True)\n",
    "    # Avoid using SciPy's entropy function, instead calculate it manually\n",
    "    hist = hist[hist > 0]  # Remove zero entries to avoid log(0)\n",
    "    hist_entropy = -np.sum(hist * np.log2(hist))\n",
    "    return hist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e555f25b",
   "metadata": {},
   "source": [
    "# TSNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def TSNR(image1, image2, image3):  \n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both images are invalid\")   \n",
    "    # Calculate the absolute difference manually\n",
    "    frame_diff = np.abs(image1.astype(np.float64) - image2.astype(np.float64))\n",
    "    # Calculate the mean and standard deviation of the difference\n",
    "    mean_diff = np.mean(frame_diff)\n",
    "    std_diff = np.std(frame_diff)\n",
    "    # Compute TSNR\n",
    "    tsnr = mean_diff / (std_diff + 1e-10)\n",
    "    return abs(1 - tsnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a222a0",
   "metadata": {},
   "source": [
    "## Absolute Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65995a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Abs_Dif(image1, image2, image3):\n",
    "    return np.mean(np.abs(image1 - image2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d12175e",
   "metadata": {},
   "source": [
    "# Optical Flow End Point Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6623cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF_EPE(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both images are invalid\")\n",
    "    # Manual grayscale conversion (approximating cv2.cvtColor)\n",
    "    gray1 = 0.299 * image1[:, :, 2] + 0.587 * image1[:, :, 1] + 0.114 * image1[:, :, 0]\n",
    "    gray2 = 0.299 * image2[:, :, 2] + 0.587 * image2[:, :, 1] + 0.114 * image2[:, :, 0]\n",
    "    # Placeholder for optical flow (since Farneback can't be done with NumPy)\n",
    "    # For real use, replace this with actual optical flow computation outside of Numba.\n",
    "    # Example: Use np.gradient or any other available method that approximates movement.\n",
    "    flow_x = np.gradient(gray2, axis=1)  # Approximation: gradient as optical flow\n",
    "    flow_y = np.gradient(gray2, axis=0)\n",
    "    flow = np.stack((flow_x, flow_y), axis=-1)\n",
    "    # EPE (End Point Error) calculation\n",
    "    epe = np.linalg.norm(flow, axis=2).mean()\n",
    "    return epe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af6d805",
   "metadata": {},
   "source": [
    "# Oprical Flow Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb1e48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF_AE(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both images are invalid\")\n",
    "    # Manual grayscale conversion (approximation of cv2.cvtColor)\n",
    "    gray1 = 0.299 * image1[:, :, 2] + 0.587 * image1[:, :, 1] + 0.114 * image1[:, :, 0]\n",
    "    gray2 = 0.299 * image2[:, :, 2] + 0.587 * image2[:, :, 1] + 0.114 * image2[:, :, 0] \n",
    "    # Placeholder for optical flow (since Farneback can't be done with NumPy)\n",
    "    # Example: Gradient approximation for optical flow\n",
    "    flow_x = np.gradient(gray2, axis=1)  # Approximation: gradient as optical flow\n",
    "    flow_y = np.gradient(gray2, axis=0)\n",
    "    u, v = flow_x, flow_y   \n",
    "    # Magnitude and angle calculation (approximation of cv2.cartToPolar)\n",
    "    magnitude = np.sqrt(u**2 + v**2)\n",
    "    angle = np.arctan2(v, u)  \n",
    "    # AE (Angular Error) calculation\n",
    "    ae = np.mean(np.abs(angle))  # Taking the mean absolute angle error  \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95ccb8f",
   "metadata": {},
   "source": [
    "# Optical Flow Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3857373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF(img1, img2, img3):\n",
    "    # Check if images are the same size\n",
    "    if img1.shape != img2.shape or img2.shape != img3.shape:\n",
    "        raise ValueError(\"All images must be of the same size\")\n",
    "    # Convert to grayscale using a simple weighted sum\n",
    "    edges1 = 0.299 * img1[:, :, 2] + 0.587 * img1[:, :, 1] + 0.114 * img1[:, :, 0]\n",
    "    edges2 = 0.299 * img2[:, :, 2] + 0.587 * img2[:, :, 1] + 0.114 * img2[:, :, 0]\n",
    "    edges3 = 0.299 * img3[:, :, 2] + 0.587 * img3[:, :, 1] + 0.114 * img3[:, :, 0]\n",
    "    # Approximate optical flow by using gradients (placeholder)\n",
    "    flow1_x = np.gradient(edges2, axis=1) - np.gradient(edges1, axis=1)\n",
    "    flow1_y = np.gradient(edges2, axis=0) - np.gradient(edges1, axis=0)\n",
    "    flow2_x = np.gradient(edges3, axis=1) - np.gradient(edges2, axis=1)\n",
    "    flow2_y = np.gradient(edges3, axis=0) - np.gradient(edges2, axis=0)\n",
    "    # Compute the magnitude and angle of the optical flow vectors for both flows\n",
    "    magnitude1 = np.sqrt(flow1_x**2 + flow1_y**2)\n",
    "    angle1 = np.arctan2(flow1_y, flow1_x)\n",
    "    magnitude2 = np.sqrt(flow2_x**2 + flow2_y**2)\n",
    "    angle2 = np.arctan2(flow2_y, flow2_x)\n",
    "    # Compute the difference between the two optical flows\n",
    "    magnitude_diff = np.abs(magnitude1 - magnitude2)\n",
    "    angle_diff = np.abs(angle1 - angle2)\n",
    "    # Normalize differences\n",
    "    magnitude_index = np.sum(magnitude_diff) / (magnitude1.size + 1e-6)  # Avoid division by zero\n",
    "    angle_index = np.sum(angle_diff) / (angle1.size + 1e-6)  # Avoid division by zero\n",
    "    # Combine magnitude and angle differences\n",
    "    difference_index = magnitude_index + angle_index\n",
    "    return difference_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe5793d",
   "metadata": {},
   "source": [
    "# Optical Flow Border Inconsistency Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF_B(img1, img2, img3):\n",
    "    # Check if images are the same size\n",
    "    if img1.shape != img2.shape or img2.shape != img3.shape:\n",
    "        raise ValueError(\"All images must be of the same size\")\n",
    "    # Simple edge detection approximation using gradients\n",
    "    def edge_detect(image):\n",
    "        dx = np.abs(np.gradient(image, axis=1))\n",
    "        dy = np.abs(np.gradient(image, axis=0))\n",
    "        return np.sqrt(dx**2 + dy**2)  \n",
    "    # Apply the edge detection approximation\n",
    "    edges1 = edge_detect(0.299 * img1[:, :, 2] + 0.587 * img1[:, :, 1] + 0.114 * img1[:, :, 0])\n",
    "    edges2 = edge_detect(0.299 * img2[:, :, 2] + 0.587 * img2[:, :, 1] + 0.114 * img2[:, :, 0])\n",
    "    edges3 = edge_detect(0.299 * img3[:, :, 2] + 0.587 * img3[:, :, 1] + 0.114 * img3[:, :, 0])\n",
    "    # Approximate optical flow by using gradients (placeholder)\n",
    "    flow1_x = np.gradient(edges2, axis=1) - np.gradient(edges1, axis=1)\n",
    "    flow1_y = np.gradient(edges2, axis=0) - np.gradient(edges1, axis=0)\n",
    "    flow2_x = np.gradient(edges3, axis=1) - np.gradient(edges2, axis=1)\n",
    "    flow2_y = np.gradient(edges3, axis=0) - np.gradient(edges2, axis=0)\n",
    "    # Compute the magnitude and angle of the optical flow vectors for both flows\n",
    "    magnitude1 = np.sqrt(flow1_x**2 + flow1_y**2)\n",
    "    angle1 = np.arctan2(flow1_y, flow1_x)\n",
    "    magnitude2 = np.sqrt(flow2_x**2 + flow2_y**2)\n",
    "    angle2 = np.arctan2(flow2_y, flow2_x)\n",
    "    # Compute the difference between the two optical flows\n",
    "    magnitude_diff = np.abs(magnitude1 - magnitude2)\n",
    "    angle_diff = np.abs(angle1 - angle2)\n",
    "    # Normalize differences\n",
    "    magnitude_index = np.sum(magnitude_diff) / (magnitude1.size + 1e-6)  # Avoid division by zero\n",
    "    angle_index = np.sum(angle_diff) / (angle1.size + 1e-6)  # Avoid division by zero\n",
    "    # Combine magnitude and angle differences\n",
    "    difference_index = magnitude_index + angle_index\n",
    "    return difference_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8676acf",
   "metadata": {},
   "source": [
    "# Gray Scale Absolute Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151f2e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Gray_Dif(image1, image2, image3):\n",
    "    def rgb_to_gray(image):\n",
    "        return (image[:, :, 2] + image[:, :, 1] + image[:, :, 0])/3\n",
    "    gray1 = rgb_to_gray(image1)\n",
    "    gray2 = rgb_to_gray(image2)\n",
    "    # Calculate the absolute difference between the two grayscale images\n",
    "    diff = np.abs(gray1 - gray2)\n",
    "    # Return the mean of the difference\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086484e",
   "metadata": {},
   "source": [
    "# Temporal Structural Similarity Index Measure for Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7834bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def TSSIM(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    ssim_value = ssim(gray1, gray2,multichannel=True,win_size=3)\n",
    "    return abs(1-ssim_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57414cac",
   "metadata": {},
   "source": [
    "# Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c800a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def MSE(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    # Convert images to grayscale\n",
    "    def rgb_to_gray(image):\n",
    "        return 0.299 * image[:, :, 2] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 0]\n",
    "    gray1 = rgb_to_gray(image1)\n",
    "    gray2 = rgb_to_gray(image2)\n",
    "    # Compute the Mean Squared Error\n",
    "    mse = np.mean((gray1 - gray2) ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795548de",
   "metadata": {},
   "source": [
    "# Border Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9f4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Border_Err(image1, image2, image3):\n",
    "    lower_threshold = 50\n",
    "    upper_threshold = 255\n",
    "    if len(image1.shape) > 1:\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    if len(image2.shape) > 1:\n",
    "        image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Canny edge detection\n",
    "    canny1 = cv2.Canny(image1, lower_threshold, upper_threshold)\n",
    "    canny2 = cv2.Canny(image2, lower_threshold, upper_threshold)\n",
    "    # Calculate the absolute difference between the two Canny images\n",
    "    abs_diff = cv2.absdiff(canny1, canny2)\n",
    "    return np.mean(abs_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d367ba",
   "metadata": {},
   "source": [
    "# Color Range Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e1d081",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def CRC(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")    \n",
    "    # Convert images to float64\n",
    "    image1 = image1.astype(np.float64)\n",
    "    image2 = image2.astype(np.float64)  \n",
    "    # Calculate differences for each channel\n",
    "    def channel_diff(image1, image2, channel):\n",
    "        min1 = np.min(image1[:,:,channel])\n",
    "        max1 = np.max(image1[:,:,channel])\n",
    "        min2 = np.min(image2[:,:,channel])\n",
    "        max2 = np.max(image2[:,:,channel])\n",
    "        diff_min = abs(min1 - min2)\n",
    "        diff_max = abs(max1 - max2)\n",
    "        return diff_min, diff_max   \n",
    "    diff_r_min, diff_r_max = channel_diff(image1, image2, 2)  # Red channel\n",
    "    diff_g_min, diff_g_max = channel_diff(image1, image2, 1)  # Green channel\n",
    "    diff_b_min, diff_b_max = channel_diff(image1, image2, 0)  # Blue channel   \n",
    "    # Calculate CRC index\n",
    "    crci = (diff_r_min + diff_r_max + diff_g_min + diff_g_max + diff_b_min + diff_b_max) / 6    \n",
    "    return crci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e089f1f3",
   "metadata": {},
   "source": [
    "# Entropy Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b496b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.numba.jit\n",
    "def Entropy_Dif(image1,image2, image3):\n",
    "    return abs(Entropy(image1)-Entropy(image2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3960e",
   "metadata": {},
   "source": [
    "# Frequency Magnitude Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de535cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Freq_Dif(img1, img2, img3):\n",
    "    # Check if images are the same size\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Images must be of the same size\")  \n",
    "    # Compute FFT\n",
    "    f1 = np.fft.fft2(img1)\n",
    "    f2 = np.fft.fft2(img2)  \n",
    "    # Shift zero frequency component to the center\n",
    "    f1_shifted = np.fft.fftshift(f1)\n",
    "    f2_shifted = np.fft.fftshift(f2)  \n",
    "    # Compute magnitude spectrum\n",
    "    magnitude1 = np.abs(f1_shifted)\n",
    "    magnitude2 = np.abs(f2_shifted)  \n",
    "    # Compute the difference in magnitude spectra\n",
    "    magnitude_diff = np.abs(magnitude1 - magnitude2)\n",
    "    # Compute the difference index\n",
    "    sum_magnitude1 = np.sum(magnitude1)\n",
    "    sum_magnitude2 = np.sum(magnitude2)\n",
    "    sum_magnitude_diff = np.sum(magnitude_diff)\n",
    "    difference_index = sum_magnitude_diff / (sum_magnitude1 + sum_magnitude2 + 1e-6)  # Avoid division by zero\n",
    "    return difference_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7860d97",
   "metadata": {},
   "source": [
    "# Combined All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f7ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Combined_Metrics(image1, image2,op):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    tsnr = TSNR(image1,image2)\n",
    "    adif = Abs_Dif(image1,image2)\n",
    "    epe = OF_EPE(image1,image2)\n",
    "    ae = OF_AE(image1,image2)\n",
    "    gray = Gray_Dif(image1,image2)\n",
    "    ssim_value = TSSIM(image1,image2)\n",
    "    mse_value = MSE(image1,image2)\n",
    "    border_consistency_value = Border_Err(image1,image2)\n",
    "    crci_value = CRC(image1,image2)\n",
    "    ent = Entropy_Dif(image1,image2)\n",
    "    # Normalize and combine the metrics into a single consistency index\n",
    "    metrics = np.array([tsnr,adif, epe, ae, gray,ssim_value, mse_value, border_consistency_value, crci_value, ent])\n",
    "    normalized_metrics = (metrics - np.min(metrics)) / (np.max(metrics) - np.min(metrics))\n",
    "    combined_consistency_index = np.mean(normalized_metrics)\n",
    "    #print(metrics)\n",
    "    # Z-score normalization\n",
    "    mean = np.mean(metrics)\n",
    "    std = np.std(metrics)\n",
    "    Z_metrics = (metrics - mean) / std\n",
    "    \n",
    "    if op==\"norm\":\n",
    "        return combined_consistency_index\n",
    "    if op==\"mean\":\n",
    "        return np.mean(metrics)\n",
    "    if op==\"log\":\n",
    "        metrics[metrics <= 0] = 1e-10\n",
    "        return np.sum(np.log(metrics))\n",
    "    if op==\"Z\":\n",
    "        return np.mean(Z_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054f5ff4",
   "metadata": {},
   "source": [
    "# Mixed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7888d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Mix_Metrics(image1, image2,image3,op=\"mean\",M = [TSSIM,MSE],W=[1,1]):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    metrics = []\n",
    "    for i in range(len(M)):\n",
    "        #print(image1.shape,image2.shape)\n",
    "        metrics.append(W[i]*M[i](image1,image2,image3))\n",
    "    # Normalize and combine the metrics into a single consistency index\n",
    "    metrics = np.array(metrics)\n",
    "    normalized_metrics = (metrics - np.min(metrics)) / (np.max(metrics) - np.min(metrics))\n",
    "    combined_consistency_index = np.mean(normalized_metrics)\n",
    "    #print(metrics)\n",
    "    # Z-score normalization\n",
    "    mean = np.mean(metrics)\n",
    "    std = np.std(metrics)\n",
    "    Z_metrics = (metrics - mean) / std\n",
    "    \n",
    "    if op==\"norm\":\n",
    "        return combined_consistency_index\n",
    "    if op==\"mean\":\n",
    "        return np.mean(metrics)\n",
    "    if op==\"log\":\n",
    "        metrics[metrics <= 0] = 1e-10\n",
    "        return np.sum(np.log(metrics))\n",
    "    if op==\"Z\":\n",
    "        return np.mean(Z_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774ff69",
   "metadata": {},
   "source": [
    "# Windowed Max Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7690ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WMax_Inconsistency(img1, img2, img3, wsize=(3,3), step=(3,3), Func=Combined_Metrics, op=\"mean\",M=[],Weights=[]):\n",
    "    kw,kh = wsize\n",
    "    H,W,_ = img1.shape\n",
    "    sw,sh = step\n",
    "    maxC = 0\n",
    "    maxR = [0,0,kw,kh]\n",
    "    # Calculate the output dimensions\n",
    "    output_height = (H - kh) // sh + 1\n",
    "    output_width = (W - kw) // sw + 1\n",
    "    # Initialize the output array\n",
    "    result = np.zeros((output_height, output_width))\n",
    "    # Perform the convolution\n",
    "    for i in range(0, output_height*sh, sh):\n",
    "        #print('Row: ',i,'/',output_height*sh,end='\\r')\n",
    "        for j in range(0, output_width*sw, sw):\n",
    "            # Extract the region of interest\n",
    "            region1 = img1[i:i + kh, j:j + kw]\n",
    "            region2 = img2[i:i + kh, j:j + kw]\n",
    "            region3 = img3[i:i + kh, j:j + kw]\n",
    "            # Perform element-wise multiplication and sum the result\n",
    "            if Func==Combined_Metrics or Func==Mix_Metrics:\n",
    "                if len(M)>0 and Func==Mix_Metrics:\n",
    "                    result[i // sh, j // sw] = Func(region1, region2, region3,op,M,Weights)\n",
    "                else:\n",
    "                    result[i // sh, j // sw] = Func(region1, region2,region3,op)\n",
    "            else:\n",
    "                result[i // sh, j // sw] = Func(region1, region2,region3)\n",
    "            if result[i//sh,j//sw]>=maxC:\n",
    "                maxR = [i,i+kh,j,j+kw] \n",
    "                maxC = result[i//sh,j//sw]\n",
    "            if (j+sw+kw)>W:\n",
    "                break\n",
    "        if (i+sh+kh)>H:\n",
    "            break\n",
    "    #print('')\n",
    "    return result,maxR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd447418",
   "metadata": {},
   "source": [
    "# Video Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308f0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vid_consistency(F,Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    C = []\n",
    "    for i in range(len(F)-2):\n",
    "        if Func==Combined_Metrics or Func==Mix_Metrics:\n",
    "            if len(M)>0 and Func==Mix_Metrics:\n",
    "                C.append(Func(F[i],F[i+1],F[i+2],op,M,W))\n",
    "            else:\n",
    "                C.append(Func(F[i],F[i+1],F[i+2],op))\n",
    "        else:\n",
    "            C.append(Func(F[i],F[i+1],F[i+2]))\n",
    "    return np.mean(np.asarray(C)),C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e31486",
   "metadata": {},
   "source": [
    "# Windowed Video Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f60435",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vid_consistency_W(F,wsize=(3,3),step=(3,3),Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    C = []\n",
    "    for i in range(len(F)-1):\n",
    "        print('Frame: ',i+1,'/',len(F)-1,end='\\r')\n",
    "        Metrics,Region = WMax_Inconsistency(F[i],F[i+1],wsize,step,Func,op,W)\n",
    "        C.append(np.mean(Metrics))\n",
    "    return np.mean(np.asarray(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6970db",
   "metadata": {},
   "source": [
    "# Draw Window with Max Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71acf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def DrawInconsistancy(F,wsize=(50,50),step=(50,50),Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    L = 3\n",
    "    DI = np.copy(F)\n",
    "    for i in range(len(F)-2):\n",
    "        print('Frame: ',i+1,'/',len(F)-1,end='\\r')\n",
    "        Metrics,Region = WMax_Inconsistency(F[i],F[i+1],F[i+2],wsize,step,Func,op,M,W)\n",
    "        #print((Region[0],Region[2]),(Region[1],Region[3]))\n",
    "        DI[i+1][Region[0]:Region[0]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[2]:Region[2]+L] = [0,255,0] \n",
    "        DI[i+1][Region[1]:Region[1]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[3]:Region[3]+L] = [0,255,0] \n",
    "        #DI[i+1] = cv2.putText(DI[i+1],str(np.mean(Metrics)),(10,20),cv2.FONT_HERSHEY_SIMPLEX,0.25,(0,0,255),1,cv2.LINE_AA) \n",
    "    return DI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c813b2d",
   "metadata": {},
   "source": [
    "# Draw Window with Max Inconsistency of Diffrent Window Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f9f03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def DrawInconsistancy1(F,Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    L = 3\n",
    "    DI = np.copy(F)\n",
    "    for i in range(1,len(F)-1):\n",
    "        print('Frame: ',i,'/',len(F)-1,end='\\r')\n",
    "        R = -1000\n",
    "        Region = [0,0,0,0]\n",
    "        for j in range(50,100,50):\n",
    "            Metrics,Reg= WMax_Inconsistency(F[i-1],F[i],F[i+1],(j,j),(j//2,j//2),Func,op,M,W)\n",
    "            if np.mean(Metrics)>R:\n",
    "                R = np.mean(Metrics)\n",
    "                Region = Reg\n",
    "        #print((Region[0],Region[2]),(Region[1],Region[3]))\n",
    "        \n",
    "        DI[i+1][Region[0]:Region[0]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[2]:Region[2]+L] = [0,255,0] \n",
    "        DI[i+1][Region[1]:Region[1]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[3]:Region[3]+L] = [0,255,0] \n",
    "        #DI[i+1] = cv2.putText(DI[i+1],str(R),(10,20),cv2.FONT_HERSHEY_SIMPLEX,0.25,(0,0,255),1,cv2.LINE_AA) \n",
    "    return DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a5a75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def InconsistentRegion(F,Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    L = 3\n",
    "    h,w,_ = F[0].shape\n",
    "    DI = [np.zeros((h,w)) for _ in F]\n",
    "    for i in range(len(F)-2):\n",
    "        #print('Frame: ',i+1,'/',len(F)-1,end='\\r')\n",
    "        R = -1000\n",
    "        Region = [0,0,0,0]\n",
    "        for j in range(20,100,20):\n",
    "            Metrics,Reg= WMax_Inconsistency(F[i],F[i+1],F[i+2],(j,j),(j//2,j//2),Func,op,M,W)\n",
    "            if np.mean(Metrics)>R:\n",
    "                R = np.mean(Metrics)\n",
    "                Region = Reg\n",
    "        #print((Region[0],Region[2]),(Region[1],Region[3]))\n",
    "        \n",
    "        DI[i+1][Region[0]:Region[1],Region[2]:Region[3]] = 255\n",
    "    return DI,R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29bd0d9",
   "metadata": {},
   "source": [
    "# Torch to Numpy and Vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frame2Torch(Frame,Size=None,Normalize=False,Denormalize=False):\n",
    "    if Size is not None:\n",
    "        interpolation = cv2.INTER_CUBIC if Size[0]>Frame.shape[0] or Size[1]>Frame.shape[1] else cv2.INTER_AREA\n",
    "        Frame = cv2.resize(Frame,Size,interpolation=interpolation)\n",
    "    if Normalize:\n",
    "        Frame = Frame.astype(float)/255.0\n",
    "    if Denormalize:\n",
    "        Frame = Frame.astype(float)*255.0\n",
    "    return torch.tensor(Frame).permute(2, 0, 1).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37830b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Frame2Numpy(Frame, Size=None, Normalize=False, Denormalize=False):\n",
    "    if Frame.ndim == 2:  # If the tensor has shape (N, M)\n",
    "        Frame = Frame.detach().cpu().numpy()\n",
    "    elif Frame.ndim == 3:  # If the tensor has shape (C, H, W)\n",
    "        Frame = Frame.squeeze(0).permute(1, 2, 0).detach().cpu().numpy()\n",
    "    if Size is not None:\n",
    "        interpolation = cv2.INTER_CUBIC if Size[0] > Frame.shape[0] or Size[1] > Frame.shape[1] else cv2.INTER_AREA\n",
    "        Frame = cv2.resize(Frame, Size, interpolation=interpolation)\n",
    "    if Normalize:\n",
    "        Frame = Frame.astype(float) / 255.0\n",
    "    if Denormalize:\n",
    "        Frame = (Frame.astype(float) * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    return Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OF_tensor1(img1: torch.Tensor, img2: torch.Tensor, alpha: float = 1.0, iterations: int = 100, kernel_size: int = 3):\n",
    "    import torch.nn.functional as Fun\n",
    "\n",
    "    assert img1.shape == img2.shape, \"Images must have the same shape\"\n",
    "\n",
    "    def rgb_to_grayscale(img):\n",
    "        return 0.2989 * img[0, :, :] + 0.5870 * img[1, :, :] + 0.1140 * img[2, :, :]\n",
    "\n",
    "    I1 = rgb_to_grayscale(img1)\n",
    "    I2 = rgb_to_grayscale(img2)\n",
    "\n",
    "    # Initialize optical flow vectors (u for x direction, v for y direction)\n",
    "    u = torch.zeros_like(I1, requires_grad=True)\n",
    "    v = torch.zeros_like(I1, requires_grad=True)\n",
    "    \n",
    "    # Define convolution kernels for gradients\n",
    "    kernel_x = torch.tensor([[[[-1, 1], [-1, 1]]]], dtype=torch.float32)\n",
    "    kernel_y = torch.tensor([[[[-1, -1], [1, 1]]]], dtype=torch.float32)\n",
    "\n",
    "    # Compute gradients with padding that maintains the original image size\n",
    "    Ix = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_x, padding=(1, 1)).squeeze(0).squeeze(0)\n",
    "    Iy = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_y, padding=(1, 1)).squeeze(0).squeeze(0)\n",
    "    It = I2 - I1  # Temporal gradient\n",
    "\n",
    "    # Ensure all tensors have matching dimensions\n",
    "    min_h = min(Ix.shape[-2], Iy.shape[-2], It.shape[-2], I1.shape[-2])\n",
    "    min_w = min(Ix.shape[-1], Iy.shape[-1], It.shape[-1], I1.shape[-1])\n",
    "\n",
    "    Ix = Ix[:min_h, :min_w]\n",
    "    Iy = Iy[:min_h, :min_w]\n",
    "    It = It[:min_h, :min_w]\n",
    "    u = u[:min_h, :min_w]\n",
    "    v = v[:min_h, :min_w]\n",
    "\n",
    "    # Adjust padding for the specified kernel size\n",
    "    padding = kernel_size // 2\n",
    "\n",
    "    # Iteratively update the optical flow\n",
    "    for _ in range(iterations):\n",
    "        u_avg = Fun.avg_pool2d(u.unsqueeze(0).unsqueeze(0), kernel_size, stride=1, padding=padding).squeeze(0).squeeze(0)\n",
    "        v_avg = Fun.avg_pool2d(v.unsqueeze(0).unsqueeze(0), kernel_size, stride=1, padding=padding).squeeze(0).squeeze(0)\n",
    "        \n",
    "        P = Ix * u_avg + Iy * v_avg + It\n",
    "        D = alpha ** 2 + Ix ** 2 + Iy ** 2\n",
    "        \n",
    "        u = u_avg - (Ix * P) / D\n",
    "        v = v_avg - (Iy * P) / D\n",
    "\n",
    "    flow = torch.stack((u, v), dim=0)\n",
    "    return flow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2d241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OF_tensor(img1: torch.Tensor, img2: torch.Tensor, alpha: float = 1.0, iterations: int = 100):\n",
    "    assert img1.shape == img2.shape, \"Images must have the same shape\"\n",
    "\n",
    "    def rgb_to_grayscale(img):\n",
    "        return 0.2989 * img[0, :, :] + 0.5870 * img[1, :, :] + 0.1140 * img[2, :, :]\n",
    "\n",
    "    I1 = rgb_to_grayscale(img1)\n",
    "    I2 = rgb_to_grayscale(img2)\n",
    "\n",
    "    # Initialize optical flow vectors (u for x direction, v for y direction)\n",
    "    u = torch.zeros_like(I1, requires_grad=True)\n",
    "    v = torch.zeros_like(I1, requires_grad=True)\n",
    "    \n",
    "    # Define convolution kernels for gradients\n",
    "    kernel_x = torch.tensor([[[[-1, 1], [-1, 1]]]], dtype=torch.float32)\n",
    "    kernel_y = torch.tensor([[[[-1, -1], [1, 1]]]], dtype=torch.float32)\n",
    "\n",
    "    # Compute gradients with padding that maintains the original image size\n",
    "    Ix = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_x, padding=(1, 1)).squeeze(0).squeeze(0)\n",
    "    Iy = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_y, padding=(1, 1)).squeeze(0).squeeze(0)\n",
    "    It = I2 - I1  # Temporal gradient\n",
    "\n",
    "    # Ensure all tensors have matching dimensions\n",
    "    min_h = min(Ix.shape[-2], Iy.shape[-2], It.shape[-2], I1.shape[-2])\n",
    "    min_w = min(Ix.shape[-1], Iy.shape[-1], It.shape[-1], I1.shape[-1])\n",
    "\n",
    "    Ix = Ix[:min_h, :min_w]\n",
    "    Iy = Iy[:min_h, :min_w]\n",
    "    It = It[:min_h, :min_w]\n",
    "    u = u[:min_h, :min_w]\n",
    "    v = v[:min_h, :min_w]\n",
    "\n",
    "    # Iteratively update the optical flow\n",
    "    for _ in range(iterations):\n",
    "        u_avg = Fun.avg_pool2d(u.unsqueeze(0).unsqueeze(0), 3, stride=1, padding=1).squeeze(0).squeeze(0)\n",
    "        v_avg = Fun.avg_pool2d(v.unsqueeze(0).unsqueeze(0), 3, stride=1, padding=1).squeeze(0).squeeze(0)\n",
    "        \n",
    "        P = Ix * u_avg + Iy * v_avg + It\n",
    "        D = alpha ** 2 + Ix ** 2 + Iy ** 2\n",
    "        \n",
    "        u = u_avg - (Ix * P) / D\n",
    "        v = v_avg - (Iy * P) / D\n",
    "\n",
    "    flow = torch.stack((u, v), dim=0)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6828b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_optical_flow1(img: torch.Tensor, flow: torch.Tensor, step: int = 10, scale: float = 1.0):\n",
    "    # Convert the image to a NumPy array for OpenCV\n",
    "    img_np = img.permute(1, 2, 0).cpu().numpy()  # (H, W, 3)\n",
    "    img_np = (img_np * 255).astype(np.uint8)  # Scale to 0-255\n",
    "\n",
    "    # Convert grayscale if needed\n",
    "    if img_np.shape[2] == 1 or len(img_np.shape) == 2:\n",
    "        img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Extract flow components\n",
    "    u = flow[0].cpu().detach().numpy()\n",
    "    v = flow[1].cpu().detach().numpy()\n",
    "\n",
    "    # Create a grid for drawing arrows\n",
    "    H, W = u.shape\n",
    "    y, x = np.mgrid[step // 2:H:step, step // 2:W:step].astype(np.int32)\n",
    "\n",
    "    # Start with the base image\n",
    "    display_img = img_np.copy()\n",
    "\n",
    "    # Draw arrows\n",
    "    for i, j in zip(x.flatten(), y.flatten()):\n",
    "        dx = int(scale * u[j, i])\n",
    "        dy = int(scale * v[j, i])\n",
    "        pt1 = (i, j)\n",
    "        pt2 = (i + dx, j + dy)\n",
    "        color = (0, 255, 0)  # Green arrows\n",
    "        thickness = 1\n",
    "        cv2.arrowedLine(display_img, pt1, pt2, color, thickness, tipLength=0.3)\n",
    "\n",
    "    # Display the result\n",
    "    #cv2.imshow(\"Optical Flow\", display_img)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    return display_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db672b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Edges_tensor(Frame):\n",
    "    K = torch.tensor([[[[0, -1, 0], [-1, 0, 1], [0, 1, 0]]]*3]*3, dtype=torch.float)\n",
    "    return Fun.conv2d(Frame,K,padding=1,stride=1).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(path, N, size):\n",
    "    # Find all video files in the specified path\n",
    "    video_files = [f for f in os.listdir(path) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "    if not video_files:\n",
    "        raise ValueError(\"No video files found in the specified path.\")\n",
    "    \n",
    "    # Choose a random video file\n",
    "    video_file = random.choice(video_files)\n",
    "    video_path = os.path.join(path, video_file)\n",
    "    \n",
    "    # Initialize the video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # If N is larger than the number of frames in the video, adjust it\n",
    "    if N > total_frames:\n",
    "        raise ValueError(f\"The video has only {total_frames} frames, but {N} frames were requested.\")\n",
    "    \n",
    "    # Select a random starting frame index such that we can capture N consecutive frames\n",
    "    start_frame = random.randint(0, total_frames - N)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    # Resize transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(N):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert the frame (BGR to RGB) and apply the resize transform\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = transform(frame_rgb)\n",
    "        frames.append(frame_tensor[[2, 1, 0], :, :])\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Stack frames and reshape to the desired output shape\n",
    "    frames_tensor = torch.stack(frames).unsqueeze(0)  # Shape (1, N, 3, H, W)\n",
    "    \n",
    "    return frames#_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391d7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOneInc1(F,Op=None,x=None,y=None,xl=None,yl=None):\n",
    "    # Convert to NumPy Array\n",
    "    F = F.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0\n",
    "    # Copy of original array\n",
    "    I = F.astype(np.uint8).copy()\n",
    "    # Random location\n",
    "    if x is None:\n",
    "        x = random.randint(0, 2 * I.shape[1] // 3)\n",
    "    if y is None:\n",
    "        y = random.randint(0, 2 * I.shape[0] // 3)\n",
    "    # Random size\n",
    "    if xl is None:\n",
    "        xl = random.randint(5, I.shape[1] // 6)\n",
    "    if yl is None:\n",
    "        yl = random.randint(5, I.shape[0] // 6)\n",
    "    # Random option\n",
    "    if Op is None:\n",
    "        Op = random.randint(0, 20)\n",
    "    #Change Color Range\n",
    "    if Op == 0:\n",
    "        I[x:x + xl, y:y + yl] = change_range_colors(\n",
    "            I[x:x + xl, y:y + yl],\n",
    "            (random.randint(30, 140), random.randint(30, 140), random.randint(30, 140)),\n",
    "            (random.randint(150, 255), random.randint(150, 255), random.randint(150, 255))\n",
    "        )\n",
    "    #Color Rectangle\n",
    "    elif Op == 1:\n",
    "        R, G, B = random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)\n",
    "        I[x:x + xl, y:y + yl] = change_range_colors(I[x:x + xl, y:y + yl], (R, G, B), (R, G, B))\n",
    "    #Color Line\n",
    "    elif Op == 2:\n",
    "        Thick = random.randint(1, 10)\n",
    "        l2 = random.randint(5, I.shape[1] // 15)\n",
    "        I = cv2.line(I, (x, y), (x + xl, y + yl), (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)), Thick)\n",
    "    #Black Line\n",
    "    elif Op == 3:\n",
    "        Thick = random.randint(1, 5)\n",
    "        l2 = random.randint(5, I.shape[1] // 20)\n",
    "        I = cv2.line(I, (x, y), (x + xl, y + yl), (0, 0, 0), Thick)\n",
    "    #Add Noise\n",
    "    elif Op == 4:\n",
    "        I[x:x + xl, y:y + yl] = I[x:x + xl, y:y + yl] + np.random.randint(-10, 10, I[x:x + xl, y:y + yl].shape)\n",
    "    #Convolution Random Kernel\n",
    "    elif Op == 5:\n",
    "        kernel = np.random.rand(3, 3)\n",
    "        I[x:x + xl, y:y + yl] = cv2.filter2D(I[x:x + xl, y:y + yl], -1, kernel)\n",
    "    # Deformation\n",
    "    elif Op>5 and Op<12:\n",
    "        I[x:x+xl,y:y+yl] = img_deformation(I[x:x+xl,y:y+yl],random.randint(10,80),random.randint(10,200))\n",
    "    else:# Op == 7:\n",
    "        I[x:x+xl,y:y+yl] = random_point_warp(I[x:x+xl,y:y+yl],random.randint(1,10))\n",
    "    \n",
    "    # Convert back to tensor\n",
    "    I_tensor = torch.tensor(I, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    return I_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52319406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOneInc2(Bin,F,Op=None,x=None,y=None,xl=None,yl=None):\n",
    "    Bin = Frame2Numpy(Bin)\n",
    "    # Convert to NumPy Array\n",
    "    F = F.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0\n",
    "    # Copy of original array\n",
    "    I = F.astype(np.uint8).copy()\n",
    "    # Random location\n",
    "    if x is None:\n",
    "        x = random.randint(0, 2 * I.shape[1] // 3)\n",
    "    if y is None:\n",
    "        y = random.randint(0, 2 * I.shape[0] // 3)\n",
    "    # Random size\n",
    "    if xl is None:\n",
    "        xl = random.randint(5, I.shape[1] // 6)\n",
    "    if yl is None:\n",
    "        yl = random.randint(5, I.shape[0] // 6)\n",
    "    # Random option\n",
    "    if Op is None:\n",
    "        Op = random.randint(0, 12)\n",
    "    #Change Color Range\n",
    "    if Op == 0:\n",
    "        I[x:x + xl, y:y + yl] = change_range_colors(\n",
    "            I[x:x + xl, y:y + yl],\n",
    "            (random.randint(30, 140), random.randint(30, 140), random.randint(30, 140)),\n",
    "            (random.randint(150, 255), random.randint(150, 255), random.randint(150, 255))\n",
    "        )\n",
    "        Bin[x:x+xl,y:y+yl] = 1.0\n",
    "    #Color Rectangle\n",
    "    elif Op == 1:\n",
    "        R, G, B = random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)\n",
    "        I[x:x + xl, y:y + yl] = change_range_colors(I[x:x + xl, y:y + yl], (R, G, B), (R, G, B))\n",
    "        Bin[x:x+xl,y:y+yl] = 1.0\n",
    "    #Color Line\n",
    "    elif Op == 2:\n",
    "        Thick = random.randint(1, 11)\n",
    "        if random.random()>0.5:\n",
    "            yl = -yl\n",
    "        I = cv2.line(I, (x, y), (x + xl, y + yl), (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)), Thick)\n",
    "        Bin = cv2.line(Bin,(x,y),(x+xl,y+yl),(1,1,1),Thick)\n",
    "    #Black Line\n",
    "    elif Op == 3:\n",
    "        Thick = random.randint(1, 5)\n",
    "        if random.random()>0.5:\n",
    "            yl = -yl\n",
    "        I = cv2.line(I, (x, y), (x + xl, y + yl), (0, 0, 0), Thick)\n",
    "        Bin = cv2.line(Bin,(x,y),(x+xl,y+yl),(1,1,1),Thick)\n",
    "    #Add Noise\n",
    "    elif Op == 4:\n",
    "        I[x:x + xl, y:y + yl] = I[x:x + xl, y:y + yl] + np.random.randint(-10, 10, I[x:x + xl, y:y + yl].shape)\n",
    "        Bin[x:x + xl, y:y + yl] = 1.0\n",
    "    #Convolution Random Kernel\n",
    "    elif Op == 5:\n",
    "        kernel = np.random.rand(3, 3)\n",
    "        I[x:x + xl, y:y + yl] = cv2.filter2D(I[x:x + xl, y:y + yl], -1, kernel)\n",
    "        Bin[x:x + xl, y:y + yl] = 1.0\n",
    "    # Deformation\n",
    "    elif Op==6:\n",
    "        I[x:x+xl,y:y+yl] = img_deformation(I[x:x+xl,y:y+yl],random.randint(10,80),random.randint(10,200))\n",
    "        Bin[x:x + xl, y:y + yl] = 1.0\n",
    "    #Warp\n",
    "    elif Op==7:\n",
    "        I[x:x+xl,y:y+yl] = random_point_warp(I[x:x+xl,y:y+yl],random.randint(1,10))\n",
    "        Bin[x:x + xl, y:y + yl] = 1.0\n",
    "    #Curved line\n",
    "    elif Op==8:\n",
    "        I,Bin = draw_random_curve(I,Bin,x,y,x+xl,y+yl)\n",
    "    #Polygon\n",
    "    elif Op==9:\n",
    "        I,B = draw_random_polygon(I, Bin, x,y,x+xl,y+yl)\n",
    "    #Curved Polygon\n",
    "    elif Op==10:\n",
    "        I,Bin = draw_random_curve_fig(I, Bin,x,y,x+xl,y+yl)\n",
    "    #Change Lightning\n",
    "    else:\n",
    "        I,Bin = light_noise(I,Bin,x,x+xl,y,y+yl)\n",
    "    # Convert back to tensor\n",
    "    I_tensor = torch.tensor(I, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    Bin = torch.tensor(Bin,dtype=torch.float32)\n",
    "    return I_tensor,Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frequency_image(freq_tensor):\n",
    "    # Ensure the tensor is on the CPU and convert it to a NumPy array\n",
    "    freq_numpy = freq_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    # Handle cases where the tensor has extra dimensions\n",
    "    if freq_numpy.ndim == 3 and freq_numpy.shape[0] in [1, 3]:  # (C, H, W) format\n",
    "        freq_numpy = freq_numpy.transpose(1, 2, 0)  # Convert to (H, W, C) for OpenCV\n",
    "    elif freq_numpy.ndim == 3:  # If it's a single-channel 3D tensor\n",
    "        freq_numpy = freq_numpy[0]  # Take the first channel\n",
    "\n",
    "    # Take the magnitude if it contains complex values\n",
    "    if np.iscomplexobj(freq_numpy):\n",
    "        freq_numpy = np.abs(freq_numpy)\n",
    "    \n",
    "    # Apply a logarithmic transformation for better visibility\n",
    "    freq_numpy = np.log(1 + freq_numpy)  # Adding 1 to avoid log(0)\n",
    "\n",
    "    # Normalize to the range [0, 255] for display\n",
    "    freq_normalized = cv2.normalize(freq_numpy, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    freq_normalized = freq_normalized.astype(np.uint8)\n",
    "    \n",
    "    # Display the frequency image\n",
    "    #cv2.imshow('Frequency Image', freq_normalized)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    \n",
    "    return freq_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_window(window_size, channels, sigma):\n",
    "    # Create a 1D Gaussian kernel\n",
    "    x = torch.arange(window_size, dtype=torch.float32) - (window_size - 1) / 2\n",
    "    gauss = torch.exp(-(x**2) / (2 * sigma**2))\n",
    "    gauss /= gauss.sum()  # Normalize the 1D kernel to ensure the sum is 1\n",
    "\n",
    "    # Create a 2D Gaussian kernel by taking the outer product\n",
    "    gauss_2d = gauss[:, None] * gauss[None, :]\n",
    "\n",
    "    # Reshape the kernel to be compatible with conv2d\n",
    "    kernel = gauss_2d.expand(channels, 1, window_size, window_size)\n",
    "\n",
    "    return kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abd6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TSSIM_tensor(img1, img2, window_size=11, sigma=1.5, C1=0.01**2, C2=0.03**2):\n",
    "\n",
    "    channels = img1.size(1)\n",
    "    gaussian_window = create_gaussian_window(window_size, channels, sigma).to(img1.device)\n",
    "\n",
    "    # Compute local means using Gaussian filter\n",
    "    mu1 = Fun.conv2d(img1, gaussian_window, padding=window_size // 2, groups=channels)\n",
    "    mu2 = Fun.conv2d(img2, gaussian_window, padding=window_size // 2, groups=channels)\n",
    "\n",
    "    # Compute local variances and covariance\n",
    "    mu1_sq = mu1 ** 2\n",
    "    mu2_sq = mu2 ** 2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = Fun.conv2d(img1 * img1, gaussian_window, padding=window_size // 2, groups=channels) - mu1_sq\n",
    "    sigma2_sq = Fun.conv2d(img2 * img2, gaussian_window, padding=window_size // 2, groups=channels) - mu2_sq\n",
    "    sigma12 = Fun.conv2d(img1 * img2, gaussian_window, padding=window_size // 2, groups=channels) - mu1_mu2\n",
    "\n",
    "    # Compute SSIM map\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / (\n",
    "        (mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2)\n",
    "    )\n",
    "\n",
    "    mean_ssim = ssim_map.mean()  # Compute the mean SSIM value\n",
    "\n",
    "    return mean_ssim, ssim_map  # Return both mean SSIM and SSIM map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80506f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw Optical Flow\n",
    "def draw_OF(image, flow, step=16, scale=1.0):\n",
    "    output_image = image.copy()\n",
    "    h, w = flow.shape[:2]\n",
    "    for y in range(0, h, step):\n",
    "        for x in range(0, w, step):\n",
    "            fx, fy = flow[y, x]\n",
    "            end_x = int((x + fx*scale))\n",
    "            end_y = int((y + fy*scale))\n",
    "            cv2.arrowedLine(output_image, (x, y), (end_x, end_y), (0, 255, 0), 1, tipLength=0.3)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ac2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_OF(image, flow):\n",
    "    h, w = flow.shape[:2]\n",
    "    y, x = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n",
    "    new_x = (x + flow[..., 0])\n",
    "    new_y = (y + flow[..., 1])\n",
    "    warped_image = cv2.remap(image, new_x, new_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229dd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bezier_curve(image,Bin, p0, p1, p2, p3, color, thickness=2):\n",
    "    curve = []\n",
    "    for t in np.linspace(0, 1, 100):  # Generate points along the Bzier curve\n",
    "        x = (1 - t)**3 * p0[0] + 3 * (1 - t)**2 * t * p1[0] + 3 * (1 - t) * t**2 * p2[0] + t**3 * p3[0]\n",
    "        y = (1 - t)**3 * p0[1] + 3 * (1 - t)**2 * t * p1[1] + 3 * (1 - t) * t**2 * p2[1] + t**3 * p3[1]\n",
    "        curve.append((int(x), int(y)))\n",
    "    for i in range(len(curve) - 1):\n",
    "        cv2.line(image, curve[i], curve[i + 1], color, thickness)\n",
    "        cv2.line(Bin,curve[i],curve[i+1],(1,1,1),thickness)\n",
    "\n",
    "def draw_random_curve(image,Bin,x,y,xl,yl):\n",
    "    p0 = (random.randint(x, xl), random.randint(y, yl))\n",
    "    p1 = (random.randint(x, xl), random.randint(y, yl))\n",
    "    p2 = (random.randint(x, xl), random.randint(y, yl))\n",
    "    p3 = (random.randint(x, xl), random.randint(y, yl))\n",
    "    color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))\n",
    "    thickness = random.randint(1, 3)\n",
    "    draw_bezier_curve(image,Bin, p0, p1, p2, p3, color, thickness)\n",
    "    return image,Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_polygon(img, binary_img, x,y,xl,yl):\n",
    "    # Generate a random number of points (between 3 and 8)\n",
    "    num_points = random.randint(3, 8)\n",
    "\n",
    "    # Generate random points within the image dimensions\n",
    "    points = np.array([(random.randint(x, xl - 1), random.randint(y, yl - 1)) for _ in range(num_points)])\n",
    "\n",
    "    # Randomly decide whether to sort the points to make it convex\n",
    "    if random.choice([True, False]):\n",
    "        hull = cv2.convexHull(points)  # Ensure convex shape\n",
    "    else:\n",
    "        hull = points  # Keep it concave\n",
    "\n",
    "    hull = hull.reshape((-1, 1, 2))  # Ensure proper shape for OpenCV\n",
    "\n",
    "    # Generate a random color\n",
    "    color = [random.randint(0, 255) for _ in range(3)]\n",
    "\n",
    "    # Draw the polygon on the image\n",
    "    cv2.fillPoly(img, [hull], color)\n",
    "\n",
    "    # Draw the polygon as 1s on the binary image\n",
    "    cv2.fillPoly(binary_img, [hull], 1)\n",
    "\n",
    "    return img, binary_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55aa2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.interpolate import splprep, splev\n",
    "\n",
    "def draw_random_curve_fig(img, binary_img, x, y, xl, yl):\n",
    "    # Ensure bounding box is valid\n",
    "    if xl <= x or yl <= y:\n",
    "        raise ValueError(\"Invalid bounding box: Ensure xl > x and yl > y.\")\n",
    "\n",
    "    # Generate 5-10 random control points\n",
    "    num_points = random.randint(5, 10)\n",
    "    control_points = np.array([(random.randint(x, xl), random.randint(y, yl)) for _ in range(num_points)], dtype=np.float32)\n",
    "\n",
    "    # Remove duplicate points (splprep requires unique points)\n",
    "    control_points = np.unique(control_points, axis=0)\n",
    "    \n",
    "    # Ensure we have enough points (at least 4 for cubic B-spline)\n",
    "    if len(control_points) < 4:\n",
    "        return img, binary_img  # Skip drawing if not enough points\n",
    "\n",
    "    # Close the curve by repeating the first point\n",
    "    control_points = np.vstack([control_points, control_points[0]])\n",
    "\n",
    "    # Fit a smooth B-spline curve\n",
    "    tck, u = splprep(control_points.T, s=0.5, per=True)\n",
    "    u_new = np.linspace(0, 1, num_points * 10)  # Smooth interpolation\n",
    "    curve = np.array(splev(u_new, tck)).T  # Generate curve points\n",
    "\n",
    "    # Convert to integer format for OpenCV\n",
    "    curve = np.round(curve).astype(np.int32).reshape(-1, 1, 2)\n",
    "\n",
    "    # Generate a random color\n",
    "    color = [random.randint(0, 255) for _ in range(3)]\n",
    "\n",
    "    # Draw the filled curved shape on the image\n",
    "    cv2.fillPoly(img, [curve], color)\n",
    "\n",
    "    # Draw the shape in the binary image (fill with 255 for visibility)\n",
    "    cv2.fillPoly(binary_img, [curve], 1)\n",
    "\n",
    "    return img, binary_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def light_noise(image,Bin,x,xl,y,yl):\n",
    "    image[y:yl, x:xl] = image[y:yl, x:xl]*random.gauss(0.5,1/6)\n",
    "    Bin[y:yl,x:xl] = 1.0\n",
    "    return image,Bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e0dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optical_flow_tensor(frame0, frame1):\n",
    "    if frame0.max() <= 1:\n",
    "        frame0 = frame0 * 255\n",
    "        frame1 = frame1 * 255\n",
    "\n",
    "    old_frame = Frame2Numpy(frame0)\n",
    "    frame = Frame2Numpy(frame1)\n",
    "    H, W = old_frame.shape[:2]\n",
    "\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute dense optical flow using Farneback\n",
    "    flow = cv2.calcOpticalFlowFarneback(old_gray, frame_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "\n",
    "    # Convert flow to PyTorch tensor\n",
    "    flow_tensor = torch.tensor(flow, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # (1, 2, H, W)\n",
    "\n",
    "    return -flow_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88aeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_image_tensor(image: torch.Tensor, flow: torch.Tensor) -> torch.Tensor:\n",
    "    \n",
    "    B, C, H, W = image.shape\n",
    "\n",
    "    # Generate a mesh grid of coordinates (height, width)\n",
    "    grid_y, grid_x = torch.meshgrid(torch.arange(H, device=image.device), \n",
    "                                    torch.arange(W, device=image.device), indexing='ij')\n",
    "    grid = torch.stack((grid_x, grid_y), dim=-1).float()  # shape: (H, W, 2)\n",
    "\n",
    "    # Normalize the grid to the range [-1, 1]\n",
    "    grid = 2.0 * grid / torch.tensor([W - 1, H - 1], device=image.device) - 1.0  # scale to [-1, 1]\n",
    "    grid = grid.view(1, H, W, 2).repeat(B, 1, 1, 1)\n",
    "\n",
    "    flow = flow.permute(0, 2, 3, 1)  # (B, 2, H, W) -> (B, H, W, 2)\n",
    "\n",
    "    # Normalize optical flow to grid scale\n",
    "    scale = torch.tensor([(W - 1) / 2, (H - 1) / 2], device=image.device)\n",
    "    flow = flow / scale  # Convert pixel displacement to [-1,1] range\n",
    "\n",
    "    grid = grid.clone() + flow \n",
    "\n",
    "    # Sample the image using the new grid\n",
    "    warped_image = Fun.grid_sample(image, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6a9222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Optical_flow_tensor1(frame0, frame1):\n",
    "    if frame0.max() <= 1:\n",
    "        frame0 = frame0 * 255\n",
    "        frame1 = frame1 * 255\n",
    "    \n",
    "    frame0 = frame0.float()\n",
    "    frame1 = frame1.float()\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    frame0_gray = 0.299 * frame0[:, 0, :, :] + 0.587 * frame0[:, 1, :, :] + 0.114 * frame0[:, 2, :, :]\n",
    "    frame1_gray = 0.299 * frame1[:, 0, :, :] + 0.587 * frame1[:, 1, :, :] + 0.114 * frame1[:, 2, :, :]\n",
    "    \n",
    "    # Compute pyramidal downsampling like Farneback\n",
    "    frame0_down = Fun.avg_pool2d(frame0_gray.unsqueeze(1), kernel_size=2, stride=2)\n",
    "    frame1_down = Fun.avg_pool2d(frame1_gray.unsqueeze(1), kernel_size=2, stride=2)\n",
    "    \n",
    "    # Compute image gradients (smooth approximation)\n",
    "    sobel_x = torch.tensor([[[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]], dtype=torch.float32, device=frame0.device).view(1, 1, 3, 3)\n",
    "    sobel_y = torch.tensor([[[-1, -2, -1], [0, 0, 0], [1, 2, 1]]], dtype=torch.float32, device=frame0.device).view(1, 1, 3, 3)\n",
    "    \n",
    "    Ix = Fun.conv2d(frame0_down, sobel_x, padding=1)\n",
    "    Iy = Fun.conv2d(frame0_down, sobel_y, padding=1)\n",
    "    It = (frame1_down - frame0_down)\n",
    "    \n",
    "    # Compute flow estimation using iterative refinement\n",
    "    eps = 1e-6\n",
    "    flow_x = -It * Ix / (Ix**2 + Iy**2 + eps)\n",
    "    flow_y = -It * Iy / (Ix**2 + Iy**2 + eps)\n",
    "    \n",
    "    # Upsample flow back to original resolution\n",
    "    flow_x = Fun.interpolate(flow_x, size=frame0_gray.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    flow_y = Fun.interpolate(flow_y, size=frame0_gray.shape[-2:], mode='bilinear', align_corners=False)\n",
    "    \n",
    "    flow = torch.cat([flow_x, flow_y], dim=1)  # Shape (B, 2, H, W)\n",
    "    return -flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf72df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variational_optical_flow(img1, img2, alpha=1.0, num_iters=10):\n",
    "    # Convert RGB to grayscale using simple averaging\n",
    "    img1_gray = img1.mean(dim=0, keepdim=True)\n",
    "    img2_gray = img2.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    # Compute gradients\n",
    "    sobel_x = torch.tensor([[1, 0, -1], [2, 0, -2], [1, 0, -1]], dtype=torch.float32).view(1, 1, 3, 3)\n",
    "    sobel_y = sobel_x.transpose(2, 3)\n",
    "    \n",
    "    I_x = Fun.conv2d(img1_gray.unsqueeze(0), sobel_x, padding=1)[0]\n",
    "    I_y = Fun.conv2d(img1_gray.unsqueeze(0), sobel_y, padding=1)[0]\n",
    "    I_t = img2_gray - img1_gray  # Temporal derivative\n",
    "\n",
    "    # Initialize flow (zero displacement)\n",
    "    u = torch.zeros_like(img1_gray)\n",
    "    v = torch.zeros_like(img1_gray)\n",
    "\n",
    "    # Smoothness kernel\n",
    "    avg_kernel = torch.ones(1, 1, 3, 3) / 9.0\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        # Compute average flow\n",
    "        u_avg = Fun.conv2d(u.unsqueeze(0), avg_kernel, padding=1)[0]\n",
    "        v_avg = Fun.conv2d(v.unsqueeze(0), avg_kernel, padding=1)[0]\n",
    "\n",
    "        # Solve for flow using iterative updates\n",
    "        flow_denom = alpha + I_x**2 + I_y**2\n",
    "        u = u_avg - (I_x * (I_x * u_avg + I_y * v_avg + I_t)) / flow_denom\n",
    "        v = v_avg - (I_y * (I_x * u_avg + I_y * v_avg + I_t)) / flow_denom\n",
    "\n",
    "    return torch.cat([u, v], dim=0)  # (2, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbe8b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_optical_flow(img1, img2, num_levels=3, alpha=0.1, num_iters=10):\n",
    "    flow = torch.zeros(2, img1.shape[1], img1.shape[2], device=img1.device)\n",
    "    \n",
    "    for level in range(num_levels, -1, -1):\n",
    "        scale = 2**level\n",
    "        H, W = img1.shape[1] // scale, img1.shape[2] // scale\n",
    "        \n",
    "        img1_resized = Fun.interpolate(img1.unsqueeze(0), size=(H, W), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        img2_resized = Fun.interpolate(img2.unsqueeze(0), size=(H, W), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "\n",
    "        flow_resized = Fun.interpolate(flow.unsqueeze(0), size=(H, W), mode=\"bilinear\", align_corners=False).squeeze(0)\n",
    "        \n",
    "        # Compute flow at this level (using variational or another method)\n",
    "        flow_update = compute_variational_optical_flow(img1_resized, img2_resized, alpha, num_iters)\n",
    "        \n",
    "        flow = flow_resized + flow_update  # Refine estimate\n",
    "        \n",
    "    return flow*2**num_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_segmentation(image, num_clusters=3, sigma=0.1):\n",
    "    C, H, W = image.shape\n",
    "    image_flat = image.view(C, -1).T  # Reshape to (H*W, C)\n",
    "    \n",
    "    # Initialize cluster centers equally distributed within image range\n",
    "    min_val, max_val = image_flat.min(dim=0)[0], image_flat.max(dim=0)[0]\n",
    "    cluster_centers = torch.linspace(0, 1, num_clusters).view(-1, 1) * (max_val - min_val) + min_val\n",
    "    cluster_centers = cluster_centers.to(image.device).requires_grad_(True)\n",
    "    \n",
    "    # Compute similarity between pixels and cluster centers\n",
    "    distances = torch.cdist(image_flat, cluster_centers)  # (H*W, num_clusters)\n",
    "    \n",
    "    # Apply soft assignment using a Gaussian kernel\n",
    "    soft_assignments = Fun.softmax(-distances / sigma, dim=-1)  # (H*W, num_clusters)\n",
    "    \n",
    "    # Reshape back to (num_clusters, H, W)\n",
    "    segmentation = soft_assignments.T.view(num_clusters, H, W)\n",
    "\n",
    "    # Generate equally spaced colors in RGB space\n",
    "    cluster_colors = torch.linspace(0, 1, num_clusters).unsqueeze(1).expand(-1, 3)  # (num_clusters, 3)\n",
    "    \n",
    "    # Make it a learnable tensor\n",
    "    cluster_colors = torch.nn.Parameter(cluster_colors.to(image.device))\n",
    "\n",
    "    # Compute RGB image as a weighted sum of cluster colors\n",
    "    segmented_rgb = torch.einsum('chw,cn->nhw', segmentation, cluster_colors)  # Shape: (3, H, W)\n",
    "    \n",
    "    return segmented_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b08d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_segmentationRGB(image, num_clusters=3, sigma=0.1):\n",
    "    C, H, W = image.shape\n",
    "    image_flat = image.view(C, -1).T  # Reshape to (H*W, C)\n",
    "    \n",
    "    # Initialize cluster centers in RGB space\n",
    "    min_val, max_val = image_flat.min(dim=0)[0], image_flat.max(dim=0)[0]\n",
    "    values = torch.linspace(0, 1, num_clusters)\n",
    "\n",
    "    # Create all combinations by repeating the values for each dimension\n",
    "    cluster_centers = torch.cartesian_prod(values, values, values).float()\n",
    "\n",
    "    num_clusters = cluster_centers.shape[0]\n",
    "    \n",
    "    # Compute similarity between pixels and cluster centers\n",
    "    distances = torch.cdist(image_flat, cluster_centers)  # (H*W, num_clusters)\n",
    "    \n",
    "    # Apply soft assignment using a Gaussian kernel\n",
    "    soft_assignments = Fun.softmax(-distances / sigma, dim=-1)  # (H*W, num_clusters)\n",
    "    \n",
    "    # Reshape back to (num_clusters, H, W)\n",
    "    segmentation = soft_assignments.T.view(num_clusters, H, W)\n",
    "    \n",
    "    # Compute RGB image as a weighted sum of cluster colors\n",
    "    segmented_rgb = torch.einsum('chw,cn->nhw', segmentation, cluster_centers)  # Shape: (3, H, W)\n",
    "\n",
    "    return segmented_rgb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
