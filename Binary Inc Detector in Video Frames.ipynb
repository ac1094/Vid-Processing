{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "060c102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a68abeb",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "514f939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(CNN, self).__init__()\n",
    "        self.N = N\n",
    "        \n",
    "        # CNN part: Extract feature maps\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # (N, 16, 300, 300)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # (N, 16, 150, 150)\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1), # (N, 32, 150, 150)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                 # (N, 32, 75, 75)\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # (N, 64, 75, 75)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)                  # (N, 64, 37, 37)\n",
    "        )\n",
    "        \n",
    "        # Flattening for Transformer\n",
    "        self.flatten = nn.Flatten(2)  # (N, 64, 37 * 37)\n",
    "\n",
    "        # Transformer part\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=64, nhead=8, num_encoder_layers=2, num_decoder_layers=2\n",
    "        )\n",
    "        \n",
    "        # Fully connected to output a single value\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64, 32),  # Reduce dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),   # Output a single value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through CNN\n",
    "        x = self.cnn(x)  # (N, 64, 37, 37)\n",
    "\n",
    "        # Flatten spatial dimensions for the transformer\n",
    "        x = self.flatten(x)  # (N, 64, 37 * 37)\n",
    "        x = x.permute(2, 0, 1)  # Transformer expects (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Create a dummy target sequence for the transformer decoder\n",
    "        tgt = torch.zeros(x.size(0), x.size(1), 64, device=x.device)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Transformer\n",
    "        x = self.transformer(x, tgt)  # (seq_len, batch_size, d_model)\n",
    "\n",
    "        # Take the first sequence's output\n",
    "        x = x[0]  # (batch_size, d_model)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = self.fc(x)  # (batch_size, 1)\n",
    "\n",
    "        # Average over the batch dimension to get a single output\n",
    "        x = x.mean(dim=0, keepdim=True)  # (1, 1)\n",
    "        return Fun.sigmoid(x.unsqueeze(-1).unsqueeze(-1))  # (1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d81a44",
   "metadata": {},
   "source": [
    "# Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cbe629be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(path, N, size):\n",
    "    # Find all video files in the specified path\n",
    "    video_files = [f for f in os.listdir(path) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "    if not video_files:\n",
    "        raise ValueError(\"No video files found in the specified path.\")\n",
    "    \n",
    "    # Choose a random video file\n",
    "    video_file = random.choice(video_files)\n",
    "    video_path = os.path.join(path, video_file)\n",
    "    \n",
    "    # Initialize the video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # If N is larger than the number of frames in the video, adjust it\n",
    "    if N > total_frames:\n",
    "        raise ValueError(f\"The video has only {total_frames} frames, but {N} frames were requested.\")\n",
    "    \n",
    "    # Select a random starting frame index such that we can capture N consecutive frames\n",
    "    start_frame = random.randint(0, total_frames - N)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    # Resize transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(N):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert the frame (BGR to RGB) and apply the resize transform\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = transform(frame_rgb)\n",
    "        frames.append(frame_tensor)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Stack frames and reshape to the desired output shape\n",
    "    frames_tensor = torch.stack(frames).unsqueeze(0)  # Shape (1, N, 3, H, W)\n",
    "    \n",
    "    return frames#_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b62ad",
   "metadata": {},
   "source": [
    "# Convert Tensor to Numpy List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e3371773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2nl(frames_tensor):\n",
    "    # Remove the batch dimension (1, N, 3, H, W) -> (N, 3, H, W)\n",
    "    frames_tensor = frames_tensor.squeeze(0)\n",
    "    \n",
    "    # Conve\n",
    "    #rt each frame tensor to a NumPy array\n",
    "    frame_list = [cv2.cvtColor(frame.permute(1, 2, 0).numpy(),cv2.COLOR_BGR2RGB) for frame in frames_tensor]\n",
    "    \n",
    "    return frame_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de486b3",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec30faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Path, State=None, N=3, Batch=10, Epochs=10, Steps=5, LR=1e-3):\n",
    "    Model = CNN(N*3)\n",
    "    Size = (200,200)\n",
    "    \n",
    "    if State is not None:\n",
    "        Model.load_state_dict(State)\n",
    "        \n",
    "    Loss = []\n",
    "    LMin = 1e20\n",
    "\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        epoch_start_time = time.time()  # Timer for each epoch\n",
    "        optimizer = optim.Adam(Model.parameters(), lr=LR)\n",
    "        total_loss = 0.0\n",
    "        Correct0 = 0\n",
    "        Correct1 = 0\n",
    "        Total0 = 0\n",
    "        Total1 = 0\n",
    "        for batch in range(Batch):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Frames = get_video_frames(Path, N, Size)\n",
    "            Frames = torch.stack(Frames)\n",
    "    \n",
    "            Target = torch.tensor([0.0]) if batch%2==0 else torch.tensor([1.0])\n",
    "\n",
    "            if Target.item() == 1.0:\n",
    "                steps = random.randint(1, Steps)\n",
    "                for s in range(steps):\n",
    "                    Frames[N // 2] = AddOneInc1(Frames[N // 2])\n",
    "                       \n",
    "            # Pixel Differences \n",
    "            diff_pix = [Frames[i]-Frames[i+1] for i in range(len(Frames)-1)]\n",
    "            diff_pix = torch.stack(diff_pix)\n",
    "            #print(diff_pix.shape)\n",
    "            \n",
    "            # Edges Frames\n",
    "            edge_frames = [Edges_tensor(Frames[i])-Edges_tensor(Frames[i+1]) for i in range(len(Frames)-1)]\n",
    "            edge_frames = torch.stack(edge_frames)\n",
    "            #print(edge_frames.shape)\n",
    "            \n",
    "            Scores = [TSSIM(Frames[i].unsqueeze(0),Frames[i+1].unsqueeze(0)) for i in range(len(Frames)-1)]\n",
    "            Score = [t[0].item() for t in Scores]\n",
    "            SimScores = [t[1].squeeze(0) for t in Scores]\n",
    "            SimScores = torch.stack(SimScores)\n",
    "            #print(SimScores.shape)\n",
    "            \n",
    "            All = torch.cat((diff_pix,edge_frames,SimScores),dim=0)\n",
    "            #print(All.shape)\n",
    "            \n",
    "            # Forward pass\n",
    "            Pred = Model(All)[0][0][0]\n",
    "            \n",
    "            \n",
    "            if Target.item() == 1.0:\n",
    "                Total1 += 1\n",
    "                if Target.item() == round(Pred.item()):\n",
    "                    Correct1 += 1\n",
    "            else:\n",
    "                Total0 += 1\n",
    "                if Target.item() == round(Pred.item()):\n",
    "                    Correct0 += 1\n",
    "            \n",
    "            # Calculate loss\n",
    "            criterion = nn.BCELoss()\n",
    "            loss = criterion(Pred, Target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_iter = elapsed_time / ((epoch * Batch) + batch + 1)\n",
    "            remaining_iters = (Epochs * Batch) - ((epoch * Batch) + batch + 1)\n",
    "            remaining_time = avg_time_per_iter * remaining_iters\n",
    "            Mins = int(remaining_time//60)\n",
    "            Secs = int(remaining_time%60)\n",
    "            print(f'LR: {LR:0.4} Batch [{batch+1}/{Batch}], Loss: {loss.item():.4f} Time:{Mins}:{Secs}, Correct0: {Correct0}/{Total0}, Correct1: {Correct1}/{Total1}', end='\\r')\n",
    "\n",
    "            # Display predictions on the frame\n",
    "            Frame = cv2.cvtColor(Frames[N // 2].permute(1, 2, 0).numpy(), cv2.COLOR_BGR2RGB)\n",
    "            Frame = cv2.putText(Frame, f'P: {round(Pred.item())}', (0, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\n",
    "            Frame = cv2.putText(Frame, f'T: {int(Target.item())}', (0, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1)\n",
    "            cv2.imshow(\"Frame\", Frame)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "        Loss.append(total_loss)\n",
    "        print(f'\\nEpoch [{epoch + 1}/{Epochs}], Loss: {total_loss / Batch:.6f}, Correct0: {Correct0}/{Total0}, Correct1: {Correct1}/{Total1}')\n",
    "        \n",
    "        LR *= 0.75\n",
    "    \n",
    "    States = Model.state_dict()\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    return Loss,States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1bf09",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "af0d1aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60ccdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "799ad465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.01 Batch [50/50], Loss: 0.6920 Time:59:38, Correct0: 10/25, Correct1: 10/25\n",
      "Epoch [1/10], Loss: 0.721118, Correct0: 10/25, Correct1: 10/25\n",
      "LR: 0.0075 Batch [50/50], Loss: 0.6877 Time:53:48, Correct0: 9/25, Correct1: 15/25\n",
      "Epoch [2/10], Loss: 0.702204, Correct0: 9/25, Correct1: 15/25\n",
      "LR: 0.005625 Batch [50/50], Loss: 0.6848 Time:48:2, Correct0: 16/25, Correct1: 7/254\n",
      "Epoch [3/10], Loss: 0.696926, Correct0: 16/25, Correct1: 7/25\n",
      "LR: 0.004219 Batch [50/50], Loss: 0.7032 Time:42:25, Correct0: 24/25, Correct1: 0/25\n",
      "Epoch [4/10], Loss: 0.695755, Correct0: 24/25, Correct1: 0/25\n",
      "LR: 0.003164 Batch [50/50], Loss: 0.7090 Time:35:35, Correct0: 25/25, Correct1: 0/25\n",
      "Epoch [5/10], Loss: 0.694621, Correct0: 25/25, Correct1: 0/25\n",
      "LR: 0.002373 Batch [50/50], Loss: 0.7116 Time:28:37, Correct0: 25/25, Correct1: 0/25\n",
      "Epoch [6/10], Loss: 0.694292, Correct0: 25/25, Correct1: 0/25\n",
      "LR: 0.00178 Batch [50/50], Loss: 0.7144 Time:21:30, Correct0: 25/25, Correct1: 0/25\n",
      "Epoch [7/10], Loss: 0.694325, Correct0: 25/25, Correct1: 0/25\n",
      "LR: 0.001335 Batch [50/50], Loss: 0.7121 Time:14:22, Correct0: 25/25, Correct1: 0/25\n",
      "Epoch [8/10], Loss: 0.693936, Correct0: 25/25, Correct1: 0/25\n",
      "LR: 0.001001 Batch [50/50], Loss: 0.7164 Time:7:11, Correct0: 25/25, Correct1: 0/254\n",
      "Epoch [9/10], Loss: 0.693734, Correct0: 25/25, Correct1: 0/25\n",
      "LR: 0.0007508 Batch [50/50], Loss: 0.7209 Time:0:0, Correct0: 25/25, Correct1: 0/254\n",
      "Epoch [10/10], Loss: 0.693814, Correct0: 25/25, Correct1: 0/25\n"
     ]
    }
   ],
   "source": [
    "L,States = train('VDB',Batch=50,State=State,N=5,Epochs=10,Steps=5,LR=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L[1:]) ##Cambios de Escenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(States,\"StatesV5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36100859",
   "metadata": {},
   "outputs": [],
   "source": [
    "States = torch.load(\"StatesV5.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db0ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Que hace el mecanismo de atencion en procesamiento de imagenes\n",
    "#Sino en PLN\n",
    "#Embeding/Classes\n",
    "#Cambios de Escenas con escena anterior (COnsecutivo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
