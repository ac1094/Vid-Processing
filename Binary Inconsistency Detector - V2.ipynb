{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "4962893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Functions import *\n",
    "import torch.nn.functional as Fun\n",
    "import torch\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07fa35",
   "metadata": {},
   "source": [
    "# Add Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "221b8dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOneInc1(F):\n",
    "    # Convert to NumPy Array\n",
    "    F = F.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255\n",
    "    # Copy of original array\n",
    "    I = F.astype(np.uint8).copy()\n",
    "    # Random location\n",
    "    x, y = random.randint(0, 2 * I.shape[0] // 3), random.randint(0, 2 * I.shape[1] // 3)\n",
    "    # Random size\n",
    "    l = random.randint(5, I.shape[1] // 20)\n",
    "    # Random option\n",
    "    Op = random.randint(0, 5)\n",
    "    if Op == 0:\n",
    "        I[x:x + l, y:y + l] = change_range_colors(\n",
    "            I[x:x + l, y:y + l],\n",
    "            (random.randint(30, 140), random.randint(30, 140), random.randint(30, 140)),\n",
    "            (random.randint(150, 255), random.randint(150, 255), random.randint(150, 255))\n",
    "        )\n",
    "    elif Op == 1:\n",
    "        R, G, B = random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)\n",
    "        I[x:x + l, y:y + l] = change_range_colors(I[x:x + l, y:y + l], (R, G, B), (R, G, B))\n",
    "    elif Op == 2:\n",
    "        Thick = random.randint(1, 10)\n",
    "        l2 = random.randint(5, I.shape[1] // 15)\n",
    "        I = cv2.line(I, (x, y), (x + l, y + l2), (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)), Thick)\n",
    "    elif Op == 3:\n",
    "        Thick = random.randint(1, 5)\n",
    "        l2 = random.randint(5, I.shape[1] // 20)\n",
    "        I = cv2.line(I, (x, y), (x + l, y + l2), (0, 0, 0), Thick)\n",
    "    elif Op == 4:\n",
    "        I[x:x + l, y:y + l] = I[x:x + l, y:y + l] + np.random.randint(-10, 10, I[x:x + l, y:y + l].shape)\n",
    "    else:\n",
    "        kernel = np.random.rand(3, 3)\n",
    "        I[x:x + l, y:y + l] = cv2.filter2D(I[x:x + l, y:y + l], -1, kernel)\n",
    "    \n",
    "    # Convert back to tensor\n",
    "    I_tensor = torch.tensor(I, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    return I_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542e6952",
   "metadata": {},
   "source": [
    "# Calculate OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "1bd3f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OF(img1: torch.Tensor, img2: torch.Tensor, alpha: float = 1.0, iterations: int = 100):\n",
    "    assert img1.shape == img2.shape, \"Images must have the same shape\"\n",
    "\n",
    "    def rgb_to_grayscale(img):\n",
    "        return 0.2989 * img[0, :, :] + 0.5870 * img[1, :, :] + 0.1140 * img[2, :, :]\n",
    "\n",
    "    I1 = rgb_to_grayscale(img1)\n",
    "    I2 = rgb_to_grayscale(img2)\n",
    "\n",
    "    # Initialize optical flow vectors (u for x direction, v for y direction)\n",
    "    u = torch.zeros_like(I1, requires_grad=True)\n",
    "    v = torch.zeros_like(I1, requires_grad=True)\n",
    "    \n",
    "    # Define convolution kernels for gradients\n",
    "    kernel_x = torch.tensor([[[[-1, 1], [-1, 1]]]], dtype=torch.float32)\n",
    "    kernel_y = torch.tensor([[[[-1, -1], [1, 1]]]], dtype=torch.float32)\n",
    "\n",
    "    # Compute gradients with padding that maintains the original image size\n",
    "    Ix = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_x, padding=(0, 1)).squeeze(0).squeeze(0)\n",
    "    Iy = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_y, padding=(1, 0)).squeeze(0).squeeze(0)\n",
    "    It = I2 - I1  # Temporal gradient\n",
    "\n",
    "    # Ensure all tensors have matching dimensions\n",
    "    min_h = min(Ix.shape[-2], Iy.shape[-2], It.shape[-2], I1.shape[-2])\n",
    "    min_w = min(Ix.shape[-1], Iy.shape[-1], It.shape[-1], I1.shape[-1])\n",
    "\n",
    "    Ix = Ix[:min_h, :min_w]\n",
    "    Iy = Iy[:min_h, :min_w]\n",
    "    It = It[:min_h, :min_w]\n",
    "    u = u[:min_h, :min_w]\n",
    "    v = v[:min_h, :min_w]\n",
    "\n",
    "    # Iteratively update the optical flow\n",
    "    for _ in range(iterations):\n",
    "        u_avg = Fun.avg_pool2d(u.unsqueeze(0).unsqueeze(0), 3, stride=1, padding=1).squeeze(0).squeeze(0)\n",
    "        v_avg = Fun.avg_pool2d(v.unsqueeze(0).unsqueeze(0), 3, stride=1, padding=1).squeeze(0).squeeze(0)\n",
    "        \n",
    "        P = Ix * u_avg + Iy * v_avg + It\n",
    "        D = alpha ** 2 + Ix ** 2 + Iy ** 2\n",
    "        \n",
    "        u = u_avg - (Ix * P) / D\n",
    "        v = v_avg - (Iy * P) / D\n",
    "\n",
    "    flow = torch.stack((u, v), dim=0)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c4f5a8",
   "metadata": {},
   "source": [
    "# Calculate Difference in Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "051a0e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_loss(image1, image2):\n",
    "    freq_image1 = torch.fft.fft2(image1 * 255, dim=(-2, -1))\n",
    "    freq_image2 = torch.fft.fft2(image2 * 255, dim=(-2, -1))\n",
    "    mag_image1 = torch.abs(freq_image1)\n",
    "    mag_image2 = torch.abs(freq_image2)\n",
    "    loss = mag_image1 - mag_image2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c1a0a",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "d7e601da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelModel(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(PixelModel, self).__init__()\n",
    "        self.N = N\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 37 * 37, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(Fun.relu(self.conv1(x)))\n",
    "        x = self.pool(Fun.relu(self.conv2(x)))\n",
    "        x = self.pool(Fun.relu(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = Fun.relu(self.fc1(x))\n",
    "        x = Fun.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "5b2b051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixModel(nn.Module):\n",
    "    def __init__(self, N):\n",
    "        super(MixModel, self).__init__()\n",
    "        self.N = N\n",
    "        self.layer1 = nn.Linear(N, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = Fun.relu(self.layer1(x))\n",
    "        x = Fun.relu(self.layer2(x))\n",
    "        x = Fun.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        return Fun.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ef703e",
   "metadata": {},
   "source": [
    "# Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "7367e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(path, N, size):\n",
    "    # Find all video files in the specified path\n",
    "    video_files = [f for f in os.listdir(path) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "    if not video_files:\n",
    "        raise ValueError(\"No video files found in the specified path.\")\n",
    "    \n",
    "    # Choose a random video file\n",
    "    video_file = random.choice(video_files)\n",
    "    video_path = os.path.join(path, video_file)\n",
    "    \n",
    "    # Initialize the video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # If N is larger than the number of frames in the video, adjust it\n",
    "    if N > total_frames:\n",
    "        raise ValueError(f\"The video has only {total_frames} frames, but {N} frames were requested.\")\n",
    "    \n",
    "    # Select a random starting frame index such that we can capture N consecutive frames\n",
    "    start_frame = random.randint(0, total_frames - N)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    # Resize transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(N):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert the frame (BGR to RGB) and apply the resize transform\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = transform(frame_rgb)\n",
    "        frames.append(frame_tensor)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Stack frames and reshape to the desired output shape\n",
    "    frames_tensor = torch.stack(frames).unsqueeze(0)  # Shape (1, N, 3, H, W)\n",
    "    \n",
    "    return frames#_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551750f",
   "metadata": {},
   "source": [
    "# Convert Tensor to Numpy List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "981cd4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2nl(frames_tensor):\n",
    "    # Remove the batch dimension (1, N, 3, H, W) -> (N, 3, H, W)\n",
    "    frames_tensor = frames_tensor.squeeze(0)\n",
    "    \n",
    "    # Convert each frame tensor to a NumPy array\n",
    "    frame_list = [cv2.cvtColor(frame.permute(1, 2, 0).numpy(),cv2.COLOR_BGR2RGB) for frame in frames_tensor]\n",
    "    \n",
    "    return frame_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f350d0",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "6592796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Path, State=None, N=3, Batch=10, Epochs=10, Steps=5, LR=1e-3):\n",
    "    pmodel = PixelModel(N)\n",
    "    dmodel = PixelModel(N-1)\n",
    "    fmodel = PixelModel(N-1)\n",
    "    mmodel = MixModel(3)\n",
    "    \n",
    "    Params = list(pmodel.parameters())+list(dmodel.parameters())+list(fmodel.parameters())+list(mmodel.parameters())\n",
    "    \n",
    "    Size = (300,300)\n",
    "    \n",
    "    if State is not None:\n",
    "        model.load_state_dict(State)\n",
    "\n",
    "    Loss= []\n",
    "    LMin = 1e20\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        optimizer = optim.Adam(Params, lr=LR)\n",
    "        total_loss = 0.0\n",
    "        for batch in range(Batch):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Frames = get_video_frames(Path, N, Size)\n",
    "            Frames = torch.stack(Frames)\n",
    "            #print('Frames: ',Frames.shape)\n",
    "            Target = torch.tensor([0.0]) if random.random() >= 0.5 else torch.tensor([1.0])\n",
    "\n",
    "            if Target.item() == 1.0:\n",
    "                steps = random.randint(1, Steps)\n",
    "                for s in range(steps):\n",
    "                    Frames[N // 2] = AddOneInc1(Frames[N // 2])\n",
    "            \n",
    "            # Difference Frames\n",
    "            diff_frames = [Frames[i+1] - Frames[i] for i in range(len(Frames)-1)]\n",
    "            diff_frames = torch.stack(diff_frames)  # Shape [1, N-1, C, H, W]\n",
    "            #print('Diff: ',diff_frames.shape)\n",
    "            \n",
    "            #Frequency Frames\n",
    "            freq_frames = [frequency_loss(Frames[i+1],Frames[i]) for i in range(len(Frames)-1)]\n",
    "            freq_frames = torch.stack(freq_frames)\n",
    "            #print('Freq: ',freq_frames.shape)\n",
    "            \n",
    "            # Forward pass\n",
    "            predP = pmodel(Frames).unsqueeze(0)\n",
    "            predD = dmodel(diff_frames).unsqueeze(0)\n",
    "            predF = fmodel(freq_frames).unsqueeze(0)\n",
    "            predAll = torch.stack([predP,predD,predF]).permute(1,0).squeeze(0)\n",
    "            #print('All: ',predAll.shape)\n",
    "            \n",
    "            Pred = mmodel(predAll)\n",
    "            #print('Pred: ',Pred.shape)\n",
    "            #print('Target: ',Target.shape)\n",
    "            # Calculate loss\n",
    "            criterion = nn.BCEWithLogitsLoss()\n",
    "            loss = criterion(Pred, Target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            print(f'LR: {LR:0.4} Batch [{batch+1}/{Batch}], Loss: {loss.item():.6f}', end='\\r')\n",
    "\n",
    "            # Display predictions on the frame\n",
    "            Frame = cv2.cvtColor(Frames[N // 2].permute(1,2,0).numpy(), cv2.COLOR_BGR2RGB)\n",
    "            Frame = cv2.putText(Frame, f'P: {round(Pred.item())}', (0, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\n",
    "            Frame = cv2.putText(Frame, f'T: {int(Target.item())}', (0, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1)\n",
    "            cv2.imshow(\"Frame\", Frame)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "        Loss.append(total_loss)\n",
    "        print(f'\\nEpoch [{epoch + 1}/{Epochs}], Loss: {total_loss / Batch:.6f}\\n')\n",
    "        \n",
    "        #if total_loss < LMin:\n",
    "         #   LMin = total_loss\n",
    "          #  State = model.state_dict()\n",
    "        #else:\n",
    "         #   model.load_state_dict(State)\n",
    "        LR *= 0.9\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "            \n",
    "    return State,pmodel,Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163730c0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "2b563df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "149b307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "7ec9616a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001 Batch [10/10], Loss: 1.313262\n",
      "Epoch [1/50], Loss: 0.828638\n",
      "\n",
      "LR: 0.0009 Batch [10/10], Loss: 1.313262\n",
      "Epoch [2/50], Loss: 0.813262\n",
      "\n",
      "LR: 0.00081 Batch [10/10], Loss: 1.313262\n",
      "Epoch [3/50], Loss: 1.113262\n",
      "\n",
      "LR: 0.000729 Batch [10/10], Loss: 0.313262\n",
      "Epoch [4/50], Loss: 0.913262\n",
      "\n",
      "LR: 0.0006561 Batch [10/10], Loss: 0.313262\n",
      "Epoch [5/50], Loss: 0.813262\n",
      "\n",
      "LR: 0.0005905 Batch [10/10], Loss: 1.313262\n",
      "Epoch [6/50], Loss: 0.913262\n",
      "\n",
      "LR: 0.0005314 Batch [10/10], Loss: 0.313262\n",
      "Epoch [7/50], Loss: 0.613262\n",
      "\n",
      "LR: 0.0004783 Batch [10/10], Loss: 1.313262\n",
      "Epoch [8/50], Loss: 0.813262\n",
      "\n",
      "LR: 0.0004305 Batch [10/10], Loss: 0.313262\n",
      "Epoch [9/50], Loss: 0.913262\n",
      "\n",
      "LR: 0.0003874 Batch [10/10], Loss: 1.313262\n",
      "Epoch [10/50], Loss: 0.913262\n",
      "\n",
      "LR: 0.0003487 Batch [10/10], Loss: 1.313262\n",
      "Epoch [11/50], Loss: 0.913262\n",
      "\n",
      "LR: 0.0003138 Batch [10/10], Loss: 1.313262\n",
      "Epoch [12/50], Loss: 0.813262\n",
      "\n",
      "LR: 0.0002824 Batch [10/10], Loss: 1.313262\n",
      "Epoch [13/50], Loss: 0.713262\n",
      "\n",
      "LR: 0.0002542 Batch [10/10], Loss: 1.313262\n",
      "Epoch [14/50], Loss: 0.913262\n",
      "\n",
      "LR: 0.0002288 Batch [10/10], Loss: 1.313262\n",
      "Epoch [15/50], Loss: 1.113262\n",
      "\n",
      "LR: 0.0002059 Batch [10/10], Loss: 0.313262\n",
      "Epoch [16/50], Loss: 0.913262\n",
      "\n",
      "LR: 0.0001853 Batch [10/10], Loss: 1.313262\n",
      "Epoch [17/50], Loss: 0.613262\n",
      "\n",
      "LR: 0.0001668 Batch [10/10], Loss: 0.313262\n",
      "Epoch [18/50], Loss: 1.047449\n",
      "\n",
      "LR: 0.0001501 Batch [10/10], Loss: 0.313262\n",
      "Epoch [19/50], Loss: 0.613262\n",
      "\n",
      "LR: 0.0001351 Batch [10/10], Loss: 1.313262\n",
      "Epoch [20/50], Loss: 0.813262\n",
      "\n",
      "LR: 0.0001216 Batch [10/10], Loss: 0.313262\n",
      "Epoch [21/50], Loss: 0.813262\n",
      "\n",
      "LR: 0.0001094 Batch [10/10], Loss: 0.974426\n",
      "Epoch [22/50], Loss: 0.979378\n",
      "\n",
      "LR: 9.848e-05 Batch [10/10], Loss: 0.313262\n",
      "Epoch [23/50], Loss: 0.813262\n",
      "\n",
      "LR: 8.863e-05 Batch [10/10], Loss: 0.313262\n",
      "Epoch [24/50], Loss: 0.813262\n",
      "\n",
      "LR: 7.977e-05 Batch [10/10], Loss: 0.313262\n",
      "Epoch [25/50], Loss: 0.413262\n",
      "\n",
      "LR: 7.179e-05 Batch [10/10], Loss: 1.171972\n",
      "Epoch [26/50], Loss: 0.999133\n",
      "\n",
      "LR: 6.461e-05 Batch [10/10], Loss: 1.313262\n",
      "Epoch [27/50], Loss: 0.613262\n",
      "\n",
      "LR: 5.815e-05 Batch [10/10], Loss: 0.313262\n",
      "Epoch [28/50], Loss: 0.413262\n",
      "\n",
      "LR: 5.233e-05 Batch [10/10], Loss: 0.313262\n",
      "Epoch [29/50], Loss: 0.813262\n",
      "\n",
      "LR: 4.71e-05 Batch [10/10], Loss: 0.313262\n",
      "Epoch [30/50], Loss: 0.713262\n",
      "\n",
      "LR: 4.239e-05 Batch [10/10], Loss: 1.313262\n",
      "Epoch [31/50], Loss: 0.913262\n",
      "\n",
      "LR: 3.815e-05 Batch [10/10], Loss: 1.313262\n",
      "Epoch [32/50], Loss: 0.813262\n",
      "\n",
      "LR: 3.434e-05 Batch [10/10], Loss: 0.313262\n",
      "Epoch [33/50], Loss: 0.913262\n",
      "\n",
      "LR: 3.09e-05 Batch [10/10], Loss: 1.313262\n",
      "Epoch [34/50], Loss: 1.013262\n",
      "\n",
      "LR: 2.781e-05 Batch [9/10], Loss: 1.313262\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-487-98721a96e702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VDB'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-484-030b45bfa4e5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Path, State, N, Batch, Epochs, Steps, LR)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[0;32m    488\u001b[0m         torch.autograd.backward(\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         )\n\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m def grad(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "State,Model,L = train('VDB',Batch=10,State=State,N=3,Epochs=50,Steps=10,LR=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357261ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(video, model, model_state, N, H, W):\n",
    "    # Load the model state\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    outputs = []\n",
    "    # Iterate over the video in steps of N frames\n",
    "    num_frames = len(video)\n",
    "    for i in range(0, num_frames - N + 1):\n",
    "        # Extract the stack of frames\n",
    "        frame_stack = video[i:i + N]  # Shape: (N, H, W, C)\n",
    "        # Convert to numpy array and add batch dimension\n",
    "        frame_stack = np.array(frame_stack)  # Shape: (N, H, W, C)\n",
    "        # Rearrange to match the model's expected input shape: (batch, N, C, H, W)\n",
    "        # In our case, the frame_stack shape should be (1, N, 3, H, W) where 3 is the number of channels (RGB)\n",
    "        frame_stack = np.transpose(frame_stack, (0, 3, 1, 2))  # Shape: (N, C, H, W)\n",
    "        frame_stack = np.expand_dims(frame_stack, axis=0)  # Shape: (1, N, C, H, W)\n",
    "        # Convert to a PyTorch tensor\n",
    "        frame_stack = torch.tensor(frame_stack, dtype=torch.float32)\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output = model(frame_stack)\n",
    "        # Append the output to the list\n",
    "        outputs.append(int(round(output.item())))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e78a91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = open_vid(\"AI Gen/movie.mp4\")\n",
    "Video = get_frames(cap)\n",
    "VI = torch.from_numpy(Video[5]).permute(2,0,1).unsqueeze(0)\n",
    "VI = AddOneInc1(VI)\n",
    "Video[5] = cv2.cvtColor(VI.permute(1, 2, 0).numpy(),cv2.COLOR_BGR2RGB)\n",
    "display_frame(Video[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f66a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video = [cv2.resize(i,(100,100)) for i in Video[:15]]\n",
    "Preds = process_video(Video,Model,State,3,100,100)\n",
    "Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177da01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(Vid,Model,State,N,Size):\n",
    "    model = BinIncModel(N,Size(0),Size(1))\n",
    "    P = []\n",
    "    V = [torch.tensor(cv2.resize(i,(320,320),interpolation=cv2.INTER_AREA),\n",
    "                              dtype=torch.float32).permute(2,0,1).unsqueeze(0)/255.0 for i in Vid]\n",
    "    \n",
    "    N = len(kt)//2\n",
    "    for f in range(len(V) - 2 * N):\n",
    "        first, current, last = f, f + N // 2, f + N\n",
    "        Pred = ObjFun(V[first:last], V[current], k, kt, kb)\n",
    "        P.append(cv2.resize((Pred[0] * 255).squeeze(0).permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8),\n",
    "                                           (Vid[f].shape[1], Vid[f].shape[0]), interpolation=cv2.INTER_AREA))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd434d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "PVid = Prediction(Video,Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3727510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(Video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a46e1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(PVid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9137bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add detail error, MSE of full image only minimizes general pixel distribution\n",
    "##Add more training examples\n",
    "##Add N previous and N post current inconsistence image\n",
    "##Add optical flow, frequencies or other features for temporal consistency\n",
    "##Different kernel sizes\n",
    "##Add Gaisian Noise\n",
    "##Reconstruct image with kernel middle between encoder and decofer\n",
    "##Use data with less loss value, selection\n",
    "##How to combine data with less error and with more error?\n",
    "##Localize area and correct error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256cea09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deformaciones\n",
    "#parches de colores, OF ventana\n",
    "#self attention mechanism\n",
    "#image processing transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a31ea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
