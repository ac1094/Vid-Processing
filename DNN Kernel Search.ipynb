{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3842a975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ConsistencyIndexes.ipynb\n",
      "importing Jupyter notebook from Functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from ConsistencyIndexes import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e67bd8",
   "metadata": {},
   "source": [
    "### Get Original Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c522caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width:  640\n",
      "Height:  320\n",
      "FPS:  24.0\n",
      "Frame Count:  36\n"
     ]
    }
   ],
   "source": [
    "cap = open_vid(\"VDB/X.mp4\")\n",
    "org = get_frames(cap)\n",
    "_,_,fps,_ = get_props(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd84a7f",
   "metadata": {},
   "source": [
    "### Get Cartoonized Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2e064b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width:  640\n",
      "Height:  320\n",
      "FPS:  24.0\n",
      "Frame Count:  36\n"
     ]
    }
   ],
   "source": [
    "cap = open_vid(\"Cartoonized/Xtoon.mp4\")\n",
    "car = get_frames(cap)\n",
    "_,_,fps,_ = get_props(cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aecc4a",
   "metadata": {},
   "source": [
    "### Calculate 2D Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3c1d02fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_2Dkernel(input_images, target_images, kernel_size=3, epochs=1000, learning_rate=1e-3):\n",
    "    # Normalize input and target images (to 0-1)\n",
    "    input_images = [img / 255.0 for img in input_images]\n",
    "    target_images = [img / 255.0 for img in target_images]\n",
    "    # Convert input and target images to tensors\n",
    "    input_images = torch.stack([torch.tensor(img, dtype=torch.float32) for img in input_images])\n",
    "    target_images = torch.stack([torch.tensor(img, dtype=torch.float32) for img in target_images])   \n",
    "    # Image Size\n",
    "    height, width = input_images[0].shape[0], input_images[0].shape[1]    \n",
    "    # Initialize the kernel\n",
    "    kernel = torch.rand(1, 1, kernel_size, kernel_size, requires_grad=True)    \n",
    "    # Use an optimizer to update the kernel\n",
    "    optimizer = torch.optim.SGD([kernel], lr=learning_rate)    \n",
    "    # List of loss over time\n",
    "    L = []    \n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0      \n",
    "        # Iterate through each image and calculate the total loss\n",
    "        for i in range(len(input_images)):\n",
    "            optimizer.zero_grad()  # Clear the gradients from the previous iteration  \n",
    "            # Separate each color channel\n",
    "            r = input_images[i][:,:,0].view(1, 1, height, width)\n",
    "            g = input_images[i][:,:,1].view(1, 1, height, width)\n",
    "            b = input_images[i][:,:,2].view(1, 1, height, width)            \n",
    "            # Apply convolution to each channel\n",
    "            cr = F.conv2d(r, kernel, padding=1)\n",
    "            cg = F.conv2d(g, kernel, padding=1)\n",
    "            cb = F.conv2d(b, kernel, padding=1)            \n",
    "            # Calculate the loss\n",
    "            loss = (F.mse_loss(cr, target_images[i][:,:,0].unsqueeze(0).unsqueeze(0)) +\n",
    "                    F.mse_loss(cg, target_images[i][:,:,1].unsqueeze(0).unsqueeze(0)) +\n",
    "                    F.mse_loss(cb, target_images[i][:,:,2].unsqueeze(0).unsqueeze(0)))            \n",
    "            # Add to total loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()  # Compute gradients        \n",
    "            optimizer.step()  # Update the kernel with the computed gradients        \n",
    "        # Add total loss to list over epochs\n",
    "        L.append(total_loss)        \n",
    "        # Print loss every 10 iterations\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {total_loss}\", end='\\r')    \n",
    "    # Return the optimized kernel and list of loss over epochs\n",
    "    return kernel.detach().numpy(), L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a1c622",
   "metadata": {},
   "source": [
    "#### Train DNN and get Kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2965dcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000/2000, Total Loss: 1.8912924006581306\r"
     ]
    }
   ],
   "source": [
    "k,l = find_2Dkernel(org,car,3,2000,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41235b00",
   "metadata": {},
   "source": [
    "#### Calculated Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9f5e40c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.11707728  0.17420238  0.11876266]\n",
      "   [ 0.08345454  0.4771772  -0.03955662]\n",
      "   [ 0.12558264  0.1051952   0.01119323]]]]\n"
     ]
    }
   ],
   "source": [
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e629d2",
   "metadata": {},
   "source": [
    "#### Loss Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "257fa364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25c881b1148>]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bd76b5",
   "metadata": {},
   "source": [
    "### Apply Calculated Karnel to image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f7fc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_2dkernel(input_image, kernel):\n",
    "    # Ensure input image is a float tensor and normalize it\n",
    "    input_image = torch.tensor(input_image, dtype=torch.float32) / 255.0\n",
    "    # Convert the kernel to a PyTorch tensor if it is not already\n",
    "    if not isinstance(kernel, torch.Tensor):\n",
    "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
    "    # Get the dimensions of the input image\n",
    "    height, width, channels = input_image.shape\n",
    "    # Ensure size of image for convolution\n",
    "    r = input_image[:,:,0].view(1, 1, height, width)\n",
    "    g = input_image[:,:,1].view(1, 1, height, width)\n",
    "    b = input_image[:,:,2].view(1, 1, height, width)\n",
    "    # Apply the kernel to each channel\n",
    "    cr = F.conv2d(r, kernel, padding=1)\n",
    "    cg = F.conv2d(g, kernel, padding=1)\n",
    "    cb = F.conv2d(b, kernel, padding=1)\n",
    "    # Stack the channels back together\n",
    "    output_image = torch.cat([cr, cg, cb], dim=1).squeeze(0).permute(1, 2, 0).detach().numpy()\n",
    "    # Denormalize the output image (from 0-1 back to 0-255) and clip values to valid range\n",
    "    output_image = (output_image * 255.0).clip(0, 255).astype('uint8')\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143b2b0",
   "metadata": {},
   "source": [
    "### Apply Kernel to Video used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "bc500125",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [apply_2dkernel(o,k) for o in org]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c4b0d03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f945b5",
   "metadata": {},
   "source": [
    "#### Differences (Black is zero difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d72a27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = [np.abs(car[i]-apply_2dkernel(org[i],k)) for i in range(len(car))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "86f812ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2862f",
   "metadata": {},
   "source": [
    "### Kernel Applied to other video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bffeeead",
   "metadata": {},
   "outputs": [],
   "source": [
    "org1 = get_frames(open_vid(\"VDB/Y.mp4\"))\n",
    "car1 = get_frames(open_vid(\"Cartoonized/Ytoon.mp4\"))\n",
    "C1 = [apply_2dkernel(o,k) for o in org1]\n",
    "D1 = [np.abs(car1[i]-apply_2dkernel(org1[i],k)) for i in range(len(car1))]\n",
    "c1 = [cv2.filter2D(o, ddepth=-1, kernel=np.asarray(k[0][0])) for o in org1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "787565f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "7e511c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6a0bea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(D1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee26540",
   "metadata": {},
   "source": [
    "### DNN for 3D Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e39fc850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_3Dkernel(input_images, target_images, kernel_size=3, epochs=1000, learning_rate=1e-3):\n",
    "    # Normalize input and target images (to 0-1)\n",
    "    input_images = [img / 255.0 for img in input_images]\n",
    "    target_images = [img / 255.0 for img in target_images]\n",
    "    # Convert input and target images to tensors\n",
    "    input_images = torch.stack([torch.tensor(img, dtype=torch.float32) for img in input_images])\n",
    "    target_images = torch.stack([torch.tensor(img, dtype=torch.float32) for img in target_images])   \n",
    "    # Image Size\n",
    "    height, width = input_images[0].shape[0], input_images[0].shape[1]    \n",
    "    # Initialize the kernel\n",
    "    kernelr = torch.rand(1, 1, kernel_size, kernel_size, requires_grad=True)  \n",
    "    kernelg = torch.rand(1, 1, kernel_size, kernel_size, requires_grad=True)  \n",
    "    kernelb = torch.rand(1, 1, kernel_size, kernel_size, requires_grad=True)  \n",
    "    # Use an optimizer to update the kernel\n",
    "    optimizer = torch.optim.SGD([kernelr,kernelg,kernelb], lr=learning_rate)    \n",
    "    # List of loss over time\n",
    "    L = []    \n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0      \n",
    "        # Iterate through each image and calculate the total loss\n",
    "        for i in range(len(input_images)):\n",
    "            optimizer.zero_grad()  # Clear the gradients from the previous iteration  \n",
    "            # Separate each color channel\n",
    "            r = input_images[i][:,:,0].view(1, 1, height, width)\n",
    "            g = input_images[i][:,:,1].view(1, 1, height, width)\n",
    "            b = input_images[i][:,:,2].view(1, 1, height, width)            \n",
    "            # Apply convolution to each channel\n",
    "            cr = F.conv2d(r, kernelr, padding=1)\n",
    "            cg = F.conv2d(g, kernelg, padding=1)\n",
    "            cb = F.conv2d(b, kernelb, padding=1)            \n",
    "            # Calculate the loss\n",
    "            loss = (F.mse_loss(cr, target_images[i][:,:,0].unsqueeze(0).unsqueeze(0)) +\n",
    "                    F.mse_loss(cg, target_images[i][:,:,1].unsqueeze(0).unsqueeze(0)) +\n",
    "                    F.mse_loss(cb, target_images[i][:,:,2].unsqueeze(0).unsqueeze(0)))            \n",
    "            # Add to total loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()  # Compute gradients        \n",
    "            optimizer.step()  # Update the kernel with the computed gradients        \n",
    "        # Add total loss to list over epochs\n",
    "        L.append(total_loss)        \n",
    "        # Print loss every 10 iterations\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {total_loss}\", end='\\r')    \n",
    "    # Return the optimized kernel and list of loss over epochs\n",
    "    return [kernelr.detach().numpy(),kernelg.detach().numpy(),kernelb.detach().numpy()], L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a872fc",
   "metadata": {},
   "source": [
    "### Train DNN for 3D Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5f345e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Total Loss: 2.0367302671074867\r"
     ]
    }
   ],
   "source": [
    "k3d,l3d = find_3Dkernel(org,car,3,100,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84342e9",
   "metadata": {},
   "source": [
    "### Calculated Kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ff18c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[-0.12880068 -0.31311655  0.148783  ]\n",
      "   [ 0.4606632   0.4521125   0.3544668 ]\n",
      "   [ 0.01233711  0.35646823 -0.39860234]]]]\n",
      "[[[[ 0.22344309  0.26510024  0.23608764]\n",
      "   [-0.07694138  0.46053693 -0.05531402]\n",
      "   [-0.18870662 -0.15387362  0.2372263 ]]]]\n",
      "[[[[-0.02202662  0.5744075  -0.10779057]\n",
      "   [-0.11189078  0.3386241  -0.09678408]\n",
      "   [-0.08710568  0.5889042  -0.15465868]]]]\n"
     ]
    }
   ],
   "source": [
    "print(k3d[0])\n",
    "print(k3d[1])\n",
    "print(k3d[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65d482",
   "metadata": {},
   "source": [
    "### Loss Over Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "d5b2fa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x25c8823c788>]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(l3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569ec66d",
   "metadata": {},
   "source": [
    "### Apply 3D Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "af5b5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_3dkernel(input_image, kernel):\n",
    "    # Ensure input image is a float tensor and normalize it\n",
    "    input_image = torch.tensor(input_image, dtype=torch.float32) / 255.0\n",
    "    # Convert the kernel to a PyTorch tensor if it is not already\n",
    "    if not isinstance(kernel, torch.Tensor):\n",
    "        kernel = torch.tensor(kernel, dtype=torch.float32)\n",
    "    # Get the dimensions of the input image\n",
    "    height, width, channels = input_image.shape\n",
    "    # Ensure size of image for convolution\n",
    "    r = input_image[:,:,0].view(1, 1, height, width)\n",
    "    g = input_image[:,:,1].view(1, 1, height, width)\n",
    "    b = input_image[:,:,2].view(1, 1, height, width)\n",
    "    # Apply the kernel to each channel\n",
    "    cr = F.conv2d(r, kernel[0], padding=1)\n",
    "    cg = F.conv2d(g, kernel[1], padding=1)\n",
    "    cb = F.conv2d(b, kernel[2], padding=1)\n",
    "    # Stack the channels back together\n",
    "    output_image = torch.cat([cr, cg, cb], dim=1).squeeze(0).permute(1, 2, 0).detach().numpy()\n",
    "    # Denormalize the output image (from 0-1 back to 0-255) and clip values to valid range\n",
    "    output_image = (output_image * 255.0).clip(0, 255).astype('uint8')\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea84de",
   "metadata": {},
   "source": [
    "### 3D Kernel applied to original video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0fed920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "C3d = [apply_3dkernel(o,k3d) for o in org]\n",
    "ThroughFrames(C3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba06762b",
   "metadata": {},
   "source": [
    "### Difference with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "5de34752",
   "metadata": {},
   "outputs": [],
   "source": [
    "D3d = [np.abs(car[i]-apply_3dkernel(org[i],k3d)) for i in range(len(car))]\n",
    "ThroughFrames(D3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fe3ecf",
   "metadata": {},
   "source": [
    "### Using another video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e85133db",
   "metadata": {},
   "outputs": [],
   "source": [
    "org13d = get_frames(open_vid(\"VDB/puppies.mp4\"))\n",
    "car13d = get_frames(open_vid(\"Cartoonized/puppiestoon.mp4\"))\n",
    "C13d = [apply_3dkernel(o,k3d) for o in org13d]\n",
    "D13d = [np.abs(car13d[i]-apply_3dkernel(org13d[i],k3d)) for i in range(len(car13d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "93a04735",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(C13d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "cfe44ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(D13d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca2602",
   "metadata": {},
   "source": [
    "### Calculate Kernel for Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "46b319eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width:  640\n",
      "Height:  320\n",
      "FPS:  24.0\n",
      "Frame Count:  36\n"
     ]
    }
   ],
   "source": [
    "cap = open_vid(\"VDB/X.mp4\")\n",
    "orgE = get_frames(cap)\n",
    "get_props(cap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "567f5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ed = [canny_edge(o) for o in orgE[:50]]\n",
    "ThroughFrames(ed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "35ba4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_2DkernelC(input_images, target_images, kernel=None, kernel_size=3, epochs=1000, lr=1e-3):\n",
    "    # Normalize input and target images (to 0-1)\n",
    "    input_images = [img / 255.0 for img in input_images]\n",
    "    target_images = [img / 255.0 for img in target_images]\n",
    "    \n",
    "    # Convert input and target images to tensors\n",
    "    input_images = torch.stack([torch.tensor(img, dtype=torch.float32) for img in input_images])\n",
    "    target_images = torch.stack([torch.tensor(img, dtype=torch.float32) for img in target_images])   \n",
    "    \n",
    "    # Image Size\n",
    "    height, width = input_images[0].shape[0], input_images[0].shape[1]    \n",
    "    \n",
    "    # Initialize the kernel\n",
    "    if kernel is None:\n",
    "        kernel = torch.rand(1, 1, kernel_size, kernel_size, requires_grad=True)\n",
    "    else:\n",
    "        kernel = torch.tensor(kernel, dtype=torch.float32).view(1,1,kernel.shape[0],kernel.shape[1])  # Ensure it's a tensor\n",
    "        kernel.requires_grad_(True)  # Set requires_grad=True for the leaf tensor\n",
    "        \n",
    "    # Use an optimizer to update the kernel\n",
    "    optimizer = torch.optim.SGD([kernel], lr=lr)    \n",
    "    \n",
    "    # List of loss over time\n",
    "    L = []    \n",
    "    \n",
    "    padding = kernel[0][0].shape[0]//2\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0      \n",
    "        # Iterate through each image and calculate the total loss\n",
    "        for i in range(len(input_images)):\n",
    "            optimizer.zero_grad()  # Clear the gradients from the previous iteration \n",
    "            \n",
    "            # Separate each color channel\n",
    "            r = input_images[i][:,:,0].view(1, 1, height, width)\n",
    "            g = input_images[i][:,:,1].view(1, 1, height, width)\n",
    "            b = input_images[i][:,:,2].view(1, 1, height, width)            \n",
    "            \n",
    "            # Apply convolution to each channel\n",
    "            cr = F.conv2d(r, kernel, padding=padding)\n",
    "            cg = F.conv2d(g, kernel, padding=padding)\n",
    "            cb = F.conv2d(b, kernel, padding=padding)            \n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = (F.mse_loss(cr, target_images[i][:,:,0].unsqueeze(0).unsqueeze(0)) +\n",
    "                    F.mse_loss(cg, target_images[i][:,:,1].unsqueeze(0).unsqueeze(0)) +\n",
    "                    F.mse_loss(cb, target_images[i][:,:,2].unsqueeze(0).unsqueeze(0)))            \n",
    "            \n",
    "            # Add to total loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()  # Compute gradients        \n",
    "            optimizer.step()  # Update the kernel with the computed gradients        \n",
    "        \n",
    "        # Add total loss to list over epochs\n",
    "        L.append(total_loss)        \n",
    "        \n",
    "        # Print loss every epoch\n",
    "        if epoch % 1 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Total Loss: {total_loss}\", end='\\r')    \n",
    "    \n",
    "    # Return the optimized kernel and list of loss over epochs\n",
    "    return kernel.detach().numpy(), L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a1338f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Total Loss: 38.213367953896524\r"
     ]
    }
   ],
   "source": [
    "ked,led = find_2DkernelC(orgE,orgE,epochs=2,lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ca0d7825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.34859854 -0.05776078  0.27837604  0.04321105 -0.40997612]\n",
      "   [ 0.333263    0.5017552   0.3312931   0.48203894 -0.3639986 ]\n",
      "   [-0.3033903   0.25969505 -0.40633988 -0.20842764  0.1559517 ]\n",
      "   [ 0.04213686 -0.3442806  -0.05506739  0.50000596 -0.33135495]\n",
      "   [ 0.41912013 -0.10673925 -0.45176157  0.36731735  0.28507236]]]]\n"
     ]
    }
   ],
   "source": [
    "print(ked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a12f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for each step in the process:\n",
    "# -edges\n",
    "# -cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227ee93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
