{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f8fab218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94192ee",
   "metadata": {},
   "source": [
    "# Binary CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e28e485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(BinCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Placeholder for the classifier; initialized later dynamically\n",
    "        self.classifier = None\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def initialize_classifier(self, input_shape):\n",
    "        # Pass a dummy tensor to the features to get the output size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)  # Batch size of 1\n",
    "            feature_map = self.features(dummy_input)\n",
    "            flattened_size = feature_map.numel()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(flattened_size, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.classifier is None:\n",
    "            self.initialize_classifier(x.shape[1:])  # Dynamically initialize based on input shape\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the feature map\n",
    "        x = self.classifier(x)\n",
    "        return Fun.softmax(torch.sum(x,dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142acddd",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c4cd27e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionCNN(nn.Module):\n",
    "    def __init__(self, n: int):\n",
    "        super(RegionCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with pooling and dropout\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)  # Output: (16, 300, 300)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) # Output: (32, 300, 300)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)                              # Output: (32, 150, 150)\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1) # Output: (64, 150, 150)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1) # Output: (128, 150, 150)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)                              # Output: (128, 75, 75)\n",
    "        self.drop2 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        # Adaptive pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((10, 5))  # Reduce spatial dimensions to (10, 5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten(0)\n",
    "        self.l1 = nn.Linear(2*n*128 * 10 * 5, 500)\n",
    "        self.drop3 = nn.Dropout(p=0.5)\n",
    "        self.l2 = nn.Linear(500, 25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = Fun.relu(self.conv1(x))\n",
    "        x = Fun.relu(self.conv2(x))\n",
    "        x = self.pool1(x)  # Apply first pooling\n",
    "        x = self.drop1(x)  # Apply dropout\n",
    "\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "        x = Fun.relu(self.conv4(x))\n",
    "        x = self.pool2(x)  # Apply second pooling\n",
    "        x = self.drop2(x)  # Apply dropout\n",
    "\n",
    "        x = self.global_pool(x)  # Adaptive pooling to (10, 5)\n",
    "        #print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        #print(x.shape)\n",
    "        x = Fun.relu(self.l1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.drop3(x)  # Apply dropout\n",
    "        #print(x.shape)\n",
    "        x = self.l2(x)\n",
    "        #print(x.shape)\n",
    "        return x.view(5, 5)  # Reshape output to match the target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e2978d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw_BoundingBoxes(img,Boxes,Threshold=0.5):\n",
    "    if torch.is_tensor(img):\n",
    "        img = Frame2Numpy(Img).copy()\n",
    "    X = img.shape[1]\n",
    "    Y = img.shape[0]\n",
    "    for i in range(len(Boxes)):\n",
    "        if Boxes[i][4]>Threshold:\n",
    "            x = int(Boxes[i][0]*X)\n",
    "            y = int(Boxes[i][1]*Y)\n",
    "            xl = int(Boxes[i][2]*X)\n",
    "            yl = int(Boxes[i][3]*Y)\n",
    "            img = cv2.rectangle(img,(x,y),(xl,yl),(0,255,0),2)\n",
    "            #print(x,y,xl,yl,Boxes[i][4].item())\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "17b0de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add content of box\n",
    "def loss_function(Pred, Target, Img):\n",
    "    X = Img.shape[2]\n",
    "    Y = Img.shape[1]\n",
    "    MinIndex = []\n",
    "    criterion = nn.MSELoss()\n",
    "    for v in range(len(Pred)):\n",
    "        mv = float('inf')\n",
    "        mi = 0\n",
    "        for h in range(len(Target)):\n",
    "            if h not in MinIndex:\n",
    "                d = criterion(Pred[v][:-1],Target[h][:-1])\n",
    "                if d < mv:\n",
    "                    mv = d\n",
    "                    mi = h\n",
    "        MinIndex.append(mi)\n",
    "    MinIndex = torch.tensor(MinIndex, dtype=torch.long)\n",
    "    loss = 0\n",
    "    for v in range(len(MinIndex)):\n",
    "        dx = criterion((Pred[v][1]-Pred[v][3])**2,(Target[MinIndex[v]][1]-Target[MinIndex[v]][3])**2)\n",
    "        dy = criterion(Pred[v][0]-Pred[v][2],Target[MinIndex[v]][0]-Target[MinIndex[v]][2])\n",
    "        cx = criterion((Pred[v][1]+Pred[v][3])/2.0,(Target[MinIndex[v]][1]+Target[MinIndex[v]][3])/2)\n",
    "        cy = criterion((Pred[v][0]+Pred[v][2])/2.0,(Target[MinIndex[v]][0]+Target[MinIndex[v]][2])/2)\n",
    "        #if torch.all(Pred[v]>0) and torch.all(Pred[v]<1):\n",
    "         #   pcontent = Img[:][int(Pred[v][0]*Y):int(Pred[v][2]*Y)][int(Pred[v][1]*X):int(Pred[v][3]*X)]\n",
    "          #  tcontent = Img[:][int(Target[MinIndex[v]][0]*Y):int(Target[MinIndex[v]][2]*Y)][int(Target[MinIndex[v]][1]*X):int(Target[MinIndex[v]][3]*X)]\n",
    "           # content = criterion(pcontent,tcontent)\n",
    "        #else:\n",
    "         #   content = 10\n",
    "        loss += criterion(Pred[v][:4],Target[MinIndex[v]][:4])+criterion(Pred[v][-1],Target[MinIndex[v]][-1])+dx+dy+cx+cy#+content\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9c77bc",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "07287454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Path, State=None, N=3, Batch=10, Epochs=10, LR=1e-4):\n",
    "    Model = RegionCNN(N)\n",
    "    Size = (300,300)\n",
    "    \n",
    "    if State is not None:\n",
    "        Model.load_state_dict(States)\n",
    "        Model.eval();\n",
    "        \n",
    "    Loss = []\n",
    "    LMin = 1e20\n",
    "\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        epoch_start_time = time.time()  # Timer for each epoch\n",
    "        optimizer = optim.Adam(Model.parameters(), lr=LR)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in range(Batch):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Frames = get_video_frames(Path, N, Size)\n",
    "            Frames = torch.stack(Frames)\n",
    "            \n",
    "            Target = torch.zeros(5,5,dtype=torch.float)\n",
    "            steps = random.randint(0,5)\n",
    "            for s in range(steps):\n",
    "                x,y,xl,yl = random.randint(0,3*Size[1]//4),random.randint(0,3*Size[0]//4),random.randint(10,Size[1]//5),random.randint(10,Size[1]//5)\n",
    "                Target[s][0] = float(x/Size[1])\n",
    "                Target[s][1] = float(y/Size[0])\n",
    "                Target[s][2] = float((x+xl)/Size[1])\n",
    "                Target[s][3] = float((y+yl)/Size[0])\n",
    "                Target[s][4] = 1.0\n",
    "                for p in range(N):\n",
    "                    if random.random()>=0.5:\n",
    "                        Frames[p] = AddOneInc1(Frames[p],x=x,y=y,xl=xl,yl=yl)\n",
    "                Frames[N // 2] = AddOneInc1(Frames[N // 2],x=x,y=y,xl=xl,yl=yl)\n",
    "                       \n",
    "            # Pixel Differences \n",
    "            diff_pix = [Frames[i]-Frames[i+1] for i in range(len(Frames)-1)]\n",
    "            diff_pix = torch.stack(diff_pix)\n",
    "            #print(diff_pix.shape)\n",
    "            \n",
    "            # Edges Frames\n",
    "            edge_frames = [Edges_tensor(Frames[i])-Edges_tensor(Frames[i+1]) for i in range(len(Frames)-1)]\n",
    "            edge_frames = torch.stack(edge_frames)\n",
    "            #print(edge_frames.shape)\n",
    "            \n",
    "            Scores = [TSSIM(Frames[i].unsqueeze(0),Frames[i+1].unsqueeze(0)) for i in range(len(Frames)-1)]\n",
    "            Score = [t[0].item() for t in Scores]\n",
    "            SimScores = [t[1].squeeze(0) for t in Scores]\n",
    "            SimScores = torch.stack(SimScores)\n",
    "            #print(SimScores.shape)\n",
    "            \n",
    "            All = torch.cat((diff_pix,edge_frames,SimScores),dim=0)\n",
    "            #print(All.shape)\n",
    "            \n",
    "            # Forward pass\n",
    "            Pred = Model(All)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_function(Pred,Target,Frames[N//2])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            total_loss += loss.item()\n",
    "                        \n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_iter = elapsed_time / ((epoch * Batch) + batch + 1)\n",
    "            remaining_iters = (Epochs * Batch) - ((epoch * Batch) + batch + 1)\n",
    "            remaining_time = avg_time_per_iter * remaining_iters\n",
    "            Mins = int(remaining_time//60)\n",
    "            Secs = int(remaining_time%60)\n",
    "            print(f'LR: {LR:0.4} Batch [{batch+1}/{Batch}], Loss: {loss.item():.4f} Time:{Mins}:{Secs:02}', end='\\r')\n",
    "\n",
    "            # Display predictions on the frame\n",
    "            Img = Frames[N//2].unsqueeze(0)\n",
    "            img = Draw_BoundingBoxes(Img,Pred,0.5)\n",
    "            cv2.imshow(\"Prediction\", img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "        Loss.append(total_loss/Batch)\n",
    "        print(f'\\nEpoch [{epoch + 1}/{Epochs}], Loss: {total_loss / Batch:.6f}')\n",
    "        \n",
    "        LR *= 0.8\n",
    "    \n",
    "    States = Model.state_dict()\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    return Loss,States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114723de",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c055e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a0df0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d138ec27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Unordered target boxes in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "11c4833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 1e-05 Batch [50/50], Loss: 2.4078 Time:51:52\n",
      "Epoch [1/10], Loss: 3.231855\n",
      "LR: 8e-06 Batch [50/50], Loss: 0.9580 Time:54:10\n",
      "Epoch [2/10], Loss: 2.444542\n",
      "LR: 6.4e-06 Batch [50/50], Loss: 1.5721 Time:49:09\n",
      "Epoch [3/10], Loss: 2.091382\n",
      "LR: 5.12e-06 Batch [50/50], Loss: 1.8496 Time:48:19\n",
      "Epoch [4/10], Loss: 2.234083\n",
      "LR: 4.096e-06 Batch [50/50], Loss: 2.0342 Time:35:04\n",
      "Epoch [5/10], Loss: 2.374458\n",
      "LR: 3.277e-06 Batch [50/50], Loss: 2.5181 Time:24:53\n",
      "Epoch [6/10], Loss: 2.273149\n",
      "LR: 2.621e-06 Batch [50/50], Loss: 1.5717 Time:17:06\n",
      "Epoch [7/10], Loss: 2.400728\n",
      "LR: 2.097e-06 Batch [50/50], Loss: 2.0899 Time:10:31\n",
      "Epoch [8/10], Loss: 2.272801\n",
      "LR: 1.678e-06 Batch [50/50], Loss: 3.4783 Time:4:55\n",
      "Epoch [9/10], Loss: 2.264990\n",
      "LR: 1.342e-06 Batch [50/50], Loss: 2.0148 Time:0:00\n",
      "Epoch [10/10], Loss: 2.243658\n"
     ]
    }
   ],
   "source": [
    "L,States = train('VDB',Batch=50,State=State,N=3,Epochs=10,LR=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3af9f54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x180c3a4d5c8>]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L) ##Cambios de Escenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fed5b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(States,\"StatesRID3N300x300.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa6fcd",
   "metadata": {},
   "source": [
    "# Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b96769db",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatesR = torch.load(\"StatesRID3N300x300.pth\")\n",
    "StatesB = torch.load(\"StatesBID3N300x300.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4b0492db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = (300,300)\n",
    "N = 3\n",
    "Model = BinCNN(2)\n",
    "dummy = torch.rand((N-1)*3,3,Size[0],Size[1])\n",
    "Model(dummy)\n",
    "Model.load_state_dict(StatesB)\n",
    "Model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "99d08d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "RIDModel = RegionCNN(3)\n",
    "RIDModel.load_state_dict(StatesR)\n",
    "RIDModel.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6662ab",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3d140857",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames = get_video_frames(\"AI Gen\",N,(300,300))\n",
    "diff_pix = [Frames[i]-Frames[i+1] for i in range(len(Frames)-1)]\n",
    "diff_pix = torch.stack(diff_pix)\n",
    "edge_frames = [Edges_tensor(Frames[i])-Edges_tensor(Frames[i+1]) for i in range(len(Frames)-1)]\n",
    "edge_frames = torch.stack(edge_frames)\n",
    "Scores = [TSSIM(Frames[i].unsqueeze(0),Frames[i+1].unsqueeze(0)) for i in range(len(Frames)-1)]\n",
    "Score = [t[0].item() for t in Scores]\n",
    "SimScores = [t[1].squeeze(0) for t in Scores]\n",
    "SimScores = torch.stack(SimScores)\n",
    "All = torch.cat((diff_pix,edge_frames,SimScores),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bfb93ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Inconsistency: 0.4114\n",
      "Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "Pred = Model(All)\n",
    "print(f'Probability of Inconsistency: {Pred[0].item():.4f}')\n",
    "print(f'Prediction: {round(Pred[0].item())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "ff796709",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames_Np = [cv2.cvtColor(Frames[n].permute(1, 2, 0).numpy(), cv2.COLOR_BGR2RGB) for n in range(len(Frames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c8dca3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(Frames_Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b59e0d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Coord = RIDModel(All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "65292d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames_Np[N//2] = Draw_BoundingBoxes(Frames_Np[N//2],Coord,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b66adacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(Frames_Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d57d5ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Que hace el mecanismo de atencion en procesamiento de imagenes\n",
    "#Sino en PLN\n",
    "#Embeding/Classes\n",
    "#Cambios de Escenas con escena anterior (COnsecutivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4a2b94a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoders y comparacion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
