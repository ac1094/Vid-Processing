{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67048c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa2e19",
   "metadata": {},
   "source": [
    "# Binary CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9caa7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinCNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(BinCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        # Placeholder for the classifier; initialized later dynamically\n",
    "        self.classifier = None\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def initialize_classifier(self, input_shape):\n",
    "        # Pass a dummy tensor to the features to get the output size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *input_shape)  # Batch size of 1\n",
    "            feature_map = self.features(dummy_input)\n",
    "            flattened_size = feature_map.numel()\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(flattened_size, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, self.num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.classifier is None:\n",
    "            self.initialize_classifier(x.shape[1:])  # Dynamically initialize based on input shape\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the feature map\n",
    "        x = self.classifier(x)\n",
    "        return Fun.softmax(torch.sum(x,dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4d730a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxPooling\n",
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b7e51",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f25229a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionCNN(nn.Module):\n",
    "    def __init__(self, n: int):\n",
    "        super(RegionCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers with pooling and dropout\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)  # Output: (16, 300, 300)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) # Output: (32, 300, 300)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)                              # Output: (32, 150, 150)\n",
    "        self.drop1 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1) # Output: (64, 150, 150)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1) # Output: (128, 150, 150)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)                              # Output: (128, 75, 75)\n",
    "        self.drop2 = nn.Dropout(p=0.25)\n",
    "        \n",
    "        # Adaptive pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((10, 5))  # Reduce spatial dimensions to (10, 5)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten(0)\n",
    "        self.l1 = nn.Linear(2*n*128 * 10 * 5, 500)\n",
    "        self.drop3 = nn.Dropout(p=0.5)\n",
    "        self.l2 = nn.Linear(500, 50)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = Fun.relu(self.conv1(x))\n",
    "        x = Fun.relu(self.conv2(x))\n",
    "        x = self.pool1(x)  # Apply first pooling\n",
    "        x = self.drop1(x)  # Apply dropout\n",
    "\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "        x = Fun.relu(self.conv4(x))\n",
    "        x = self.pool2(x)  # Apply second pooling\n",
    "        x = self.drop2(x)  # Apply dropout\n",
    "\n",
    "        x = self.global_pool(x)  # Adaptive pooling to (10, 5)\n",
    "        #print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        #print(x.shape)\n",
    "        x = Fun.relu(self.l1(x))\n",
    "        #print(x.shape)\n",
    "        x = self.drop3(x)  # Apply dropout\n",
    "        #print(x.shape)\n",
    "        x = self.l2(x)\n",
    "        #print(x.shape)\n",
    "        return x.view(10, 5)  # Reshape output to match the target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0acdb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Draw_BoundingBoxes(Img,Boxes,Threshold=0.5):\n",
    "    img = Frame2Numpy(Img).copy()\n",
    "    X = img.shape[1]\n",
    "    Y = img.shape[0]\n",
    "    for i in range(len(Boxes)):\n",
    "        if Boxes[i][4]>Threshold:\n",
    "            x = int(Boxes[i][0]*X)\n",
    "            y = int(Boxes[i][1]*Y)\n",
    "            xl = int(Boxes[i][2]*X)\n",
    "            yl = int(Boxes[i][3]*Y)\n",
    "            img = cv2.rectangle(img,(x,y),(xl,yl),(0,255,0),2)\n",
    "            #print(x,y,xl,yl,Boxes[i][4].item())\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "803b7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def earth_movers_distance(predicted, target):\n",
    "    # Ensure tensors are the same size\n",
    "    assert predicted.size(0) == target.size(0), \"Both tensors must have the same number of rows.\"\n",
    "    \n",
    "    # Compute the pairwise distance matrix (N, N)\n",
    "    pairwise_dist = torch.cdist(predicted, target, p=2)  # L2 distances between all rows\n",
    "    \n",
    "    # Uniform weights for rows (assumes equal importance for each row)\n",
    "    N = predicted.size(0)\n",
    "    weights = torch.ones(N, device=predicted.device) / N\n",
    "    \n",
    "    # Solve the optimal transport problem using Sinkhorn's algorithm\n",
    "    # Initialize transport plan (uniform coupling matrix)\n",
    "    transport_plan = torch.outer(weights, weights)  # Shape (N, N)\n",
    "\n",
    "    # Iteratively refine the transport plan (Sinkhorn updates)\n",
    "    for _ in range(50):  # Number of Sinkhorn iterations\n",
    "        transport_plan /= transport_plan.sum(dim=1, keepdim=True)  # Normalize rows\n",
    "        transport_plan /= transport_plan.sum(dim=0, keepdim=True)  # Normalize columns\n",
    "\n",
    "    # Compute EMD as the weighted sum of transport costs\n",
    "    emd_loss = (transport_plan * pairwise_dist).sum()\n",
    "\n",
    "    return emd_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7784d064",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82ac4154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Path, State=None, N=3, Batch=10, Epochs=10, LR=1e-4):\n",
    "    Model = RegionCNN(N)\n",
    "    Size = (300,300)\n",
    "    \n",
    "    if State is not None:\n",
    "        Model.load_state_dict(States)\n",
    "        Model.eval();\n",
    "        \n",
    "    Loss = []\n",
    "    LMin = 1e20\n",
    "\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        epoch_start_time = time.time()  # Timer for each epoch\n",
    "        optimizer = optim.Adam(Model.parameters(), lr=LR)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in range(Batch):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Frames = get_video_frames(Path, N, Size)\n",
    "            Frames = torch.stack(Frames)\n",
    "            \n",
    "            Target = torch.zeros(10,5,dtype=torch.float)\n",
    "            steps = random.randint(0,10)\n",
    "            for s in range(steps):\n",
    "                x,y,xl,yl = random.randint(0,3*Size[1]//4),random.randint(0,3*Size[0]//4),random.randint(10,Size[1]//5),random.randint(10,Size[1]//5)\n",
    "                Target[s][0] = float(x/Size[1])\n",
    "                Target[s][1] = float(y/Size[0])\n",
    "                Target[s][2] = float((x+xl)/Size[1])\n",
    "                Target[s][3] = float((y+yl)/Size[0])\n",
    "                Target[s][4] = 1.0\n",
    "                for p in range(N):\n",
    "                    if random.random()>=0.5:\n",
    "                        Frames[p] = AddOneInc1(Frames[p],x=x,y=y,xl=xl,yl=yl)\n",
    "                Frames[N // 2] = AddOneInc1(Frames[N // 2],x=x,y=y,xl=xl,yl=yl)\n",
    "                       \n",
    "            # Pixel Differences \n",
    "            diff_pix = [Frames[i]-Frames[i+1] for i in range(len(Frames)-1)]\n",
    "            diff_pix = torch.stack(diff_pix)\n",
    "            #print(diff_pix.shape)\n",
    "            \n",
    "            # Edges Frames\n",
    "            edge_frames = [Edges_tensor(Frames[i])-Edges_tensor(Frames[i+1]) for i in range(len(Frames)-1)]\n",
    "            edge_frames = torch.stack(edge_frames)\n",
    "            #print(edge_frames.shape)\n",
    "            \n",
    "            Scores = [TSSIM(Frames[i].unsqueeze(0),Frames[i+1].unsqueeze(0)) for i in range(len(Frames)-1)]\n",
    "            Score = [t[0].item() for t in Scores]\n",
    "            SimScores = [t[1].squeeze(0) for t in Scores]\n",
    "            SimScores = torch.stack(SimScores)\n",
    "            #print(SimScores.shape)\n",
    "            \n",
    "            All = torch.cat((diff_pix,edge_frames,SimScores),dim=0)\n",
    "            #print(All.shape)\n",
    "            \n",
    "            # Forward pass\n",
    "            Pred = Model(All)\n",
    "            \n",
    "            # Calculate loss\n",
    "            criterion1 = nn.MSELoss()\n",
    "            #loss = 0\n",
    "            #for v in range(10):\n",
    "             #   for h in range(10):\n",
    "              #      loss += criterion1(Target[v],Pred[h])\n",
    "            #loss = chamfer_distance(Pred,Target)\n",
    "            loss = earth_movers_distance(Pred,Target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            total_loss += loss.item()\n",
    "                        \n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_iter = elapsed_time / ((epoch * Batch) + batch + 1)\n",
    "            remaining_iters = (Epochs * Batch) - ((epoch * Batch) + batch + 1)\n",
    "            remaining_time = avg_time_per_iter * remaining_iters\n",
    "            Mins = int(remaining_time//60)\n",
    "            Secs = int(remaining_time%60)\n",
    "            print(f'LR: {LR:0.4} Batch [{batch+1}/{Batch}], Loss: {loss.item():.4f} Time:{Mins}:{Secs:02}', end='\\r')\n",
    "\n",
    "            # Display predictions on the frame\n",
    "            Img = Frames[N//2].unsqueeze(0)\n",
    "            img = Draw_BoundingBoxes(Img,Pred)\n",
    "            cv2.imshow(\"Prediction\", img)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "        Loss.append(total_loss)\n",
    "        print(f'\\nEpoch [{epoch + 1}/{Epochs}], Loss: {total_loss / Batch:.6f}')\n",
    "        \n",
    "        LR *= 0.75\n",
    "    \n",
    "    States = Model.state_dict()\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    return Loss,States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829bbe27",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "794290a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3183b24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4a0ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Unordered target boxes in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b69713eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.001 Batch [50/50], Loss: 10.8003 Time:21:47\n",
      "Epoch [1/10], Loss: 7.435368\n",
      "LR: 0.00075 Batch [50/50], Loss: 9.8869 Time:18:285\n",
      "Epoch [2/10], Loss: 7.033089\n",
      "LR: 0.0005625 Batch [50/50], Loss: 4.7373 Time:15:553\n",
      "Epoch [3/10], Loss: 6.831982\n",
      "LR: 0.0004219 Batch [50/50], Loss: 1.8785 Time:13:360\n",
      "Epoch [4/10], Loss: 6.150690\n",
      "LR: 0.0003164 Batch [11/50], Loss: 7.0789 Time:13:07\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-6f7c0611f20c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mStates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'VDB'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mState\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mEpochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-24-01793fd41071>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(Path, State, N, Batch, Epochs, LR)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;31m# Forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mPred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[1;31m# Calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-35ff3609f056>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFun\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Apply first pooling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Apply dropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "L,States = train('VDB',Batch=50,State=State,N=3,Epochs=10,LR=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dec720",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(L[1:]) ##Cambios de Escenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df0714",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(States,\"StatesRID3N300x300.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803e174",
   "metadata": {},
   "source": [
    "# Load and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "StatesR = torch.load(\"StatesRID3N300x300.pth\")\n",
    "StatesB = torch.load(\"StatesBID3N300x300.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874b11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Size = (300,300)\n",
    "N = 3\n",
    "Model = BinCNN(2)\n",
    "dummy = torch.rand((N-1)*3,3,Size[0],Size[1])\n",
    "Model(dummy)\n",
    "Model.load_state_dict(StatesB)\n",
    "Model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500fdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames = get_video_frames(\"AI Gen\",N,(300,300))\n",
    "diff_pix = [Frames[i]-Frames[i+1] for i in range(len(Frames)-1)]\n",
    "diff_pix = torch.stack(diff_pix)\n",
    "edge_frames = [Edges_tensor(Frames[i])-Edges_tensor(Frames[i+1]) for i in range(len(Frames)-1)]\n",
    "edge_frames = torch.stack(edge_frames)\n",
    "Scores = [TSSIM(Frames[i].unsqueeze(0),Frames[i+1].unsqueeze(0)) for i in range(len(Frames)-1)]\n",
    "Score = [t[0].item() for t in Scores]\n",
    "SimScores = [t[1].squeeze(0) for t in Scores]\n",
    "SimScores = torch.stack(SimScores)\n",
    "All = torch.cat((diff_pix,edge_frames,SimScores),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812039d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = Model(All)\n",
    "print(f'Probability of Inconsistency: {Pred[0].item():.4f}')\n",
    "print(f'Prediction: {round(Pred[0].item())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c67399",
   "metadata": {},
   "outputs": [],
   "source": [
    "Frames_Np = [cv2.cvtColor(Frames[n].permute(1, 2, 0).numpy(), cv2.COLOR_BGR2RGB) for n in range(len(Frames))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3255660",
   "metadata": {},
   "outputs": [],
   "source": [
    "ThroughFrames(Frames_Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8c9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Que hace el mecanismo de atencion en procesamiento de imagenes\n",
    "#Sino en PLN\n",
    "#Embeding/Classes\n",
    "#Cambios de Escenas con escena anterior (COnsecutivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e6f9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoders y comparacion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
