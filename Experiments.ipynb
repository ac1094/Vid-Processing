{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f66a1fe",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10052bb8",
   "metadata": {},
   "source": [
    "This Jupyter Notebook presents different experiments done to videos. This experiments focus on the temporal consistency in cartoon-like videos. We focus on using the optical flow to see how consecutive frames change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bef7e7",
   "metadata": {},
   "source": [
    "First, we import the functions needed from the notebook called \"Functions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc5c5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run Functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f71705c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63efe709",
   "metadata": {},
   "source": [
    "We load a video and store its frames in a list to then add occlusions to 3 consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e524633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width:  640\n",
      "Height:  360\n",
      "FPS:  23.976023976023978\n",
      "Frame Count:  2326\n"
     ]
    }
   ],
   "source": [
    "cap = open_vid('VDB\\L.mp4')\n",
    "Frames = get_frames(cap)\n",
    "_,_,fps,_ = get_props(cap)\n",
    "F = Frames.copy()\n",
    "loc=[[150,50]]\n",
    "sizes = [[50,100]]\n",
    "N = 15\n",
    "F[N] = occlusions(F[N],loc=loc,sizes=sizes,shapes=['rectangle'], colors=(200,100,50))\n",
    "F[N+1] = occlusions(F[N+1],loc=loc,sizes=sizes,shapes=['rectangle'], colors=(200,100,50))\n",
    "F[N+2] = occlusions(F[N+2],loc=loc,sizes=sizes,shapes=['rectangle'], colors=(200,100,50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb84c4c",
   "metadata": {},
   "source": [
    "We play the video to see the occlusions.<br>\n",
    "Use 'q' to stop and close window at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e22d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delay:  41\n"
     ]
    }
   ],
   "source": [
    "play_frames(F[:30],fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab6b3b",
   "metadata": {},
   "source": [
    "Then we calculate different properties in the series of frames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc0ce29",
   "metadata": {},
   "source": [
    "First, we calculate and display the magnitude of the optical flow by using Furneback's method.<br>\n",
    "Use the arrows to move forward or backward in the frame sequence.<br>\n",
    "Use 'q' at any time to stop and close windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15602c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r"
     ]
    }
   ],
   "source": [
    "_ = OFM(F[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79a822",
   "metadata": {},
   "source": [
    "We can also see the vector field of the optical flow in with the green arrows. Also, we see the sum of these vectors as the red arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "274a9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = OFV(F[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0a93f",
   "metadata": {},
   "source": [
    "We can also use the Lucas-Kanade method to calculate the optical flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92aedc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = OFLK(F[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13018630",
   "metadata": {},
   "source": [
    "And finally, we can compute the optical flow using Phase Correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7107b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = PhaseC(F[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04daa22",
   "metadata": {},
   "source": [
    "Another way to se how consecutive frames change from one to another, we can calculate its difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bb16f",
   "metadata": {},
   "source": [
    "First, we calculate the difference between a pair of consecutive frames and use a threshold to create a binary image that shows the region with most changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5796e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = frame_dif(F[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521f26ee",
   "metadata": {},
   "source": [
    "But for more simplicity, we can just calculate the differences and normalize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df383d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = frame_dif1(F[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee09ea8",
   "metadata": {},
   "source": [
    "Now, lets try to cartoonize an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15df901",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'IDB/therock.jpg'\n",
    "img = cv2.imread(file)\n",
    "toon = cartoonize_image(img,kc=3)\n",
    "display_images([img,toon])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5748bd6c",
   "metadata": {},
   "source": [
    "What about cartoonizing a video?<br>\n",
    "To speed up the cartoonization, the frames in the video are resized to smaller ones and then returned to their original size. Take in consideration that this will also degrade the quality of the cartoonized video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "340f320d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width:  640\n",
      "Height:  320\n",
      "FPS:  24.0\n",
      "Frame Count:  69\n"
     ]
    }
   ],
   "source": [
    "cap = open_vid('VDB/U.mp4')\n",
    "Frames = get_frames(cap)\n",
    "w,h,fps,_ = get_props(cap)\n",
    "Toon = [cv2.resize(cartoonize_image(cv2.resize(f,(200,200),interpolation=cv2.INTER_AREA)),(w,h),interpolation=cv2.INTER_CUBIC) for f in Frames]\n",
    "#save_vid(Toon,'Cartoonized/U_toonzz.mp4',fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ca7328",
   "metadata": {},
   "source": [
    "Lets see the original video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04ef8604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delay:  41\n"
     ]
    }
   ],
   "source": [
    "play_frames(Frames,fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134cbe6",
   "metadata": {},
   "source": [
    "Lets see the cartoonized video now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e7e862de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delay:  41\n"
     ]
    }
   ],
   "source": [
    "play_frames(Toon,fps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259316fb",
   "metadata": {},
   "source": [
    "Now lets see how the optical flow behave for the original video and the cartoonized video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a41b6db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delay:  41\n",
      "Delay:  41\n"
     ]
    }
   ],
   "source": [
    "cap = open_vid('VDB/U.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "Frames = get_frames(cap)\n",
    "cap = open_vid('Cartoonized/U_toon.mp4')\n",
    "Toon = get_frames(cap)\n",
    "play_frames(Frames,fps)\n",
    "play_frames(Toon,fps)\n",
    "_ = OFV(Frames,20)\n",
    "_ = OFV(Toon,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f8a4c5",
   "metadata": {},
   "source": [
    "To have more control over the optical flow, lets capture the video frames from the webcam and display it in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac1c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_of()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b003c9",
   "metadata": {},
   "source": [
    "We can compute different variability indexes of the pixel of an image. Some of these are calculated below as the relative luminance, the color variace and a variability index based on neighboring pixels. The greater the variability index, the more variability there is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21d23409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5007049189404322\n",
      "0.003574134719467966\n",
      "0.7609107919731268\n"
     ]
    }
   ],
   "source": [
    "file = 'IDB/party.jpg'\n",
    "img = cv2.imread(file)\n",
    "print(relative_luminance(img))\n",
    "print(color_variance(img))\n",
    "print(calculate_variability(img))\n",
    "display_frame(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61de935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baaf9de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entorno/Interfaz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f009c3af",
   "metadata": {},
   "source": [
    "## Temporal Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0106f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = open_vid('Cartoonized/U_toon.mp4')\n",
    "Frames = get_frames(cap)\n",
    "delete_png_files('saved_frames')\n",
    "save_frames(Frames,'saved_frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb1ca38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_and_display_gradients(img1, img2):\n",
    "    # Calculate the spatial gradients using Sobel operator\n",
    "    grad_x1 = cv2.Sobel(img1, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    grad_y1 = cv2.Sobel(img1, cv2.CV_64F, 0, 1, ksize=5)\n",
    "\n",
    "    grad_x2 = cv2.Sobel(img2, cv2.CV_64F, 1, 0, ksize=5)\n",
    "    grad_y2 = cv2.Sobel(img2, cv2.CV_64F, 0, 1, ksize=5)\n",
    "\n",
    "    # Calculate the temporal gradient\n",
    "    temporal_gradient = cv2.absdiff(img2, img1)\n",
    "\n",
    "    # Normalize gradients for display purposes\n",
    "    grad_x1_display = cv2.convertScaleAbs(grad_x1)\n",
    "    grad_y1_display = cv2.convertScaleAbs(grad_y1)\n",
    "    grad_x2_display = cv2.convertScaleAbs(grad_x2)\n",
    "    grad_y2_display = cv2.convertScaleAbs(grad_y2)\n",
    "    temporal_gradient_display = cv2.convertScaleAbs(temporal_gradient)\n",
    "\n",
    "    return grad_x1, grad_y1, grad_x2, grad_y2, temporal_gradient, grad_x1_display, grad_y1_display, grad_x2_display, grad_y2_display, temporal_gradient_display\n",
    "\n",
    "def calculate_and_display_movement(img1_path, img2_path):\n",
    "    # Read the two consecutive images\n",
    "    img1 = cv2.imread(img1_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img2 = cv2.imread(img2_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both of the images could not be loaded. Check the file paths.\")\n",
    "\n",
    "    # Calculate gradients\n",
    "    grad_x1, grad_y1, grad_x2, grad_y2, temporal_gradient, grad_x1_display, grad_y1_display, grad_x2_display, grad_y2_display, temporal_gradient_display = calculate_and_display_gradients(img1, img2)\n",
    "\n",
    "    # Calculate the magnitude and angle of optical flow\n",
    "    magnitude = cv2.magnitude(grad_x2, grad_y2)\n",
    "    angle = cv2.phase(grad_x2, grad_y2, angleInDegrees=True)\n",
    "\n",
    "    # Normalize magnitude for display purposes\n",
    "    magnitude_display = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    magnitude_display = cv2.convertScaleAbs(magnitude_display)\n",
    "\n",
    "    # Display the results\n",
    "    cv2.imshow(\"Image 1\", img1)\n",
    "    cv2.imshow(\"Image 2\", img2)\n",
    "    #cv2.imshow(\"Gradient X - Image 1\", grad_x1_display)\n",
    "    #cv2.imshow(\"Gradient Y - Image 1\", grad_y1_display)\n",
    "    #cv2.imshow(\"Gradient X - Image 2\", grad_x2_display)\n",
    "    #cv2.imshow(\"Gradient Y - Image 2\", grad_y2_display)\n",
    "    cv2.imshow(\"Temporal Gradient\", temporal_gradient_display)\n",
    "    cv2.imshow(\"Optical Flow Magnitude\", magnitude_display)\n",
    "\n",
    "    # Optional: Visualize flow direction using HSV color space\n",
    "    hsv = np.zeros((img1.shape[0], img1.shape[1], 3), dtype=np.uint8)\n",
    "    hsv[..., 1] = 255\n",
    "    hsv[..., 0] = angle / 2\n",
    "    hsv[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    optical_flow = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "    cv2.imshow(\"Optical Flow Direction\", optical_flow)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "calculate_and_display_movement('saved_frames/frame_000.png', 'saved_frames/frame_001.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "260b59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_and_display_differences(image1_path, image2_path):\n",
    "    # Read the images\n",
    "    img1 = cv2.imread(image1_path)\n",
    "    img2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both images could not be loaded. Check the file paths.\")\n",
    "\n",
    "    # Convert images to RGB (OpenCV loads images in BGR format by default)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert images to grayscale\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the absolute difference between the images\n",
    "    diff_rgb = np.abs(img1_rgb - img2_rgb)\n",
    "    diff_gray = np.abs(img1_gray.astype(np.float32) - img2_gray.astype(np.float32))\n",
    "\n",
    "    # Normalize the differences to [0, 255]\n",
    "    norm_diff_rgb = cv2.normalize(diff_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_diff_gray = cv2.normalize(diff_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Split the RGB difference into color channels\n",
    "    diff_r, diff_g, diff_b = cv2.split(norm_diff_rgb)\n",
    "    \n",
    "    # Convert the single channel differences back to 3-channel images for display\n",
    "    diff_r_colored = cv2.merge([diff_r, np.zeros_like(diff_r), np.zeros_like(diff_r)])\n",
    "    diff_g_colored = cv2.merge([np.zeros_like(diff_g), diff_g, np.zeros_like(diff_g)])\n",
    "    diff_b_colored = cv2.merge([np.zeros_like(diff_b), np.zeros_like(diff_b), diff_b])\n",
    "    \n",
    "    # Display the original images and differences\n",
    "    cv2.imshow('Original Image 1', img1)\n",
    "    cv2.imshow('Original Image 2', img2)\n",
    "    cv2.imshow('Normalized Difference - Red Channel', diff_r_colored)\n",
    "    cv2.imshow('Normalized Difference - Green Channel', diff_g_colored)\n",
    "    cv2.imshow('Normalized Difference - Blue Channel', diff_b_colored)\n",
    "    cv2.imshow('Normalized Grayscale Difference', norm_diff_gray.astype(np.uint8))\n",
    "    \n",
    "    # Wait until a key is pressed and close the windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "calculate_and_display_differences('saved_frames/frame_000.png', 'saved_frames/frame_001.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9040af46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_temporal_gradient(image1_path, image2_path):\n",
    "    # Read the images\n",
    "    img1 = cv2.imread(image1_path)\n",
    "    img2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both images could not be loaded. Check the file paths.\")\n",
    "\n",
    "    # Convert images to RGB (OpenCV loads images in BGR format by default)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Convert images to grayscale\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Calculate the temporal gradient (difference) between the images\n",
    "    grad_rgb = np.abs(img2_rgb - img1_rgb)\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    # Normalize the gradients to [0, 255]\n",
    "    norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Split the RGB gradient into color channels\n",
    "    grad_r, grad_g, grad_b = cv2.split(norm_grad_rgb)\n",
    "    \n",
    "    # Convert the single channel gradients back to 3-channel images for display\n",
    "    grad_r_colored = cv2.merge([grad_r, np.zeros_like(grad_r), np.zeros_like(grad_r)])\n",
    "    grad_g_colored = cv2.merge([np.zeros_like(grad_g), grad_g, np.zeros_like(grad_g)])\n",
    "    grad_b_colored = cv2.merge([np.zeros_like(grad_b), np.zeros_like(grad_b), grad_b])\n",
    "    \n",
    "    # Display the gradients\n",
    "    cv2.imshow('Original Image 1', img1)\n",
    "    cv2.imshow('Original Image 2', img2)\n",
    "    cv2.imshow('Temporal Gradient - Red Channel', grad_r_colored)\n",
    "    cv2.imshow('Temporal Gradient - Green Channel', grad_g_colored)\n",
    "    cv2.imshow('Temporal Gradient - Blue Channel', grad_b_colored)\n",
    "    cv2.imshow('Temporal Gradient - Grayscale', norm_grad_gray.astype(np.uint8))\n",
    "    \n",
    "    # Wait until a key is pressed and close the windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "calculate_temporal_gradient('saved_frames/frame_000.png', 'saved_frames/frame_001.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7a9e94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Consistency Index (RGB): R: 13.011466811685, G: 12.35071282527026, B: 12.59720345104442\n",
      "Temporal Consistency Index (Grayscale): 12.210450228522806\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_temporal_consistency(video_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if frame_count < 2:\n",
    "        raise ValueError(\"Video must have at least two frames to calculate temporal consistency.\")\n",
    "    \n",
    "    # Initialize variables to accumulate error values\n",
    "    total_error_rgb = np.zeros(3)  # R, G, B channels\n",
    "    total_error_gray = 0\n",
    "    num_frames = 0\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Compute the absolute difference between consecutive frames\n",
    "        diff_rgb = np.abs(curr_frame_rgb.astype(np.float32) - prev_frame_rgb.astype(np.float32))\n",
    "        diff_gray = np.abs(curr_frame_gray.astype(np.float32) - prev_frame_gray.astype(np.float32))\n",
    "        \n",
    "        # Calculate the mean absolute error for the current frame pair\n",
    "        mean_error_rgb = np.mean(diff_rgb, axis=(0, 1))  # Mean error for R, G, B channels\n",
    "        mean_error_gray = np.mean(diff_gray)\n",
    "        \n",
    "        total_error_rgb += mean_error_rgb\n",
    "        total_error_gray += mean_error_gray\n",
    "        num_frames += 1\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Compute the average temporal consistency index\n",
    "    if num_frames == 0:\n",
    "        raise ValueError(\"No frames processed. Check the video file.\")\n",
    "    \n",
    "    temporal_consistency_index_rgb = total_error_rgb / num_frames\n",
    "    temporal_consistency_index_gray = total_error_gray / num_frames\n",
    "    \n",
    "    # Release video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    return temporal_consistency_index_rgb, temporal_consistency_index_gray\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Cartoonized/U_toonzz.mp4'\n",
    "temporal_consistency_index_rgb, temporal_consistency_index_gray = calculate_temporal_consistency(video_path)\n",
    "print(f'Temporal Consistency Index (RGB): R: {temporal_consistency_index_rgb[0]}, G: {temporal_consistency_index_rgb[1]}, B: {temporal_consistency_index_rgb[2]}')\n",
    "print(f'Temporal Consistency Index (Grayscale): {temporal_consistency_index_gray}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a0da3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Indexes: {'MSE_RGB': 56.19447428385417, 'SSIM_Gray': 0.7325608369867386, 'Hist_Corr_R': 0.16422995202312327, 'Hist_Corr_G': 0.2144371040869527, 'Hist_Corr_B': 0.3156782404000596, 'Edge_Similarity': 0.9294189453125}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_similarity_indexes(image1_path, image2_path):\n",
    "    # Read the images\n",
    "    img1 = cv2.imread(image1_path)\n",
    "    img2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both images could not be loaded. Check the file paths.\")\n",
    "    \n",
    "    # Convert images to RGB (OpenCV loads images in BGR format by default)\n",
    "    img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert images to grayscale\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "    \n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    # Compute histogram correlation for each channel\n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "image1_path = 'saved_frames/frame_000.png'\n",
    "image2_path = 'saved_frames/frame_001.png'\n",
    "similarity_indexes = calculate_similarity_indexes(image1_path, image2_path)\n",
    "print(f'Similarity Indexes: {similarity_indexes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a1df4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Indexes: {'MSE_RGB': 56.19447428385417, 'SSIM_Gray': 0.7325608369867386, 'Hist_Corr_R': 0.16422995202312327, 'Hist_Corr_G': 0.2144371040869527, 'Hist_Corr_B': 0.3156782404000596, 'Edge_Similarity': 0.9294189453125}\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def identify_abnormality(image1_path, image2_path):\n",
    "    # Read the images\n",
    "    img1 = cv2.imread(image1_path)\n",
    "    img2 = cv2.imread(image2_path)\n",
    "    \n",
    "    if img1 is None or img2 is None:\n",
    "        raise ValueError(\"One or both images could not be loaded. Check the file paths.\")\n",
    "    \n",
    "    # Calculate temporal gradients\n",
    "    grad_rgb, grad_gray = calculate_temporal_gradient(img1, img2)\n",
    "    \n",
    "    # Calculate similarity indexes\n",
    "    similarity_indexes = calculate_similarity_indexes(img1, img2)\n",
    "    \n",
    "    # Combine gradients and similarity indexes to identify abnormalities\n",
    "    # Normalize gradients for visibility\n",
    "    norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    \n",
    "    # Create a mask where significant changes or low similarity occur\n",
    "    threshold = 50  # Set an appropriate threshold value\n",
    "    abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "    abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "    # Highlight the regions of interest\n",
    "    highlighted_rgb = img1.copy()\n",
    "    highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "    highlighted_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "    \n",
    "    # Display the results\n",
    "    cv2.imshow('Original Image 1', img1)\n",
    "    cv2.imshow('Original Image 2', img2)\n",
    "    cv2.imshow('Temporal Gradient (RGB)', norm_grad_rgb.astype(np.uint8))\n",
    "    cv2.imshow('Temporal Gradient (Gray)', norm_grad_gray.astype(np.uint8))\n",
    "    cv2.imshow('Abnormal Regions (RGB)', highlighted_rgb)\n",
    "    cv2.imshow('Abnormal Regions (Gray)', highlighted_gray)\n",
    "    \n",
    "    # Print similarity indexes\n",
    "    print(f'Similarity Indexes: {similarity_indexes}')\n",
    "    \n",
    "    # Wait until a key is pressed and close the windows\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "image1_path = 'saved_frames/frame_000.png'\n",
    "image2_path = 'saved_frames/frame_001.png'\n",
    "identify_abnormality(image1_path, image2_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2b09f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using MP4 codec\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width * 2, frame_height * 2))  # Updated size for combined frames\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = rgb2gray(prev_frame_rgb)\n",
    "    \n",
    "    while True:\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = rgb2gray(curr_frame_rgb)\n",
    "        \n",
    "        # Calculate temporal gradients and similarity indexes\n",
    "        grad_rgb, grad_gray = calculate_temporal_gradient(prev_frame, curr_frame)\n",
    "        similarity_indexes = calculate_similarity_indexes(prev_frame, curr_frame)\n",
    "        \n",
    "        # Normalize gradients for visibility\n",
    "        norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        # Create masks for abnormalities\n",
    "        threshold = 50  # Set an appropriate threshold value\n",
    "        abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "        abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "        # Highlight the regions of interest\n",
    "        highlighted_rgb = curr_frame.copy()\n",
    "        highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "        highlighted_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "        \n",
    "        # Combine the results into a single frame\n",
    "        combined_frame = np.hstack((curr_frame, cv2.cvtColor(highlighted_rgb, cv2.COLOR_BGR2RGB)))\n",
    "        combined_frame = np.vstack((combined_frame, np.hstack((cv2.cvtColor(highlighted_gray, cv2.COLOR_GRAY2BGR), norm_grad_rgb.astype(np.uint8)))))\n",
    "        \n",
    "        # Write the combined frame to the output video\n",
    "        out.write(combined_frame)\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Cartoonized/U_toon.mp4'\n",
    "output_path = 'output_abnormalities.mp4'\n",
    "process_video(video_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82b6b386",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "def calculate_temporal_gradient(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "\n",
    "    # Calculate the temporal gradient (difference) for each channel and grayscale\n",
    "    grad_rgb = np.abs(img2_rgb.astype(np.float32) - img1_rgb.astype(np.float32))\n",
    "    grad_gray = np.abs(img2_gray.astype(np.float32) - img1_gray.astype(np.float32))\n",
    "\n",
    "    return grad_rgb, grad_gray\n",
    "\n",
    "def calculate_similarity_indexes(image1, image2):\n",
    "    # Convert images to RGB and grayscale\n",
    "    img1_rgb = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "    img2_rgb = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "    img1_gray = rgb2gray(img1_rgb)\n",
    "    img2_gray = rgb2gray(img2_rgb)\n",
    "    \n",
    "    # Resize images to the same size if they are different\n",
    "    if img1_rgb.shape != img2_rgb.shape:\n",
    "        img2_rgb = cv2.resize(img2_rgb, (img1_rgb.shape[1], img1_rgb.shape[0]))\n",
    "        img2_gray = cv2.resize(img2_gray, (img1_gray.shape[1], img1_gray.shape[0]))\n",
    "\n",
    "    # Compute Mean Squared Error (MSE) for RGB\n",
    "    mse_rgb = np.mean((img1_rgb - img2_rgb) ** 2)\n",
    "    \n",
    "    # Compute Structural Similarity Index (SSIM) for grayscale\n",
    "    ssim_gray, _ = ssim(img1_gray, img2_gray, full=True)\n",
    "    \n",
    "    # Compute Color Histograms for RGB\n",
    "    hist1_r = cv2.calcHist([img1_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist2_r = cv2.calcHist([img2_rgb[:, :, 0]], [0], None, [256], [0, 256])\n",
    "    hist1_g = cv2.calcHist([img1_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist2_g = cv2.calcHist([img2_rgb[:, :, 1]], [0], None, [256], [0, 256])\n",
    "    hist1_b = cv2.calcHist([img1_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    hist2_b = cv2.calcHist([img2_rgb[:, :, 2]], [0], None, [256], [0, 256])\n",
    "    \n",
    "    hist_corr_r = cv2.compareHist(hist1_r, hist2_r, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_g = cv2.compareHist(hist1_g, hist2_g, cv2.HISTCMP_CORREL)\n",
    "    hist_corr_b = cv2.compareHist(hist1_b, hist2_b, cv2.HISTCMP_CORREL)\n",
    "    \n",
    "    # Compute Edge-based similarity (using Canny edge detector)\n",
    "    edges1 = cv2.Canny(cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    edges2 = cv2.Canny(cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY), 100, 200)\n",
    "    \n",
    "    edge_similarity = np.sum(edges1 == edges2) / np.size(edges1)\n",
    "    \n",
    "    return {\n",
    "        'MSE_RGB': mse_rgb,\n",
    "        'SSIM_Gray': ssim_gray,\n",
    "        'Hist_Corr_R': hist_corr_r,\n",
    "        'Hist_Corr_G': hist_corr_g,\n",
    "        'Hist_Corr_B': hist_corr_b,\n",
    "        'Edge_Similarity': edge_similarity\n",
    "    }\n",
    "\n",
    "def add_label(image, text, position=(10, 30), font_scale=1, color=(255, 255, 255), thickness=2):\n",
    "    \"\"\"Add a label to an image.\"\"\"\n",
    "    return cv2.putText(image, text, position, cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness)\n",
    "\n",
    "def process_video(video_path, output_path):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file. Check the file path.\")\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Define codec and create VideoWriter object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Using MP4 codec\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width * 3, frame_height * 2))  # Updated size for combined frames\n",
    "    \n",
    "    ret, prev_frame = cap.read()\n",
    "    if not ret:\n",
    "        raise ValueError(\"Failed to read the first frame.\")\n",
    "    \n",
    "    prev_frame_rgb = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2RGB)\n",
    "    prev_frame_gray = rgb2gray(prev_frame_rgb)\n",
    "    \n",
    "    while True:\n",
    "        ret, curr_frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        curr_frame_rgb = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2RGB)\n",
    "        curr_frame_gray = rgb2gray(curr_frame_rgb)\n",
    "        \n",
    "        # Calculate temporal gradients and similarity indexes\n",
    "        grad_rgb, grad_gray = calculate_temporal_gradient(prev_frame, curr_frame)\n",
    "        similarity_indexes = calculate_similarity_indexes(prev_frame, curr_frame)\n",
    "        \n",
    "        # Normalize gradients for visibility\n",
    "        norm_grad_rgb = cv2.normalize(grad_rgb, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        norm_grad_gray = cv2.normalize(grad_gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "        \n",
    "        # Convert single-channel grayscale to 3-channel RGB for displaying\n",
    "        norm_grad_gray_rgb = cv2.cvtColor(norm_grad_gray, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Create masks for abnormalities\n",
    "        threshold = 50  # Set an appropriate threshold value\n",
    "        abnormal_rgb_mask = np.max(norm_grad_rgb, axis=2) > threshold\n",
    "        abnormal_gray_mask = norm_grad_gray > threshold\n",
    "\n",
    "        # Highlight the regions of interest\n",
    "        highlighted_rgb = curr_frame.copy()\n",
    "        highlighted_rgb[abnormal_rgb_mask] = [0, 0, 255]  # Mark changes in red\n",
    "        highlighted_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "        highlighted_gray[abnormal_gray_mask] = 255  # Mark changes in white\n",
    "        \n",
    "        # Create the combined frame\n",
    "        combined_frame = np.zeros((frame_height * 2, frame_width * 3, 3), dtype=np.uint8)\n",
    "        combined_frame[0:frame_height, 0:frame_width] = prev_frame\n",
    "        combined_frame[0:frame_height, frame_width:frame_width*2] = curr_frame\n",
    "        combined_frame[0:frame_height, frame_width*2:frame_width*3] = highlighted_rgb\n",
    "        combined_frame[frame_height:frame_height*2, 0:frame_width] = cv2.cvtColor(highlighted_gray, cv2.COLOR_GRAY2BGR)\n",
    "        combined_frame[frame_height:frame_height*2, frame_width:frame_width*2] = norm_grad_rgb\n",
    "        combined_frame[frame_height:frame_height*2, frame_width*2:frame_width*3] = norm_grad_gray_rgb\n",
    "        \n",
    "        # Add labels to the combined frame\n",
    "        combined_frame = add_label(combined_frame, \"Previous Frame\", position=(10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Current Frame\", position=(frame_width + 10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Highlighted Changes\", position=(frame_width * 2 + 10, 30))\n",
    "        combined_frame = add_label(combined_frame, \"Gray Gradient\", position=(10, frame_height + 30))\n",
    "        combined_frame = add_label(combined_frame, \"RGB Gradient\", position=(frame_width + 10, frame_height + 30))\n",
    "        combined_frame = add_label(combined_frame, \"Gray Gradient (Norm)\", position=(frame_width * 2 + 10, frame_height + 30))\n",
    "        \n",
    "        # Write the combined frame to the output video\n",
    "        out.write(combined_frame)\n",
    "        \n",
    "        # Update previous frame\n",
    "        prev_frame = curr_frame\n",
    "        prev_frame_rgb = curr_frame_rgb\n",
    "        prev_frame_gray = curr_frame_gray\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "video_path = 'Cartoonized/U_toon.mp4'\n",
    "output_path = 'output_abnormalities1.mp4'\n",
    "process_video(video_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8135a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
