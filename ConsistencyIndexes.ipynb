{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790bfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Functions import *\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6101f",
   "metadata": {},
   "source": [
    "## Display Frames in List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fef3ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ThroughFrames(frames):\n",
    "    i = 0\n",
    "    while True:\n",
    "        cv2.imshow('Frame', frames[i])\n",
    "         # Wait for a key press to move to the next frame\n",
    "        key = cv2.waitKeyEx(0)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        if key == 2424832:  # Left arrow key\n",
    "            i = i - 1\n",
    "            if i<0:\n",
    "                i = 0\n",
    "        if key == 2555904:  # Right arrow key\n",
    "            i = i + 1\n",
    "            if i>(len(frames)-1):\n",
    "                i = len(frames)-1\n",
    "    # Release the video capture object\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37fe9b",
   "metadata": {},
   "source": [
    "##  Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e43c5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Entropy(image):\n",
    "    # Convert the image to grayscale using manual conversion\n",
    "    gray_image = 0.299 * image[:, :, 2] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 0]\n",
    "    # Calculate the histogram of the grayscale image\n",
    "    hist, _ = np.histogram(gray_image, bins=256, range=(0, 256), density=True)\n",
    "    # Avoid using SciPy's entropy function, instead calculate it manually\n",
    "    hist = hist[hist > 0]  # Remove zero entries to avoid log(0)\n",
    "    hist_entropy = -np.sum(hist * np.log2(hist))\n",
    "    return hist_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c28e0",
   "metadata": {},
   "source": [
    "## Temporal Signal to Noise Ratio for Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452d3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def TSNR(image1, image2, image3):  \n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both images are invalid\")   \n",
    "    # Calculate the absolute difference manually\n",
    "    frame_diff = np.abs(image1.astype(np.float64) - image2.astype(np.float64))\n",
    "    # Calculate the mean and standard deviation of the difference\n",
    "    mean_diff = np.mean(frame_diff)\n",
    "    std_diff = np.std(frame_diff)\n",
    "    # Compute TSNR\n",
    "    tsnr = mean_diff / (std_diff + 1e-10)\n",
    "    return abs(1 - tsnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609b448b",
   "metadata": {},
   "source": [
    "## Absolute Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c1c4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Abs_Dif(image1, image2, image3):\n",
    "    return np.mean(np.abs(image1 - image2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f009b7",
   "metadata": {},
   "source": [
    "## Optical Flow End Point Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f96d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF_EPE(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both images are invalid\")\n",
    "    # Manual grayscale conversion (approximating cv2.cvtColor)\n",
    "    gray1 = 0.299 * image1[:, :, 2] + 0.587 * image1[:, :, 1] + 0.114 * image1[:, :, 0]\n",
    "    gray2 = 0.299 * image2[:, :, 2] + 0.587 * image2[:, :, 1] + 0.114 * image2[:, :, 0]\n",
    "    # Placeholder for optical flow (since Farneback can't be done with NumPy)\n",
    "    # For real use, replace this with actual optical flow computation outside of Numba.\n",
    "    # Example: Use np.gradient or any other available method that approximates movement.\n",
    "    flow_x = np.gradient(gray2, axis=1)  # Approximation: gradient as optical flow\n",
    "    flow_y = np.gradient(gray2, axis=0)\n",
    "    flow = np.stack((flow_x, flow_y), axis=-1)\n",
    "    # EPE (End Point Error) calculation\n",
    "    epe = np.linalg.norm(flow, axis=2).mean()\n",
    "    return epe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afee7f",
   "metadata": {},
   "source": [
    "## Oprical Flow Angular Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4adbdbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF_AE(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both images are invalid\")\n",
    "    # Manual grayscale conversion (approximation of cv2.cvtColor)\n",
    "    gray1 = 0.299 * image1[:, :, 2] + 0.587 * image1[:, :, 1] + 0.114 * image1[:, :, 0]\n",
    "    gray2 = 0.299 * image2[:, :, 2] + 0.587 * image2[:, :, 1] + 0.114 * image2[:, :, 0] \n",
    "    # Placeholder for optical flow (since Farneback can't be done with NumPy)\n",
    "    # Example: Gradient approximation for optical flow\n",
    "    flow_x = np.gradient(gray2, axis=1)  # Approximation: gradient as optical flow\n",
    "    flow_y = np.gradient(gray2, axis=0)\n",
    "    u, v = flow_x, flow_y   \n",
    "    # Magnitude and angle calculation (approximation of cv2.cartToPolar)\n",
    "    magnitude = np.sqrt(u**2 + v**2)\n",
    "    angle = np.arctan2(v, u)  \n",
    "    # AE (Angular Error) calculation\n",
    "    ae = np.mean(np.abs(angle))  # Taking the mean absolute angle error  \n",
    "    return ae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9370f14",
   "metadata": {},
   "source": [
    "## Optical Flow Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4ee19",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF(img1, img2, img3):\n",
    "    # Check if images are the same size\n",
    "    if img1.shape != img2.shape or img2.shape != img3.shape:\n",
    "        raise ValueError(\"All images must be of the same size\")\n",
    "    # Convert to grayscale using a simple weighted sum\n",
    "    edges1 = 0.299 * img1[:, :, 2] + 0.587 * img1[:, :, 1] + 0.114 * img1[:, :, 0]\n",
    "    edges2 = 0.299 * img2[:, :, 2] + 0.587 * img2[:, :, 1] + 0.114 * img2[:, :, 0]\n",
    "    edges3 = 0.299 * img3[:, :, 2] + 0.587 * img3[:, :, 1] + 0.114 * img3[:, :, 0]\n",
    "    # Approximate optical flow by using gradients (placeholder)\n",
    "    flow1_x = np.gradient(edges2, axis=1) - np.gradient(edges1, axis=1)\n",
    "    flow1_y = np.gradient(edges2, axis=0) - np.gradient(edges1, axis=0)\n",
    "    flow2_x = np.gradient(edges3, axis=1) - np.gradient(edges2, axis=1)\n",
    "    flow2_y = np.gradient(edges3, axis=0) - np.gradient(edges2, axis=0)\n",
    "    # Compute the magnitude and angle of the optical flow vectors for both flows\n",
    "    magnitude1 = np.sqrt(flow1_x**2 + flow1_y**2)\n",
    "    angle1 = np.arctan2(flow1_y, flow1_x)\n",
    "    magnitude2 = np.sqrt(flow2_x**2 + flow2_y**2)\n",
    "    angle2 = np.arctan2(flow2_y, flow2_x)\n",
    "    # Compute the difference between the two optical flows\n",
    "    magnitude_diff = np.abs(magnitude1 - magnitude2)\n",
    "    angle_diff = np.abs(angle1 - angle2)\n",
    "    # Normalize differences\n",
    "    magnitude_index = np.sum(magnitude_diff) / (magnitude1.size + 1e-6)  # Avoid division by zero\n",
    "    angle_index = np.sum(angle_diff) / (angle1.size + 1e-6)  # Avoid division by zero\n",
    "    # Combine magnitude and angle differences\n",
    "    difference_index = magnitude_index + angle_index\n",
    "    return difference_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfedafad",
   "metadata": {},
   "source": [
    "## Optical Flow Border Inconsistency Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def OF_B(img1, img2, img3):\n",
    "    # Check if images are the same size\n",
    "    if img1.shape != img2.shape or img2.shape != img3.shape:\n",
    "        raise ValueError(\"All images must be of the same size\")\n",
    "    # Simple edge detection approximation using gradients\n",
    "    def edge_detect(image):\n",
    "        dx = np.abs(np.gradient(image, axis=1))\n",
    "        dy = np.abs(np.gradient(image, axis=0))\n",
    "        return np.sqrt(dx**2 + dy**2)  \n",
    "    # Apply the edge detection approximation\n",
    "    edges1 = edge_detect(0.299 * img1[:, :, 2] + 0.587 * img1[:, :, 1] + 0.114 * img1[:, :, 0])\n",
    "    edges2 = edge_detect(0.299 * img2[:, :, 2] + 0.587 * img2[:, :, 1] + 0.114 * img2[:, :, 0])\n",
    "    edges3 = edge_detect(0.299 * img3[:, :, 2] + 0.587 * img3[:, :, 1] + 0.114 * img3[:, :, 0])\n",
    "    # Approximate optical flow by using gradients (placeholder)\n",
    "    flow1_x = np.gradient(edges2, axis=1) - np.gradient(edges1, axis=1)\n",
    "    flow1_y = np.gradient(edges2, axis=0) - np.gradient(edges1, axis=0)\n",
    "    flow2_x = np.gradient(edges3, axis=1) - np.gradient(edges2, axis=1)\n",
    "    flow2_y = np.gradient(edges3, axis=0) - np.gradient(edges2, axis=0)\n",
    "    # Compute the magnitude and angle of the optical flow vectors for both flows\n",
    "    magnitude1 = np.sqrt(flow1_x**2 + flow1_y**2)\n",
    "    angle1 = np.arctan2(flow1_y, flow1_x)\n",
    "    magnitude2 = np.sqrt(flow2_x**2 + flow2_y**2)\n",
    "    angle2 = np.arctan2(flow2_y, flow2_x)\n",
    "    # Compute the difference between the two optical flows\n",
    "    magnitude_diff = np.abs(magnitude1 - magnitude2)\n",
    "    angle_diff = np.abs(angle1 - angle2)\n",
    "    # Normalize differences\n",
    "    magnitude_index = np.sum(magnitude_diff) / (magnitude1.size + 1e-6)  # Avoid division by zero\n",
    "    angle_index = np.sum(angle_diff) / (angle1.size + 1e-6)  # Avoid division by zero\n",
    "    # Combine magnitude and angle differences\n",
    "    difference_index = magnitude_index + angle_index\n",
    "    return difference_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a0ac9",
   "metadata": {},
   "source": [
    "## Gray Scale Absolute Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be12657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Gray_Dif(image1, image2, image3):\n",
    "    def rgb_to_gray(image):\n",
    "        return (image[:, :, 2] + image[:, :, 1] + image[:, :, 0])/3\n",
    "    gray1 = rgb_to_gray(image1)\n",
    "    gray2 = rgb_to_gray(image2)\n",
    "    # Calculate the absolute difference between the two grayscale images\n",
    "    diff = np.abs(gray1 - gray2)\n",
    "    # Return the mean of the difference\n",
    "    return np.mean(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d014e1",
   "metadata": {},
   "source": [
    "## Temporal Structural Similarity Index Measure for Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "226cddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def TSSIM(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    gray1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    ssim_value = ssim(gray1, gray2,multichannel=True,win_size=3)\n",
    "    return abs(1-ssim_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b39c0",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84d72a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def MSE(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    # Convert images to grayscale\n",
    "    def rgb_to_gray(image):\n",
    "        return 0.299 * image[:, :, 2] + 0.587 * image[:, :, 1] + 0.114 * image[:, :, 0]\n",
    "    gray1 = rgb_to_gray(image1)\n",
    "    gray2 = rgb_to_gray(image2)\n",
    "    # Compute the Mean Squared Error\n",
    "    mse = np.mean((gray1 - gray2) ** 2)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a330bb9d",
   "metadata": {},
   "source": [
    "## Border Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9f18492",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Border_Err(image1, image2, image3):\n",
    "    lower_threshold = 50\n",
    "    upper_threshold = 255\n",
    "    if len(image1.shape) > 1:\n",
    "        image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "    if len(image2.shape) > 1:\n",
    "        image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "    # Apply Canny edge detection\n",
    "    canny1 = cv2.Canny(image1, lower_threshold, upper_threshold)\n",
    "    canny2 = cv2.Canny(image2, lower_threshold, upper_threshold)\n",
    "    # Calculate the absolute difference between the two Canny images\n",
    "    abs_diff = cv2.absdiff(canny1, canny2)\n",
    "    return np.mean(abs_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2848f7c1",
   "metadata": {},
   "source": [
    "## Color Range Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3f09bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def CRC(image1, image2, image3):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")    \n",
    "    # Convert images to float64\n",
    "    image1 = image1.astype(np.float64)\n",
    "    image2 = image2.astype(np.float64)  \n",
    "    # Calculate differences for each channel\n",
    "    def channel_diff(image1, image2, channel):\n",
    "        min1 = np.min(image1[:,:,channel])\n",
    "        max1 = np.max(image1[:,:,channel])\n",
    "        min2 = np.min(image2[:,:,channel])\n",
    "        max2 = np.max(image2[:,:,channel])\n",
    "        diff_min = abs(min1 - min2)\n",
    "        diff_max = abs(max1 - max2)\n",
    "        return diff_min, diff_max   \n",
    "    diff_r_min, diff_r_max = channel_diff(image1, image2, 2)  # Red channel\n",
    "    diff_g_min, diff_g_max = channel_diff(image1, image2, 1)  # Green channel\n",
    "    diff_b_min, diff_b_max = channel_diff(image1, image2, 0)  # Blue channel   \n",
    "    # Calculate CRC index\n",
    "    crci = (diff_r_min + diff_r_max + diff_g_min + diff_g_max + diff_b_min + diff_b_max) / 6    \n",
    "    return crci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282ecddb",
   "metadata": {},
   "source": [
    "## Entropy Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ac307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.numba.jit\n",
    "def Entropy_Dif(image1,image2, image3):\n",
    "    return abs(Entropy(image1)-Entropy(image2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d60826",
   "metadata": {},
   "source": [
    "# Frequency Magnitude Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0737df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Freq_Dif(img1, img2, img3):\n",
    "    # Check if images are the same size\n",
    "    if img1.shape != img2.shape:\n",
    "        raise ValueError(\"Images must be of the same size\")  \n",
    "    # Compute FFT\n",
    "    f1 = np.fft.fft2(img1)\n",
    "    f2 = np.fft.fft2(img2)  \n",
    "    # Shift zero frequency component to the center\n",
    "    f1_shifted = np.fft.fftshift(f1)\n",
    "    f2_shifted = np.fft.fftshift(f2)  \n",
    "    # Compute magnitude spectrum\n",
    "    magnitude1 = np.abs(f1_shifted)\n",
    "    magnitude2 = np.abs(f2_shifted)  \n",
    "    # Compute the difference in magnitude spectra\n",
    "    magnitude_diff = np.abs(magnitude1 - magnitude2)\n",
    "    # Compute the difference index\n",
    "    sum_magnitude1 = np.sum(magnitude1)\n",
    "    sum_magnitude2 = np.sum(magnitude2)\n",
    "    sum_magnitude_diff = np.sum(magnitude_diff)\n",
    "    difference_index = sum_magnitude_diff / (sum_magnitude1 + sum_magnitude2 + 1e-6)  # Avoid division by zero\n",
    "    return difference_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e55548",
   "metadata": {},
   "source": [
    "## Combined All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f62b9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Combined_Metrics(image1, image2,op):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    tsnr = TSNR(image1,image2)\n",
    "    adif = Abs_Dif(image1,image2)\n",
    "    epe = OF_EPE(image1,image2)\n",
    "    ae = OF_AE(image1,image2)\n",
    "    gray = Gray_Dif(image1,image2)\n",
    "    ssim_value = TSSIM(image1,image2)\n",
    "    mse_value = MSE(image1,image2)\n",
    "    border_consistency_value = Border_Err(image1,image2)\n",
    "    crci_value = CRC(image1,image2)\n",
    "    ent = Entropy_Dif(image1,image2)\n",
    "    # Normalize and combine the metrics into a single consistency index\n",
    "    metrics = np.array([tsnr,adif, epe, ae, gray,ssim_value, mse_value, border_consistency_value, crci_value, ent])\n",
    "    normalized_metrics = (metrics - np.min(metrics)) / (np.max(metrics) - np.min(metrics))\n",
    "    combined_consistency_index = np.mean(normalized_metrics)\n",
    "    #print(metrics)\n",
    "    # Z-score normalization\n",
    "    mean = np.mean(metrics)\n",
    "    std = np.std(metrics)\n",
    "    Z_metrics = (metrics - mean) / std\n",
    "    \n",
    "    if op==\"norm\":\n",
    "        return combined_consistency_index\n",
    "    if op==\"mean\":\n",
    "        return np.mean(metrics)\n",
    "    if op==\"log\":\n",
    "        metrics[metrics <= 0] = 1e-10\n",
    "        return np.sum(np.log(metrics))\n",
    "    if op==\"Z\":\n",
    "        return np.mean(Z_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f574d20d",
   "metadata": {},
   "source": [
    "## Mixed Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b103e299",
   "metadata": {},
   "outputs": [],
   "source": [
    "@numba.jit\n",
    "def Mix_Metrics(image1, image2,image3,op=\"mean\",M = [TSSIM,MSE],W=[1,1]):\n",
    "    if image1 is None or image2 is None:\n",
    "        raise ValueError(\"One or both image paths are invalid\")\n",
    "    metrics = []\n",
    "    for i in range(len(M)):\n",
    "        #print(image1.shape,image2.shape)\n",
    "        metrics.append(W[i]*M[i](image1,image2,image3))\n",
    "    # Normalize and combine the metrics into a single consistency index\n",
    "    metrics = np.array(metrics)\n",
    "    normalized_metrics = (metrics - np.min(metrics)) / (np.max(metrics) - np.min(metrics))\n",
    "    combined_consistency_index = np.mean(normalized_metrics)\n",
    "    #print(metrics)\n",
    "    # Z-score normalization\n",
    "    mean = np.mean(metrics)\n",
    "    std = np.std(metrics)\n",
    "    Z_metrics = (metrics - mean) / std\n",
    "    \n",
    "    if op==\"norm\":\n",
    "        return combined_consistency_index\n",
    "    if op==\"mean\":\n",
    "        return np.mean(metrics)\n",
    "    if op==\"log\":\n",
    "        metrics[metrics <= 0] = 1e-10\n",
    "        return np.sum(np.log(metrics))\n",
    "    if op==\"Z\":\n",
    "        return np.mean(Z_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaca53d2",
   "metadata": {},
   "source": [
    "## WIndowed Max Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e80c35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WMax_Inconsistency(img1, img2, img3, wsize=(3,3), step=(3,3), Func=Combined_Metrics, op=\"mean\",M=[],Weights=[]):\n",
    "    kw,kh = wsize\n",
    "    H,W,_ = img1.shape\n",
    "    sw,sh = step\n",
    "    maxC = 0\n",
    "    maxR = [0,0,kw,kh]\n",
    "    # Calculate the output dimensions\n",
    "    output_height = (H - kh) // sh + 1\n",
    "    output_width = (W - kw) // sw + 1\n",
    "    # Initialize the output array\n",
    "    result = np.zeros((output_height, output_width))\n",
    "    # Perform the convolution\n",
    "    for i in range(0, output_height*sh, sh):\n",
    "        #print('Row: ',i,'/',output_height*sh,end='\\r')\n",
    "        for j in range(0, output_width*sw, sw):\n",
    "            # Extract the region of interest\n",
    "            region1 = img1[i:i + kh, j:j + kw]\n",
    "            region2 = img2[i:i + kh, j:j + kw]\n",
    "            region3 = img3[i:i + kh, j:j + kw]\n",
    "            # Perform element-wise multiplication and sum the result\n",
    "            if Func==Combined_Metrics or Func==Mix_Metrics:\n",
    "                if len(M)>0 and Func==Mix_Metrics:\n",
    "                    result[i // sh, j // sw] = Func(region1, region2, region3,op,M,Weights)\n",
    "                else:\n",
    "                    result[i // sh, j // sw] = Func(region1, region2,region3,op)\n",
    "            else:\n",
    "                result[i // sh, j // sw] = Func(region1, region2,region3)\n",
    "            if result[i//sh,j//sw]>=maxC:\n",
    "                maxR = [i,i+kh,j,j+kw] \n",
    "                maxC = result[i//sh,j//sw]\n",
    "            if (j+sw+kw)>W:\n",
    "                break\n",
    "        if (i+sh+kh)>H:\n",
    "            break\n",
    "    #print('')\n",
    "    return result,maxR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d1fa9",
   "metadata": {},
   "source": [
    "## Video Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab2304ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vid_consistency(F,Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    C = []\n",
    "    for i in range(len(F)-2):\n",
    "        if Func==Combined_Metrics or Func==Mix_Metrics:\n",
    "            if len(M)>0 and Func==Mix_Metrics:\n",
    "                C.append(Func(F[i],F[i+1],F[i+2],op,M,W))\n",
    "            else:\n",
    "                C.append(Func(F[i],F[i+1],F[i+2],op))\n",
    "        else:\n",
    "            C.append(Func(F[i],F[i+1],F[i+2]))\n",
    "    return np.mean(np.asarray(C)),C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4566400",
   "metadata": {},
   "source": [
    "## Windowed Video Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d786711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Vid_consistency_W(F,wsize=(3,3),step=(3,3),Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    C = []\n",
    "    for i in range(len(F)-1):\n",
    "        print('Frame: ',i+1,'/',len(F)-1,end='\\r')\n",
    "        Metrics,Region = WMax_Inconsistency(F[i],F[i+1],wsize,step,Func,op,W)\n",
    "        C.append(np.mean(Metrics))\n",
    "    return np.mean(np.asarray(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cc044",
   "metadata": {},
   "source": [
    "## Draw Window with Max Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bc1f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def DrawInconsistancy(F,wsize=(50,50),step=(50,50),Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    L = 3\n",
    "    DI = np.copy(F)\n",
    "    for i in range(len(F)-2):\n",
    "        print('Frame: ',i+1,'/',len(F)-1,end='\\r')\n",
    "        Metrics,Region = WMax_Inconsistency(F[i],F[i+1],F[i+2],wsize,step,Func,op,M,W)\n",
    "        #print((Region[0],Region[2]),(Region[1],Region[3]))\n",
    "        DI[i+1][Region[0]:Region[0]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[2]:Region[2]+L] = [0,255,0] \n",
    "        DI[i+1][Region[1]:Region[1]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[3]:Region[3]+L] = [0,255,0] \n",
    "        #DI[i+1] = cv2.putText(DI[i+1],str(np.mean(Metrics)),(10,20),cv2.FONT_HERSHEY_SIMPLEX,0.25,(0,0,255),1,cv2.LINE_AA) \n",
    "    return DI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a949850f",
   "metadata": {},
   "source": [
    "## Draw Window with Max Inconsistency of Diffrent Window Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2dc31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def DrawInconsistancy1(F,Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    L = 3\n",
    "    DI = np.copy(F)\n",
    "    for i in range(1,len(F)-1):\n",
    "        print('Frame: ',i,'/',len(F)-1,end='\\r')\n",
    "        R = -1000\n",
    "        Region = [0,0,0,0]\n",
    "        for j in range(50,100,50):\n",
    "            Metrics,Reg= WMax_Inconsistency(F[i-1],F[i],F[i+1],(j,j),(j//2,j//2),Func,op,M,W)\n",
    "            if np.mean(Metrics)>R:\n",
    "                R = np.mean(Metrics)\n",
    "                Region = Reg\n",
    "        #print((Region[0],Region[2]),(Region[1],Region[3]))\n",
    "        \n",
    "        DI[i+1][Region[0]:Region[0]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[2]:Region[2]+L] = [0,255,0] \n",
    "        DI[i+1][Region[1]:Region[1]+L,Region[2]:Region[3]] = [0,255,0] \n",
    "        DI[i+1][Region[0]:Region[1],Region[3]:Region[3]+L] = [0,255,0] \n",
    "        #DI[i+1] = cv2.putText(DI[i+1],str(R),(10,20),cv2.FONT_HERSHEY_SIMPLEX,0.25,(0,0,255),1,cv2.LINE_AA) \n",
    "    return DI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5125fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@jit\n",
    "def InconsistentRegion(F,Func=Combined_Metrics,op=\"mean\",M=[],W=[]):\n",
    "    L = 3\n",
    "    h,w,_ = F[0].shape\n",
    "    DI = [np.zeros((h,w)) for _ in F]\n",
    "    for i in range(len(F)-2):\n",
    "        #print('Frame: ',i+1,'/',len(F)-1,end='\\r')\n",
    "        R = -1000\n",
    "        Region = [0,0,0,0]\n",
    "        for j in range(20,100,20):\n",
    "            Metrics,Reg= WMax_Inconsistency(F[i],F[i+1],F[i+2],(j,j),(j//2,j//2),Func,op,M,W)\n",
    "            if np.mean(Metrics)>R:\n",
    "                R = np.mean(Metrics)\n",
    "                Region = Reg\n",
    "        #print((Region[0],Region[2]),(Region[1],Region[3]))\n",
    "        \n",
    "        DI[i+1][Region[0]:Region[1],Region[2]:Region[3]] = 255\n",
    "    return DI,R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce2992d",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7654b0d",
   "metadata": {},
   "source": [
    "F = read_images(\"saved_frames\")\n",
    "F[3] = F[3][:,:,:3]\n",
    "F[6] = F[6][:,:,:3]\n",
    "F[9] = F[9][:,:,:3]\n",
    "F[14] = F[14][:,:,:3]\n",
    "I = DrawInconsistancy(F[:13],(50,50),(10,10),Mix_Metrics,\"mean\",[CRC,Border_Err,Abs_Dif],[2,1.5,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d83452",
   "metadata": {},
   "source": [
    "ThroughFrames(I)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a06eb79",
   "metadata": {},
   "source": [
    "F = read_images(\"saved_frames\")\n",
    "F[3] = F[3][:,:,:3]\n",
    "F[6] = F[6][:,:,:3]\n",
    "F[9] = F[9][:,:,:3]\n",
    "F[14] = F[14][:,:,:3]\n",
    "I = DrawInconsistancy1(F[:13],Mix_Metrics,\"mean\",[CRC,Border_Err],[1.5,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369e4aad",
   "metadata": {},
   "source": [
    "ThroughFrames(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d4d9bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check which values with min mean consistency and which max mean consistency. Ex: 1-ssim\n",
    "#Weights of each metric\n",
    "#Which metrics are the best?\n",
    "#Look for change in entropy too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "205650c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buscar origen del ruido\n",
    "#Checar si el ruido proviene desde el origen\n",
    "#Trabajar con fracuencias\n",
    "#DNN para cada tipo de error\n",
    "#Cambiar parametros de caricaturizacion por medio de DNN\n",
    "\n",
    "#Lista de parametros e indices para identificar cuales son los mejores valores\n",
    "#Algoritmos Geneticos\n",
    "#For x,y in zip(X,Y) \n",
    "\n",
    "#Para indices de inconsistencia y para caricaturizacion\n",
    "#CHecar con frames pasados (mean,var,etc) de cada parametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971932e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
