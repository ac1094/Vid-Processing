{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "862ffe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "from Functions import *\n",
    "import torch.nn.functional as Fun\n",
    "import torch\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415bd89f",
   "metadata": {},
   "source": [
    "# Add Inconsistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "6b87d139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AddOneInc1(F):\n",
    "    # Convert to NumPy Array\n",
    "    F = F.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255.0\n",
    "    # Copy of original array\n",
    "    I = F.astype(np.uint8).copy()\n",
    "    # Random location\n",
    "    x, y = random.randint(0, 2 * I.shape[0] // 3), random.randint(0, 2 * I.shape[1] // 3)\n",
    "    # Random size\n",
    "    l = random.randint(5, I.shape[1] // 20)\n",
    "    # Random option\n",
    "    Op = random.randint(0, 5)\n",
    "    if Op == 0:\n",
    "        I[x:x + l, y:y + l] = change_range_colors(\n",
    "            I[x:x + l, y:y + l],\n",
    "            (random.randint(30, 140), random.randint(30, 140), random.randint(30, 140)),\n",
    "            (random.randint(150, 255), random.randint(150, 255), random.randint(150, 255))\n",
    "        )\n",
    "    elif Op == 1:\n",
    "        R, G, B = random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)\n",
    "        I[x:x + l, y:y + l] = change_range_colors(I[x:x + l, y:y + l], (R, G, B), (R, G, B))\n",
    "    elif Op == 2:\n",
    "        Thick = random.randint(1, 10)\n",
    "        l2 = random.randint(5, I.shape[1] // 15)\n",
    "        I = cv2.line(I, (x, y), (x + l, y + l2), (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)), Thick)\n",
    "    elif Op == 3:\n",
    "        Thick = random.randint(1, 5)\n",
    "        l2 = random.randint(5, I.shape[1] // 20)\n",
    "        I = cv2.line(I, (x, y), (x + l, y + l2), (0, 0, 0), Thick)\n",
    "    elif Op == 4:\n",
    "        I[x:x + l, y:y + l] = I[x:x + l, y:y + l] + np.random.randint(-10, 10, I[x:x + l, y:y + l].shape)\n",
    "    else:\n",
    "        kernel = np.random.rand(3, 3)\n",
    "        I[x:x + l, y:y + l] = cv2.filter2D(I[x:x + l, y:y + l], -1, kernel)\n",
    "    \n",
    "    # Convert back to tensor\n",
    "    I_tensor = torch.tensor(I, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "    return I_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f0d2d",
   "metadata": {},
   "source": [
    "# Calculate OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "35a4924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OF(img1: torch.Tensor, img2: torch.Tensor, alpha: float = 1.0, iterations: int = 100):\n",
    "    assert img1.shape == img2.shape, \"Images must have the same shape\"\n",
    "\n",
    "    def rgb_to_grayscale(img):\n",
    "        return 0.2989 * img[0, :, :] + 0.5870 * img[1, :, :] + 0.1140 * img[2, :, :]\n",
    "\n",
    "    I1 = rgb_to_grayscale(img1)\n",
    "    I2 = rgb_to_grayscale(img2)\n",
    "\n",
    "    # Initialize optical flow vectors (u for x direction, v for y direction)\n",
    "    u = torch.zeros_like(I1, requires_grad=True)\n",
    "    v = torch.zeros_like(I1, requires_grad=True)\n",
    "    \n",
    "    # Define convolution kernels for gradients\n",
    "    kernel_x = torch.tensor([[[[-1, 1], [-1, 1]]]], dtype=torch.float32)\n",
    "    kernel_y = torch.tensor([[[[-1, -1], [1, 1]]]], dtype=torch.float32)\n",
    "\n",
    "    # Compute gradients with padding that maintains the original image size\n",
    "    Ix = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_x, padding=(0, 1)).squeeze(0).squeeze(0)\n",
    "    Iy = Fun.conv2d(I1.unsqueeze(0).unsqueeze(0), kernel_y, padding=(1, 0)).squeeze(0).squeeze(0)\n",
    "    It = I2 - I1  # Temporal gradient\n",
    "\n",
    "    # Ensure all tensors have matching dimensions\n",
    "    min_h = min(Ix.shape[-2], Iy.shape[-2], It.shape[-2], I1.shape[-2])\n",
    "    min_w = min(Ix.shape[-1], Iy.shape[-1], It.shape[-1], I1.shape[-1])\n",
    "\n",
    "    Ix = Ix[:min_h, :min_w]\n",
    "    Iy = Iy[:min_h, :min_w]\n",
    "    It = It[:min_h, :min_w]\n",
    "    u = u[:min_h, :min_w]\n",
    "    v = v[:min_h, :min_w]\n",
    "\n",
    "    # Iteratively update the optical flow\n",
    "    for _ in range(iterations):\n",
    "        u_avg = Fun.avg_pool2d(u.unsqueeze(0).unsqueeze(0), 3, stride=1, padding=1).squeeze(0).squeeze(0)\n",
    "        v_avg = Fun.avg_pool2d(v.unsqueeze(0).unsqueeze(0), 3, stride=1, padding=1).squeeze(0).squeeze(0)\n",
    "        \n",
    "        P = Ix * u_avg + Iy * v_avg + It\n",
    "        D = alpha ** 2 + Ix ** 2 + Iy ** 2\n",
    "        \n",
    "        u = u_avg - (Ix * P) / D\n",
    "        v = v_avg - (Iy * P) / D\n",
    "\n",
    "    flow = torch.stack((u, v), dim=0)\n",
    "    return flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9462d0",
   "metadata": {},
   "source": [
    "# Calculate Difference in Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "163a32ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_loss(image1, image2):\n",
    "    freq_image1 = torch.fft.fft2(image1 * 255, dim=(-2, -1))\n",
    "    freq_image2 = torch.fft.fft2(image2 * 255, dim=(-2, -1))\n",
    "    mag_image1 = torch.abs(freq_image1)\n",
    "    mag_image2 = torch.abs(freq_image2)\n",
    "    loss = mag_image1 - mag_image2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760031f1",
   "metadata": {},
   "source": [
    "# Generate Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "46bc9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_params(N):\n",
    "    k_pix = [torch.rand(3,3,3,3,requires_grad=True) for i in range(N)]\n",
    "    k_dif = [torch.rand(3,3,3,3,requires_grad=True) for i in range(N)]\n",
    "    k_freq = [torch.rand(3,3,3,3,requires_grad=True) for i in range(N)]\n",
    "    k_of = [torch.rand(3,3,3,3,requires_grad=True) for i in range(N)]\n",
    "    return k_pix,k_dif,k_freq,k_of,nn.Linear(3,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e71afb",
   "metadata": {},
   "source": [
    "# Main Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "0f657d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reduce(frame,k):\n",
    "    if frame.shape != (1,3,256,256):\n",
    "        print(\"Tensor not the right size.\")\n",
    "        return\n",
    "    if k.shape != (3,3,3,3):\n",
    "        print(\"Kernel not the right size.\")\n",
    "        return\n",
    "    R = frame.clone()\n",
    "    while R.shape[2]>1 and R.shape[3]>1:\n",
    "        R = torch.relu(Fun.conv2d(R,k,stride=2,padding=1))\n",
    "    return torch.relu(torch.mean(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "9dda816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OFreduce(input_tensor, kernels):\n",
    "\n",
    "    if input_tensor.ndim != 4 or input_tensor.size(1) != 2:\n",
    "        raise ValueError(\"Input tensor must have shape (N, 2, 255, 255).\")\n",
    "    \n",
    "    N, C, H, W = input_tensor.shape\n",
    "\n",
    "    # Initialize a list to hold the results\n",
    "    results = []\n",
    "\n",
    "    for kernel in kernels:\n",
    "        if kernel.ndim != 2:\n",
    "            raise ValueError(\"Each kernel must be a 2D tensor.\")\n",
    "        kernel_expanded = kernel.unsqueeze(0).expand(C, -1, -1)  # Match input channels\n",
    "        kernel_expanded = kernel_expanded.unsqueeze(0)  # Add batch dimension for conv2d\n",
    "        \n",
    "        # Perform convolution\n",
    "        output = Fun.conv2d(input_tensor, kernel_expanded, groups=C)  # (N, 1, H', W')\n",
    "        results.append(output.mean(dim=(1, 2, 3)))  # Aggregate result (N,)\n",
    "    \n",
    "    # Combine results (e.g., sum or average)\n",
    "    final_result = torch.stack(results, dim=1).mean(dim=1)  # Shape: (N,)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "d3ade978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenFun(frames,dframes,fframes,k_pix,k_dif,k_freq,Lineal):#,k_freq,k_of,fframes,offrames):\n",
    "    N = frames.shape[0]\n",
    "    framesR = torch.stack([Reduce(f.unsqueeze(0),k) for f,k in zip(frames,k_pix)])\n",
    "    dframesR = torch.stack([Reduce(f.unsqueeze(0),k) for f,k in zip(dframes,k_dif)])\n",
    "    fframesR = torch.stack([Reduce(f.unsqueeze(0),k) for f,k in zip(fframes,k_freq)])\n",
    "    framesR = torch.mean(framesR/(torch.max(framesR)+0.001))\n",
    "    dframesR = torch.mean(dframesR/(torch.max(dframesR)+0.001))\n",
    "    fframesR = torch.mean(fframesR/(torch.max(fframesR)+0.001))\n",
    "    stack = torch.stack([framesR,dframesR,fframesR])\n",
    "    return torch.sigmoid(Lineal(stack))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353acb25",
   "metadata": {},
   "source": [
    "# Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "71672c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frames(path, N, size):\n",
    "    # Find all video files in the specified path\n",
    "    video_files = [f for f in os.listdir(path) if f.endswith(('.mp4', '.avi', '.mov', '.mkv'))]\n",
    "    if not video_files:\n",
    "        raise ValueError(\"No video files found in the specified path.\")\n",
    "    \n",
    "    # Choose a random video file\n",
    "    video_file = random.choice(video_files)\n",
    "    video_path = os.path.join(path, video_file)\n",
    "    \n",
    "    # Initialize the video capture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # If N is larger than the number of frames in the video, adjust it\n",
    "    if N > total_frames:\n",
    "        raise ValueError(f\"The video has only {total_frames} frames, but {N} frames were requested.\")\n",
    "    \n",
    "    # Select a random starting frame index such that we can capture N consecutive frames\n",
    "    start_frame = random.randint(0, total_frames - N)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "    \n",
    "    # Resize transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    frames = []\n",
    "    for _ in range(N):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert the frame (BGR to RGB) and apply the resize transform\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_tensor = transform(frame_rgb)\n",
    "        frames.append(frame_tensor)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    # Stack frames and reshape to the desired output shape\n",
    "    frames_tensor = torch.stack(frames).unsqueeze(0)  # Shape (1, N, 3, H, W)\n",
    "    \n",
    "    return frames#_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c95f3b2",
   "metadata": {},
   "source": [
    "# Convert Tensor to Numpy List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "15134ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2nl(frames_tensor):\n",
    "    # Remove the batch dimension (1, N, 3, H, W) -> (N, 3, H, W)\n",
    "    frames_tensor = frames_tensor.squeeze(0)\n",
    "    \n",
    "    # Convert each frame tensor to a NumPy array\n",
    "    frame_list = [cv2.cvtColor(frame.permute(1, 2, 0).numpy(),cv2.COLOR_BGR2RGB) for frame in frames_tensor]\n",
    "    \n",
    "    return frame_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a263c27",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "733827b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(Path, State=None, N=3, Batch=10, Epochs=10, Steps=5, LR=1e-3):\n",
    "    k_pix, k_dif, k_freq, k_of, Lineal = generate_params(N)\n",
    "    Size = (256, 256)\n",
    "    \n",
    "    if State is not None:\n",
    "        model.load_state_dict(State)\n",
    "\n",
    "    Loss = []\n",
    "    LMin = 1e20\n",
    "\n",
    "    start_time = time.time()  # Start timer\n",
    "\n",
    "    for epoch in range(Epochs):\n",
    "        epoch_start_time = time.time()  # Timer for each epoch\n",
    "        \n",
    "        optimizer = optim.Adam(k_pix + k_dif + k_freq + list(Lineal.parameters()), lr=LR)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in range(Batch):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Frames = get_video_frames(Path, N, Size)\n",
    "            Frames = torch.stack(Frames)\n",
    "    \n",
    "            Target = torch.tensor([0.0]) if random.random() >= 0.5 else torch.tensor([1.0])\n",
    "\n",
    "            if Target.item() == 1.0:\n",
    "                steps = random.randint(1, Steps)\n",
    "                for s in range(steps):\n",
    "                    Frames[N // 2] = AddOneInc1(Frames[N // 2])\n",
    "\n",
    "            # Difference Frames\n",
    "            diff_frames = [Frames[i + 1] - Frames[i] for i in range(len(Frames) - 1)]\n",
    "            diff_frames = torch.stack(diff_frames)\n",
    "            \n",
    "            # Frequency Frames\n",
    "            freq_frames = [frequency_loss(Frames[i + 1], Frames[i]) for i in range(len(Frames) - 1)]\n",
    "            freq_frames = torch.stack(freq_frames)\n",
    "            \n",
    "            # OF Frames\n",
    "            of_frames = [OF(Frames[i + 1], Frames[i]) for i in range(len(Frames) - 1)]\n",
    "            of_frames = torch.stack(of_frames)\n",
    "            print(of_frames.shape)\n",
    "            \n",
    "            # Forward pass\n",
    "            Pred = GenFun(Frames, diff_frames, freq_frames, k_pix, k_dif, k_freq, Lineal)\n",
    "            \n",
    "            # Calculate loss\n",
    "            criterion = nn.BCELoss()\n",
    "            loss = criterion(Pred, Target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time_per_iter = elapsed_time / ((epoch * Batch) + batch + 1)\n",
    "            remaining_iters = (Epochs * Batch) - ((epoch * Batch) + batch + 1)\n",
    "            remaining_time = avg_time_per_iter * remaining_iters\n",
    "\n",
    "            print(f'LR: {LR:0.4} Batch [{batch+1}/{Batch}], Loss: {loss.item():.4f} Time:{remaining_time / 60:.2f} minutes', end='\\r')\n",
    "\n",
    "            # Display predictions on the frame\n",
    "            Frame = cv2.cvtColor(Frames[N // 2].permute(1, 2, 0).numpy(), cv2.COLOR_BGR2RGB)\n",
    "            Frame = cv2.putText(Frame, f'P: {round(Pred.item())}', (0, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 1)\n",
    "            Frame = cv2.putText(Frame, f'T: {int(Target.item())}', (0, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 1)\n",
    "            cv2.imshow(\"Frame\", Frame)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "        Loss.append(total_loss)\n",
    "        print(f'\\nEpoch [{epoch + 1}/{Epochs}], Loss: {total_loss / Batch:.6f}')\n",
    "        \n",
    "        LR *= 0.9\n",
    "    \n",
    "    cv2.destroyAllWindows()\n",
    "    return Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0b8cba",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "5dc6e3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "59282bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "State = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "edf2e70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 255, 255])\n",
      "LR: 0.1 Batch [1/1], Loss: 0.8160 Time:0.02 minutes\n",
      "Epoch [1/2], Loss: 0.816040\n",
      "Time taken for epoch: 1.13 seconds\n",
      "\n",
      "torch.Size([4, 2, 255, 255])\n",
      "LR: 0.09 Batch [1/1], Loss: 0.6902 Time:0.00 minutes\n",
      "Epoch [2/2], Loss: 0.690234\n",
      "Time taken for epoch: 1.28 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "L,Parameters = train('VDB',Batch=1,State=State,N=5,Epochs=2,Steps=10,LR=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "902dec1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2c1fd72c948>]"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "d16b2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(video, model, model_state, N, H, W):\n",
    "    # Load the model state\n",
    "    model.load_state_dict(model_state)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    outputs = []\n",
    "    # Iterate over the video in steps of N frames\n",
    "    num_frames = len(video)\n",
    "    for i in range(0, num_frames - N + 1):\n",
    "        # Extract the stack of frames\n",
    "        frame_stack = video[i:i + N]  # Shape: (N, H, W, C)\n",
    "        # Convert to numpy array and add batch dimension\n",
    "        frame_stack = np.array(frame_stack)  # Shape: (N, H, W, C)\n",
    "        # Rearrange to match the model's expected input shape: (batch, N, C, H, W)\n",
    "        # In our case, the frame_stack shape should be (1, N, 3, H, W) where 3 is the number of channels (RGB)\n",
    "        frame_stack = np.transpose(frame_stack, (0, 3, 1, 2))  # Shape: (N, C, H, W)\n",
    "        frame_stack = np.expand_dims(frame_stack, axis=0)  # Shape: (1, N, C, H, W)\n",
    "        # Convert to a PyTorch tensor\n",
    "        frame_stack = torch.tensor(frame_stack, dtype=torch.float32)\n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            output = model(frame_stack)\n",
    "        # Append the output to the list\n",
    "        outputs.append(int(round(output.item())))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "6922abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cap = open_vid(\"AI Gen/movie.mp4\")\n",
    "#Video = get_frames(cap)\n",
    "#VI = torch.from_numpy(Video[5]).permute(2,0,1).unsqueeze(0)\n",
    "#VI = AddOneInc1(VI)\n",
    "#Video[5] = cv2.cvtColor(VI.permute(1, 2, 0).numpy(),cv2.COLOR_BGR2RGB)\n",
    "#display_frame(Video[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "fa2e07e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Video = [cv2.resize(i,(100,100)) for i in Video[:15]]\n",
    "#Preds = process_video(Video,Model,State,3,100,100)\n",
    "#Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "a1b0dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Prediction(Vid,Model,State,N,Size):\n",
    "    model = BinIncModel(N,Size(0),Size(1))\n",
    "    P = []\n",
    "    V = [torch.tensor(cv2.resize(i,(320,320),interpolation=cv2.INTER_AREA),\n",
    "                              dtype=torch.float32).permute(2,0,1).unsqueeze(0)/255.0 for i in Vid]\n",
    "    \n",
    "    N = len(kt)//2\n",
    "    for f in range(len(V) - 2 * N):\n",
    "        first, current, last = f, f + N // 2, f + N\n",
    "        Pred = ObjFun(V[first:last], V[current], k, kt, kb)\n",
    "        P.append(cv2.resize((Pred[0] * 255).squeeze(0).permute(1, 2, 0).detach().cpu().numpy().astype(np.uint8),\n",
    "                                           (Vid[f].shape[1], Vid[f].shape[0]), interpolation=cv2.INTER_AREA))\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "2fb2e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PVid = Prediction(Video,Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "6c7da413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ThroughFrames(Video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "a76dcaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ThroughFrames(PVid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "7dbcca8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Add detail error, MSE of full image only minimizes general pixel distribution\n",
    "##Add more training examples\n",
    "##Add N previous and N post current inconsistence image\n",
    "##Add optical flow, frequencies or other features for temporal consistency\n",
    "##Different kernel sizes\n",
    "##Add Gaisian Noise\n",
    "##Reconstruct image with kernel middle between encoder and decofer\n",
    "##Use data with less loss value, selection\n",
    "##How to combine data with less error and with more error?\n",
    "##Localize area and correct error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "25c87795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deformaciones\n",
    "#parches de colores, OF ventana\n",
    "#self attention mechanism\n",
    "#image processing transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a25c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
